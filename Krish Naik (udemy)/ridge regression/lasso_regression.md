# লাসো রিগ্রেশন (Lasso Regression) — বাংলায় সহজ ব্যাখ্যা

---

## লাসো রিগ্রেশন কী?

লাসো রিগ্রেশনকে বলা হয় **L1 রেগুলারাইজেশন**।
এটি একটি বিশেষ ধরণের লিনিয়ার রিগ্রেশন যা **ফিচার সিলেকশন** করতে পারে।

---

## কেন লাসো রিগ্রেশন ব্যবহার করি?

### প্রধান ব্যবহার: ফিচার সিলেকশন (Feature Selection)

- ডেটাসেটে যে ফিচারগুলো গুরুত্বপূর্ণ নয়, সেগুলো অটোমেটিক্যালি বাদ দেওয়া
- কেবলমাত্র গুরুত্বপূর্ণ ফিচারগুলো রাখা

---

## কস্ট ফাংশন (Cost Function)

### লিনিয়ার রিগ্রেশনের সাধারণ কস্ট ফাংশন:
$$
J(θ) = \frac{1}{2m} \sum_{i=1}^{m} (h_θ(x^{(i)}) - y^{(i)})^2
$$

### লাসো রিগ্রেশনের কস্ট ফাংশন:
$$
J(θ) = \frac{1}{2m} \sum_{i=1}^{m} (h_θ(x^{(i)}) - y^{(i)})^2 + λ \sum_{j=1}^{n} |θ_j|
$$

**এখানে:**
- $λ$ = রেগুলারাইজেশন প্যারামিটার
- $|θ_j|$ = প্রতিটি ফিচারের কো-ইফিসিয়েন্টের মান (Magnitude)

---

## কিভাবে ফিচার সিলেকশন হয়?

### উদাহরণ:

ধরুন মডেলের হাইপোথিসিস সমীকরণ:
$$
h_θ(x) = θ_0 + θ_1X_1 + θ_2X_2 + θ_3X_3 + θ_4X_4
$$

**প্রাথমিক কো-ইফিসিয়েন্ট:**
- $θ_0 = 0.52$
- $θ_1 = 65$
- $θ_2 = 72$
- $θ_3 = 34$
- $θ_4 = 0.12$ (খুব ছোট, তেমন গুরুত্বপূর্ণ নয়)

**লাসো প্রয়োগের পর:**
$$
h_θ(x) = 0.52 + 65X_1 + 72X_2 + 34X_3 + 0 \cdot X_4
$$

এখানে $X_4$-এর টার্ম পুরোপুরি বাদ হয়ে গেছে — এটিই ফিচার সিলেকশন।

---

## Lambda (λ) এর প্রভাব

- **λ = 0** → সাধারণ লিনিয়ার রিগ্রেশনের মতো
- **λ বাড়লে** → কো-ইফিসিয়েন্ট ছোট হতে থাকে
- **λ আরও বাড়লে** → কিছু কো-ইফিসিয়েন্ট শূন্য (0) হয়ে যায়

---

## রিজ বনাম লাসো রিগ্রেশন

| দিক | রিজ (Ridge) - L2 | লাসো (Lasso) - L1 |
|------|------------------|---------------------|
| পেনাল্টি টাইপ | স্কোয়ারড ($θ^2$) | ম্যাগনিটিউড ($|θ|$) |
| কো-ইফিসিয়েন্ট শূন্য হয়? | ❌ না | ✅ হ্যাঁ |
| প্রধান ব্যবহার | ওভারফিটিং কমানো | ফিচার সিলেকশন |

---

## সংক্ষেপে

- লাসো রিগ্রেশন = L1 রেগুলারাইজেশন
- প্রধান সুবিধা: অপ্রয়োজনীয় ফিচার অটোমেটিক বাদ দেয়
- λ প্যারামিটার দিয়ে রেগুলারাইজেশনের মাত্রা নিয়ন্ত্রণ করা যায়

---

*© 2025 - লাসো রিগ্রেশন বাংলা টিউটোরিয়াল*
