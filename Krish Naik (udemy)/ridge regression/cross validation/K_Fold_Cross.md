# K-Fold Cross Validation

K-Fold Cross Validation ‡¶π‡¶≤‡ßã ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ú‡¶®‡¶™‡ßç‡¶∞‡¶ø‡¶Ø‡¶º ‡¶ü‡ßá‡¶ï‡¶®‡¶ø‡¶ï ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶™‡ßÅ‡¶∞‡ßã ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü‡¶ï‡ßá ‡¶∏‡¶Æ‡¶æ‡¶® ‡¶Ü‡¶ï‡¶æ‡¶∞‡ßá‡¶∞ K ‡¶ü‡¶ø ‡¶´‡ßã‡¶≤‡ßç‡¶°‡ßá ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§

‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¨‡¶æ‡¶∞ ‡ßß‡¶ü‡¶ø fold ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶∏‡ßá‡¶ü ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶æ‡¶ï‡¶ø K-1 fold ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶∏‡ßá‡¶ü ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡¶Ø‡¶º‡•§

‡¶è‡¶á ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ K ‡¶¨‡¶æ‡¶∞ ‡¶π‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶´‡ßã‡¶≤‡ßç‡¶° ‡¶è‡¶ï‡¶¨‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá test ‡¶∏‡ßá‡¶ü ‡¶π‡¶Ø‡¶º‡•§

‡¶∏‡¶¨ ‡¶∂‡ßá‡¶∑‡ßá ‡¶ó‡¶°‡¶º ‡¶™‡¶æ‡¶∞‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶∏ ‡¶®‡ßá‡¶ì‡¶Ø‡¶º‡¶æ ‡¶π‡¶Ø‡¶º‡•§

---

## üéØ ‡¶ï‡ßá‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º?

- **Hold-Out ‡¶è‡¶∞ ‡¶Æ‡¶§‡ßã ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶è‡¶ï‡¶¨‡¶æ‡¶∞ ‡¶≠‡¶æ‡¶ó ‡¶®‡¶æ ‡¶ï‡¶∞‡ßá**, ‡¶è‡¶ï‡¶æ‡¶ß‡¶ø‡¶ï ‡¶≠‡¶æ‡¶ó‡ßá ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡•§
- **‡¶õ‡ßã‡¶ü ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü‡ßá** ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ generalization ‡¶ï‡ßç‡¶∑‡¶Æ‡¶§‡¶æ ‡¶Ø‡¶æ‡¶ö‡¶æ‡¶á ‡¶ï‡¶∞‡¶§‡ßá ‡¶ï‡¶æ‡¶ú‡ßá ‡¶≤‡¶æ‡¶ó‡ßá‡•§
- **Random train-test split ‡¶è‡¶∞ ‡¶ï‡¶æ‡¶∞‡¶£‡ßá** performance variation ‡¶ï‡¶Æ‡ßá‡•§

---

## üìù ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ (K = 5 ‡¶ß‡¶∞‡ßá ‡¶®‡¶ø‡¶á)

‡¶Ø‡¶¶‡¶ø ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü‡ßá **100‡¶ü‡¶ø ‡¶°‡ßá‡¶ü‡¶æ** ‡¶•‡¶æ‡¶ï‡ßá ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶Æ‡¶∞‡¶æ **5-Fold** ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶ø:

- ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø fold = **20 ‡¶°‡ßá‡¶ü‡¶æ**‡•§

### Iteration-wise ‡¶¨‡¶ø‡¶≠‡¶æ‡¶ú‡¶®:

| Iteration | Test Set | Training Set |
|-----------|----------|--------------|
| **Iteration 1** | 20 test | 80 train |
| **Iteration 2** | ‡¶Ö‡¶®‡ßç‡¶Ø 20 test | 80 train |
| **Iteration 3** | ‡¶Ö‡¶®‡ßç‡¶Ø 20 test | 80 train |
| **Iteration 4** | ‡¶Ö‡¶®‡ßç‡¶Ø 20 test | 80 train |
| **Iteration 5** | ‡¶∂‡ßá‡¶∑ 20 test | 80 train |

üëâ **‡¶∏‡¶¨‡¶∂‡ßá‡¶∑‡ßá ‡ß´‡¶ü‡¶ø ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü ‡¶ó‡¶°‡¶º ‡¶ï‡¶∞‡ßá ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ Accuracy ‡¶¨‡ßá‡¶∞ ‡¶π‡¶Ø‡¶º‡•§**

---

## üßë‚Äçüíª Python ‡¶ï‡ßã‡¶° (K-Fold Cross Validation)

### ‡¶¨‡ßá‡¶∏‡¶ø‡¶ï Implementation:

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import KFold, cross_val_score
from sklearn.linear_model import LogisticRegression

# ‡¶°‡ßá‡¶ü‡¶æ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ
data = load_iris()
X = data.data
y = data.target

# Logistic Regression ‡¶Æ‡¶°‡ßá‡¶≤
model = LogisticRegression(max_iter=200)

# 5-Fold Cross Validation
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶´‡ßã‡¶≤‡ßç‡¶°‡ßá‡¶∞ ‡¶∏‡ßç‡¶ï‡ßã‡¶∞ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ
scores = cross_val_score(model, X, y, cv=kfold)

print("‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶´‡ßã‡¶≤‡ßç‡¶°‡ßá‡¶∞ ‡¶∏‡ßç‡¶ï‡ßã‡¶∞:", scores)
print("‡¶ó‡¶°‡¶º Accuracy:", scores.mean())
print("Standard Deviation:", scores.std())
```

### ‚úÖ ‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü (‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£‡¶∏‡ßç‡¶¨‡¶∞‡ßÇ‡¶™):
```
‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶´‡ßã‡¶≤‡ßç‡¶°‡ßá‡¶∞ ‡¶∏‡ßç‡¶ï‡ßã‡¶∞: [0.9666 1.     0.9666 0.9333 1.    ]
‡¶ó‡¶°‡¶º Accuracy: 0.9733
Standard Deviation: 0.0230
```

---

## üîç ‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§ Manual Implementation:

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# ‡¶°‡ßá‡¶ü‡¶æ ‡¶≤‡ßã‡¶°
data = load_iris()
X = data.data
y = data.target

# K-Fold setup
k = 5
kfold = KFold(n_splits=k, shuffle=True, random_state=42)
model = LogisticRegression(max_iter=200)

# Manual implementation
fold_scores = []
fold_number = 1

for train_index, test_index in kfold.split(X):
    # ‡¶ü‡ßç‡¶∞‡ßá‡¶á‡¶® ‡¶ì ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶°‡ßá‡¶ü‡¶æ ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ
    X_train_fold, X_test_fold = X[train_index], X[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]
    
    # ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ü‡ßç‡¶∞‡ßá‡¶á‡¶® ‡¶ï‡¶∞‡¶æ
    model.fit(X_train_fold, y_train_fold)
    
    # ‡¶™‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶ï‡¶∂‡¶®
    y_pred = model.predict(X_test_fold)
    
    # Accuracy ‡¶π‡¶ø‡¶∏‡¶æ‡¶¨
    accuracy = accuracy_score(y_test_fold, y_pred)
    fold_scores.append(accuracy)
    
    print(f"Fold {fold_number}: Accuracy = {accuracy:.4f}")
    print(f"  Training samples: {len(X_train_fold)}")
    print(f"  Test samples: {len(X_test_fold)}")
    print("-" * 40)
    
    fold_number += 1

# ‡¶´‡¶æ‡¶á‡¶®‡¶æ‡¶≤ ‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü
print(f"\n‡¶∏‡¶¨ ‡¶´‡ßã‡¶≤‡ßç‡¶°‡ßá‡¶∞ ‡¶∏‡ßç‡¶ï‡ßã‡¶∞: {fold_scores}")
print(f"‡¶ó‡¶°‡¶º Accuracy: {np.mean(fold_scores):.4f}")
print(f"Standard Deviation: {np.std(fold_scores):.4f}")
```

---

## üìä ‡¶ï‡¶ñ‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨‡ßá‡¶®?

### ‚úÖ ‡¶â‡¶™‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ ‡¶™‡¶∞‡¶ø‡¶∏‡ßç‡¶•‡¶ø‡¶§‡¶ø:

1. **‡¶Ø‡¶ñ‡¶® ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü ‡¶õ‡ßã‡¶ü** ‚Üí ‡¶è‡¶ï‡¶¨‡¶æ‡¶∞ train-test split ‡¶ï‡¶∞‡¶≤‡ßá bias ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§
2. **‡¶Ø‡¶ñ‡¶® ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ performance ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶≠‡¶∞‡¶∏‡¶æ ‡¶ï‡¶∞‡¶§‡ßá ‡¶ö‡¶æ‡¶®‡•§**
3. **‡¶Ø‡¶ñ‡¶® ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ generalization ability ‡¶Ø‡¶æ‡¶ö‡¶æ‡¶á ‡¶ï‡¶∞‡¶§‡ßá ‡¶ö‡¶æ‡¶®‡•§**
4. **Model selection ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø** (‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® algorithm ‡¶§‡ßÅ‡¶≤‡¶®‡¶æ)
5. **Hyperparameter tuning ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø**

### ‚ùå ‡¶è‡¶°‡¶º‡¶ø‡¶Ø‡¶º‡ßá ‡¶ö‡¶≤‡ßÅ‡¶® ‡¶Ø‡¶ñ‡¶®:

1. **‡¶ñ‡ßÅ‡¶¨ ‡¶¨‡¶°‡¶º ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü** (computational cost ‡¶¨‡ßá‡¶∂‡¶ø)
2. **Time series data** (temporal order ‡¶®‡¶∑‡ßç‡¶ü ‡¶π‡¶Ø‡¶º)
3. **‡¶Ö‡¶§‡ßç‡¶Ø‡¶®‡ßç‡¶§ imbalanced dataset** (Stratified K-Fold ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®)

---

## üõ†Ô∏è ‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® K ‡¶Æ‡¶æ‡¶®‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶≠‡¶æ‡¶¨:

```python
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier

# ‡¶°‡ßá‡¶ü‡¶æ ‡¶≤‡ßã‡¶°
data = load_breast_cancer()
X, y = data.data, data.target
model = RandomForestClassifier(random_state=42)

# ‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® K ‡¶Æ‡¶æ‡¶® ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ
k_values = [3, 5, 7, 10, 15]

for k in k_values:
    kfold = KFold(n_splits=k, shuffle=True, random_state=42)
    scores = cross_val_score(model, X, y, cv=kfold)
    
    print(f"K = {k:2d}: Mean = {scores.mean():.4f}, Std = {scores.std():.4f}")
```

### ‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü:
```
K =  3: Mean = 0.9455, Std = 0.0123
K =  5: Mean = 0.9490, Std = 0.0187
K =  7: Mean = 0.9507, Std = 0.0211
K = 10: Mean = 0.9472, Std = 0.0234
K = 15: Mean = 0.9455, Std = 0.0298
```

---

## üìà K ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®‡ßá‡¶∞ ‡¶ó‡¶æ‡¶á‡¶°‡¶≤‡¶æ‡¶á‡¶®:

### **K = 5 (‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶ú‡¶®‡¶™‡ßç‡¶∞‡¶ø‡¶Ø‡¶º)**
- **‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:** ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§, reasonable bias-variance tradeoff
- **‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞:** ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø‡ßá

### **K = 10 (Research standard)**
- **‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:** ‡¶ï‡¶Æ bias, ‡¶ó‡¶¨‡ßá‡¶∑‡¶£‡¶æ‡¶Ø‡¶º standard
- **‡¶Ö‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:** ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶¨‡ßá‡¶∂‡¶ø ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶≤‡¶æ‡¶ó‡ßá

### **K = 3 (‡¶¶‡ßç‡¶∞‡ßÅ‡¶§ ‡¶ü‡ßá‡¶∏‡ßç‡¶ü)**
- **‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:** ‡¶ñ‡ßÅ‡¶¨ ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§
- **‡¶Ö‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:** ‡¶¨‡ßá‡¶∂‡¶ø bias

### **K = n (LOOCV)**
- **‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:** ‡¶∏‡¶∞‡ßç‡¶¨‡¶®‡¶ø‡¶Æ‡ßç‡¶® bias
- **‡¶Ö‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:** ‡¶Ö‡¶§‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶ß‡ßÄ‡¶∞, high variance

---

## üéØ Ridge Regression ‡¶è‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá K-Fold:

```python
from sklearn.linear_model import Ridge
from sklearn.model_selection import KFold, cross_val_score
from sklearn.datasets import load_boston
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# ‡¶¨‡ßã‡¶∏‡ßç‡¶ü‡¶® ‡¶π‡¶æ‡¶â‡¶ú‡¶ø‡¶Ç ‡¶°‡ßá‡¶ü‡¶æ (regression example)
# Note: load_boston deprecated, using alternative
from sklearn.datasets import fetch_california_housing

data = fetch_california_housing()
X, y = data.data, data.target

# Ridge regression with different alpha values
alphas = [0.1, 1.0, 10.0, 100.0]
k = 5

for alpha in alphas:
    # Pipeline ‡¶§‡ßà‡¶∞‡¶ø (scaling + ridge)
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('ridge', Ridge(alpha=alpha))
    ])
    
    # K-Fold CV
    kfold = KFold(n_splits=k, shuffle=True, random_state=42)
    scores = cross_val_score(pipeline, X, y, cv=kfold, 
                           scoring='neg_mean_squared_error')
    
    rmse_scores = np.sqrt(-scores)  # RMSE ‡¶è convert
    
    print(f"Alpha = {alpha:5.1f}: RMSE = {rmse_scores.mean():.4f} ¬± {rmse_scores.std():.4f}")
```

---

## üìã Best Practices:

### 1. **Always Shuffle:**
```python
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
```

### 2. **Set Random State:**
```python
# Reproducible results ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
random_state=42
```

### 3. **Use Pipeline:**
```python
# Data leakage ‡¶è‡¶°‡¶º‡¶æ‡¶§‡ßá
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', Ridge())
])
```

### 4. **Multiple Metrics:**
```python
from sklearn.model_selection import cross_validate

scoring = ['accuracy', 'precision', 'recall', 'f1']
results = cross_validate(model, X, y, cv=kfold, scoring=scoring)

for metric in scoring:
    scores = results[f'test_{metric}']
    print(f"{metric.capitalize()}: {scores.mean():.4f} ¬± {scores.std():.4f}")
```

---

## üîç ‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ ‡¶ì ‡¶Ö‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:

### ‚úÖ ‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:
1. **‡¶Ü‡¶∞‡ßã ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞‡¶Ø‡ßã‡¶ó‡ßç‡¶Ø performance estimate**
2. **‡¶∏‡¶¨ ‡¶°‡ßá‡¶ü‡¶æ training ‡¶ì testing ‡¶â‡¶≠‡¶Ø‡¶º‡ßá ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡¶Ø‡¶º**
3. **Performance variance measure ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º**
4. **Model selection ‡¶è ‡¶ï‡¶æ‡¶∞‡ßç‡¶Ø‡¶ï‡¶∞**
5. **Overfitting detection ‡¶∏‡¶π‡¶ú**

### ‚ùå ‡¶Ö‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:
1. **Computational cost ‡¶¨‡ßá‡¶∂‡¶ø** (K ‡¶¨‡¶æ‡¶∞ training)
2. **Time series data ‡¶è suitable ‡¶®‡¶Ø‡¶º**
3. **Memory requirements ‡¶¨‡ßá‡¶∂‡¶ø**
4. **K ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶® subjective**

---

## üìä K-Fold vs Hold-Out ‡¶§‡ßÅ‡¶≤‡¶®‡¶æ:

| ‡¶¨‡ßà‡¶∂‡¶ø‡¶∑‡ßç‡¶ü‡ßç‡¶Ø | K-Fold | Hold-Out |
|----------|--------|----------|
| **Reliability** | ‡¶â‡¶ö‡ßç‡¶ö | ‡¶Æ‡¶æ‡¶ù‡¶æ‡¶∞‡¶ø |
| **Computational Cost** | ‡¶¨‡ßá‡¶∂‡¶ø | ‡¶ï‡¶Æ |
| **Data Utilization** | ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ | ‡¶Ü‡¶Ç‡¶∂‡¶ø‡¶ï |
| **Variance** | ‡¶ï‡¶Æ | ‡¶¨‡ßá‡¶∂‡¶ø |
| **Implementation** | ‡¶ú‡¶ü‡¶ø‡¶≤ | ‡¶∏‡¶π‡¶ú |
| **Small Dataset** | ‡¶â‡¶™‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ | ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ |

---

## üí° ‡¶Æ‡¶®‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ‡¶∞ ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º:

1. **K-Fold ‡¶π‡¶≤‡ßã gold standard** for model evaluation
2. **K=5 ‡¶¨‡¶æ K=10** ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá common choices
3. **‡¶õ‡ßã‡¶ü ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü‡ßá ‡¶Ö‡¶™‡¶∞‡¶ø‡¶π‡¶æ‡¶∞‡ßç‡¶Ø**
4. **Pipeline ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá data leakage ‡¶è‡¶°‡¶º‡¶æ‡¶®**
5. **Standard deviation ‡¶¶‡ßá‡¶ñ‡ßá consistency ‡¶¨‡ßÅ‡¶ù‡ßÅ‡¶®**

---

**‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡ßá‡¶™‡ßá:** K-Fold Cross Validation ‡¶Ü‡¶™‡¶®‡¶æ‡¶ï‡ßá ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ **‡¶∏‡¶§‡ßç‡¶Ø‡¶ø‡¶ï‡¶æ‡¶∞‡ßá‡¶∞ performance** ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡ßá, ‡¶Ø‡¶æ ‡¶è‡¶ï‡¶ï train-test split ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶∏‡¶Æ‡ßç‡¶≠‡¶¨ ‡¶®‡¶Ø‡¶º‡•§