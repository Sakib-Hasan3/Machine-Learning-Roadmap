
# ‡¶ï‡ßç‡¶∞‡¶∏ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡¶ø‡¶°‡ßá‡¶∂‡¶® (Cross Validation)

‡¶Æ‡ßá‡¶∂‡¶ø‡¶® ‡¶≤‡¶æ‡¶∞‡ßç‡¶®‡¶ø‡¶Ç-‡¶è **Cross Validation (‡¶ï‡ßç‡¶∞‡¶∏ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡¶ø‡¶°‡ßá‡¶∂‡¶®)** ‡¶π‡¶≤‡ßã ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ü‡ßá‡¶ï‡¶®‡¶ø‡¶ï ‡¶Ø‡¶æ ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶™‡¶æ‡¶∞‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶∏ ‡¶Ø‡¶æ‡¶ö‡¶æ‡¶á ‡¶ï‡¶∞‡¶ø‡•§ ‡¶è‡¶ü‡¶ø ‡¶Æ‡ßÇ‡¶≤‡¶§ ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü‡¶ï‡ßá ‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶≠‡¶æ‡¶ó‡ßá ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡ßá ‡¶¨‡¶æ‡¶∞‡¶¨‡¶æ‡¶∞ ‡¶ü‡ßç‡¶∞‡ßá‡¶® ‡¶ì ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡¶Ø‡¶æ‡¶§‡ßá ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ì‡¶≠‡¶æ‡¶∞‡¶´‡¶ø‡¶ü ‡¶®‡¶æ ‡¶ï‡¶∞‡ßá ‡¶è‡¶¨‡¶Ç ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶≤‡¶æ‡¶á‡¶ú‡ßá‡¶∂‡¶® ‡¶ï‡ßç‡¶∑‡¶Æ‡¶§‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶π‡¶Ø‡¶º‡•§

‡¶®‡¶ø‡¶ö‡ßá ‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶ß‡¶∞‡¶£‡ßá‡¶∞ **Cross Validation** ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶≤‡ßã:

---

## ‡ßß. **Hold-Out Validation**

- ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü‡¶ï‡ßá ‡¶¶‡ßÅ‡¶á ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º: **Training Set** ‡¶ì **Testing Set**‡•§
- ‡¶Æ‡¶°‡ßá‡¶≤‡¶ï‡ßá ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶∏‡ßá‡¶ü ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶ü‡ßç‡¶∞‡ßá‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶∏‡ßá‡¶ü ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§
- ‡¶∏‡¶π‡¶ú ‡¶π‡¶≤‡ßá‡¶ì ‡¶°‡ßá‡¶ü‡¶æ ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶´‡¶≤‡¶æ‡¶´‡¶≤ ‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§

### ‡¶¨‡ßà‡¶∂‡¶ø‡¶∑‡ßç‡¶ü‡ßç‡¶Ø:
- **‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:** ‡¶∏‡¶π‡¶ú ‡¶ì ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§
- **‡¶Ö‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:** ‡¶°‡ßá‡¶ü‡¶æ split-‡¶è‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞‡¶∂‡ßÄ‡¶≤
- **‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞:** ‡¶¨‡¶°‡¶º ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶â‡¶™‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§

---

## ‡ß®. **K-Fold Cross Validation**

- ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü‡¶ï‡ßá **K ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶ï ‡¶∏‡¶Æ‡¶æ‡¶® ‡¶Ö‡¶Ç‡¶∂‡ßá (folds)** ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§
- ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¨‡¶æ‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø fold ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶°‡ßá‡¶ü‡¶æ ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶æ‡¶ï‡¶ø K-1 fold ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ü‡ßç‡¶∞‡ßá‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§
- ‡¶è‡¶á ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ K ‡¶¨‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶ó‡¶°‡¶º ‡¶´‡¶≤‡¶æ‡¶´‡¶≤ ‡¶®‡ßá‡¶ì‡¶Ø‡¶º‡¶æ ‡¶π‡¶Ø‡¶º‡•§
- ‡¶è‡¶ü‡¶ø ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶ú‡¶®‡¶™‡ßç‡¶∞‡¶ø‡¶Ø‡¶º Cross Validation ‡¶™‡¶¶‡ßç‡¶ß‡¶§‡¶ø‡•§

### ‡¶¨‡ßà‡¶∂‡¶ø‡¶∑‡ßç‡¶ü‡ßç‡¶Ø:
- **‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ K ‡¶Æ‡¶æ‡¶®:** 5 ‡¶¨‡¶æ 10
- **‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:** ‡¶Ü‡¶∞‡ßã ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞‡¶Ø‡ßã‡¶ó‡ßç‡¶Ø ‡¶´‡¶≤‡¶æ‡¶´‡¶≤
- **‡¶Ö‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:** ‡¶¨‡ßá‡¶∂‡¶ø computation time
- **‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞:** ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶¨‡ßá‡¶∂‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§

### Python ‡¶ï‡ßã‡¶° ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:
```python
from sklearn.model_selection import KFold, cross_val_score
from sklearn.linear_model import LinearRegression

# K-Fold setup
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
model = LinearRegression()

# Cross validation score
scores = cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_squared_error')
print(f"Average CV Score: {scores.mean()}")
```

---

## ‡ß©. **Stratified K-Fold Cross Validation**

- ‡¶è‡¶ü‡¶ø K-Fold-‡¶è‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡¶∞‡¶£‡•§
- ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü‡ßá ‡¶Ø‡ßá‡¶∏‡¶¨ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ ‡¶Ü‡¶õ‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ **class distribution** ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø fold-‡¶è ‡¶∏‡¶Æ‡¶æ‡¶®‡¶≠‡¶æ‡¶¨‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ ‡¶π‡¶Ø‡¶º‡•§
- ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£‡¶§ classification ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ‡¶Ø‡¶º ‡¶è‡¶ü‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡¶Ø‡¶æ‡¶§‡ßá data imbalance ‡¶®‡¶æ ‡¶π‡¶Ø‡¶º‡•§

### ‡¶¨‡ßà‡¶∂‡¶ø‡¶∑‡ßç‡¶ü‡ßç‡¶Ø:
- **‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø:** Class distribution ‡¶¨‡¶ú‡¶æ‡¶Ø‡¶º ‡¶∞‡¶æ‡¶ñ‡¶æ
- **‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞:** Classification problems-‡¶è
- **‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨:** Imbalanced dataset-‡¶è ‡¶Ö‡¶§‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶ï‡¶æ‡¶∞‡ßç‡¶Ø‡¶ï‡¶∞

### Python ‡¶ï‡ßã‡¶° ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:
```python
from sklearn.model_selection import StratifiedKFold

# Stratified K-Fold setup
skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=skfold, scoring='accuracy')
```

---

## ‡ß™. **Leave-One-Out Cross Validation (LOOCV)**

- ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü‡ßá ‡¶Ø‡¶§‡¶ó‡ßÅ‡¶≤‡ßã ‡¶°‡ßá‡¶ü‡¶æ ‡¶Ü‡¶õ‡ßá, ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¨‡¶æ‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶°‡ßá‡¶ü‡¶æ test ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ ‡¶π‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶æ‡¶ï‡¶ø‡¶ó‡ßÅ‡¶≤‡ßã train ‡¶π‡¶Ø‡¶º‡•§
- ‡¶è‡¶ü‡¶ø ‡¶ñ‡ßÅ‡¶¨‡¶á ‡¶∏‡¶Æ‡¶Ø‡¶º‡¶∏‡¶æ‡¶™‡ßá‡¶ï‡ßç‡¶∑, ‡¶§‡¶¨‡ßá ‡¶õ‡ßã‡¶ü ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü‡ßá‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá ‡¶≠‡¶æ‡¶≤‡ßã ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá‡•§

### ‡¶¨‡ßà‡¶∂‡¶ø‡¶∑‡ßç‡¶ü‡ßç‡¶Ø:
- **K = n** (‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá n = ‡¶°‡ßá‡¶ü‡¶æ ‡¶™‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ)
- **‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:** Maximum data utilization
- **‡¶Ö‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:** Computationally expensive
- **‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞:** ‡¶õ‡ßã‡¶ü ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø

### Python ‡¶ï‡ßã‡¶° ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:
```python
from sklearn.model_selection import LeaveOneOut

# LOOCV setup
loo = LeaveOneOut()
scores = cross_val_score(model, X, y, cv=loo, scoring='neg_mean_squared_error')
```

---

## ‡ß´. **Leave-P-Out Cross Validation**

- LOOCV-‡¶è‡¶∞ ‡¶Æ‡¶§ ‡¶§‡¶¨‡ßá ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¨‡¶æ‡¶∞ **P ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶ï ‡¶°‡ßá‡¶ü‡¶æ** test set ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ ‡¶π‡¶Ø‡¶º‡•§
- ‡¶¨‡¶°‡¶º ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø computationally ‡¶¨‡ßç‡¶Ø‡¶Ø‡¶º‡¶¨‡¶π‡ßÅ‡¶≤ ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§

### ‡¶¨‡ßà‡¶∂‡¶ø‡¶∑‡ßç‡¶ü‡ßç‡¶Ø:
- **‡¶®‡¶Æ‡¶®‡ßÄ‡¶Ø‡¶º‡¶§‡¶æ:** P ‡¶Æ‡¶æ‡¶® ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º
- **‡¶Ö‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:** ‡¶Ö‡¶®‡ßá‡¶ï ‡¶¨‡ßá‡¶∂‡¶ø computation
- **‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞:** ‡¶ñ‡ßÅ‡¶¨ ‡¶ï‡¶Æ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡¶Ø‡¶º

---

## ‡ß¨. **Time Series Cross Validation**

- Time series ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá ‡¶°‡ßá‡¶ü‡¶æ‡¶ï‡ßá ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§
- ‡¶Ø‡ßá‡¶Æ‡¶®: ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶≠‡¶æ‡¶ó train, ‡¶™‡¶∞‡ßá‡¶∞ ‡¶≠‡¶æ‡¶ó test‡•§ ‡¶è‡¶∞‡¶™‡¶∞ ‡¶¨‡¶°‡¶º train ‡¶∏‡ßá‡¶ü ‡¶ì ‡¶®‡¶§‡ßÅ‡¶® test ‡¶∏‡ßá‡¶ü ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§
- ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞ ‡¶°‡ßá‡¶ü‡¶æ‡¶Ø‡¶º ‡¶è‡¶ü‡¶ø ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£‡•§

### ‡¶¨‡ßà‡¶∂‡¶ø‡¶∑‡ßç‡¶ü‡ßç‡¶Ø:
- **‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶§‡ßç‡¶¨:** Time order ‡¶¨‡¶ú‡¶æ‡¶Ø‡¶º ‡¶∞‡¶æ‡¶ñ‡ßá
- **‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞:** Stock price, weather data ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø
- **‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨:** Data leakage ‡¶è‡¶°‡¶º‡¶æ‡¶§‡ßá ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡ßá

### Python ‡¶ï‡ßã‡¶° ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:
```python
from sklearn.model_selection import TimeSeriesSplit

# Time Series CV setup
tscv = TimeSeriesSplit(n_splits=5)
scores = cross_val_score(model, X, y, cv=tscv, scoring='neg_mean_squared_error')
```

---

## üéØ Cross Validation-‡¶è‡¶∞ ‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ

1. **Overfitting Detection:** ‡¶Æ‡¶°‡ßá‡¶≤ overfitting ‡¶ï‡¶∞‡¶õ‡ßá ‡¶ï‡¶ø‡¶®‡¶æ ‡¶¨‡ßÅ‡¶ù‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º
2. **Better Evaluation:** ‡¶è‡¶ï‡¶ï train-test split-‡¶è‡¶∞ ‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®
3. **Model Selection:** ‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶§‡ßÅ‡¶≤‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º
4. **Hyperparameter Tuning:** ‡¶∏‡ßá‡¶∞‡¶æ parameter ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º
5. **Confidence Interval:** Performance-‡¶è‡¶∞ uncertainty measure ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º

---

## ‚ö†Ô∏è ‡¶¨‡¶ø‡¶¨‡ßá‡¶ö‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º

### **‡¶ï‡ßã‡¶® CV ‡¶ï‡¶ñ‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨‡ßá‡¶®:**

| CV Type | Dataset Size | Problem Type | Time Complexity |
|---------|-------------|--------------|-----------------|
| Hold-Out | Large | Any | Low |
| K-Fold | Medium-Large | Any | Medium |
| Stratified K-Fold | Medium-Large | Classification | Medium |
| LOOCV | Small | Any | High |
| Time Series CV | Any | Time Series | Medium |

---

## üìä Practical Tips

### **1. K-Fold ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø K ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®:**
- **K=5:** ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§ ‡¶ì ‡¶Ø‡¶•‡ßá‡¶∑‡ßç‡¶ü ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞‡¶Ø‡ßã‡¶ó‡ßç‡¶Ø
- **K=10:** ‡¶Ü‡¶∞‡ßã ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞‡¶Ø‡ßã‡¶ó‡ßç‡¶Ø ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶ß‡ßÄ‡¶∞
- **‡¶¨‡¶°‡¶º K:** Bias ‡¶ï‡¶Æ ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ Variance ‡¶¨‡ßá‡¶∂‡¶ø

### **2. ‡¶°‡ßá‡¶ü‡¶æ Preprocessing:**
```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# Pipeline ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶® data leakage ‡¶è‡¶°‡¶º‡¶æ‡¶§‡ßá
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', LinearRegression())
])

scores = cross_val_score(pipeline, X, y, cv=5)
```

### **3. Custom Scoring:**
```python
from sklearn.metrics import make_scorer

# Custom scoring function
def custom_score(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))

custom_scorer = make_scorer(custom_score, greater_is_better=False)
scores = cross_val_score(model, X, y, cv=5, scoring=custom_scorer)
```

---

## üìà ‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡¶ø‡¶™‡ßç‡¶§ ‡¶§‡ßÅ‡¶≤‡¶®‡¶æ:

| ‡¶™‡¶¶‡ßç‡¶ß‡¶§‡¶ø | ‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ | ‡¶Ö‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ | ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ |
|--------|--------|---------|--------|
| **Hold-Out** | ‡¶∏‡¶π‡¶ú ‡¶ì ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§ | ‡¶ï‡¶Æ ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞‡¶Ø‡ßã‡¶ó‡ßç‡¶Ø | ‡¶¨‡¶°‡¶º ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü |
| **K-Fold** | ‡¶∏‡¶¨‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡¶ú‡¶®‡¶™‡ßç‡¶∞‡¶ø‡¶Ø‡¶º | ‡¶Æ‡¶æ‡¶ù‡¶æ‡¶∞‡¶ø ‡¶∏‡¶Æ‡¶Ø‡¶º | ‡¶∏‡¶¨ ‡¶ß‡¶∞‡¶®‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ |
| **Stratified K-Fold** | Class balance ‡¶¨‡¶ú‡¶æ‡¶Ø‡¶º ‡¶∞‡¶æ‡¶ñ‡ßá | Classification ‡¶è ‡¶∏‡ßÄ‡¶Æ‡¶ø‡¶§ | Classification |
| **LOOCV** | ‡¶õ‡ßã‡¶ü ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü‡ßá ‡¶≠‡¶æ‡¶≤‡ßã | ‡¶ñ‡ßÅ‡¶¨ ‡¶ß‡ßÄ‡¶∞ | ‡¶õ‡ßã‡¶ü ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü |
| **Time Series CV** | Time order ‡¶∞‡¶ï‡ßç‡¶∑‡¶æ ‡¶ï‡¶∞‡ßá | ‡¶ú‡¶ü‡¶ø‡¶≤ | Time series data |

---


**‡¶Æ‡¶®‡ßá ‡¶∞‡¶æ‡¶ñ‡¶¨‡ßá‡¶®:** Cross Validation ‡¶∂‡ßÅ‡¶ß‡ßÅ model evaluation ‡¶®‡¶Ø‡¶º, ‡¶è‡¶ü‡¶ø model selection ‡¶ì hyperparameter tuning-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø‡¶ì ‡¶Ö‡¶§‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£‡•§
