# ‡¶Æ‡¶æ‡¶≤‡ßç‡¶ü‡¶ø‡¶ï‡¶≤‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞‡¶ø‡¶ü‡¶ø (Multicollinearity)

‡¶Æ‡¶æ‡¶≤‡ßç‡¶ü‡¶ø‡¶ï‡¶≤‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞‡¶ø‡¶ü‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ statistical ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶Ø‡¶æ regression analysis-‡¶è ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶¶‡ßá‡¶Ø‡¶º‡•§ ‡¶è‡¶ü‡¶ø machine learning model-‡¶è‡¶∞ performance ‡¶è‡¶¨‡¶Ç interpretation-‡¶è ‡¶¨‡¶ø‡¶∞‡ßÇ‡¶™ ‡¶™‡ßç‡¶∞‡¶≠‡¶æ‡¶¨ ‡¶´‡ßá‡¶≤‡ßá‡•§

---

## üìå ‡¶Æ‡¶æ‡¶≤‡ßç‡¶ü‡¶ø‡¶ï‡¶≤‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞‡¶ø‡¶ü‡¶ø ‡¶ï‡ßÄ?

‡¶Æ‡¶æ‡¶≤‡ßç‡¶ü‡¶ø‡¶ï‡¶≤‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞‡¶ø‡¶ü‡¶ø ‡¶π‡¶≤‡ßã ‡¶∏‡ßá‡¶á ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ ‡¶Ø‡¶ñ‡¶® ‡¶¶‡ßÅ‡¶á ‡¶¨‡¶æ ‡¶§‡¶§‡ßã‡¶ß‡¶ø‡¶ï **independent variables** (features) ‡¶™‡¶∞‡¶∏‡ßç‡¶™‡¶∞‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá **‡¶â‡¶ö‡ßç‡¶ö ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ‡¶Ø‡¶º ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡¶ø‡¶§** (highly correlated) ‡¶•‡¶æ‡¶ï‡ßá‡•§

‡¶∏‡¶π‡¶ú ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º: ‡¶Ø‡¶ñ‡¶® ‡¶è‡¶ï‡¶ü‡¶ø feature ‡¶Ö‡¶®‡ßç‡¶Ø feature ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá predict ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º, ‡¶§‡¶ñ‡¶® multicollinearity ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶¶‡ßá‡¶Ø‡¶º‡•§

---

## üîç ‡¶Æ‡¶æ‡¶≤‡ßç‡¶ü‡¶ø‡¶ï‡¶≤‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞‡¶ø‡¶ü‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡¶æ‡¶∞‡¶≠‡ßá‡¶¶

### 1. **Perfect Multicollinearity (‡¶®‡¶ø‡¶ñ‡ßÅ‡¶Å‡¶§ ‡¶Æ‡¶æ‡¶≤‡ßç‡¶ü‡¶ø‡¶ï‡¶≤‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞‡¶ø‡¶ü‡¶ø)**
- ‡¶¶‡ßÅ‡¶á‡¶ü‡¶ø variable-‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ linear relationship
- Correlation coefficient = ¬±1
- ‡¶è‡¶ï‡¶ü‡¶ø variable ‡¶Ö‡¶®‡ßç‡¶Ø‡¶ü‡¶ø ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£‡¶≠‡¶æ‡¶¨‡ßá predict ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º

### 2. **Near Perfect Multicollinearity (‡¶™‡ßç‡¶∞‡¶æ‡¶Ø‡¶º ‡¶®‡¶ø‡¶ñ‡ßÅ‡¶Å‡¶§ ‡¶Æ‡¶æ‡¶≤‡ßç‡¶ü‡¶ø‡¶ï‡¶≤‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞‡¶ø‡¶ü‡¶ø)**
- ‡¶â‡¶ö‡ßç‡¶ö correlation ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶®‡¶Ø‡¶º
- Correlation coefficient > 0.8 ‡¶¨‡¶æ < -0.8
- ‡¶¨‡¶æ‡¶∏‡ßç‡¶§‡¶¨ ‡¶ú‡ßÄ‡¶¨‡¶®‡ßá ‡¶è‡¶ü‡¶ø‡¶á ‡¶¨‡ßá‡¶∂‡¶ø ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º

---

## üìä ‡¶ï‡ßÄ‡¶≠‡¶æ‡¶¨‡ßá ‡¶ö‡¶ø‡¶π‡ßç‡¶®‡¶ø‡¶§ ‡¶ï‡¶∞‡¶¨‡ßá‡¶®?

### 1. **Correlation Matrix**
```python
import pandas as pd
import seaborn as sns

# Correlation matrix ‡¶§‡ßà‡¶∞‡¶ø
corr_matrix = df.corr()
sns.heatmap(corr_matrix, annot=True)
```

### 2. **Variance Inflation Factor (VIF)**
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor

# VIF calculation
vif = pd.DataFrame()
vif["features"] = X.columns
vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
```

**VIF ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ:**
- VIF = 1: ‡¶ï‡ßã‡¶®‡ßã multicollinearity ‡¶®‡ßá‡¶á
- VIF = 1-5: ‡¶Æ‡¶æ‡¶ù‡¶æ‡¶∞‡¶ø multicollinearity
- VIF > 5: ‡¶â‡¶ö‡ßç‡¶ö multicollinearity (‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ‡¶ú‡¶®‡¶ï)
- VIF > 10: ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡¶∞ multicollinearity

### 3. **Condition Index**
- Eigenvalue-‡¶è‡¶∞ ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø‡ßá ‡¶ó‡¶£‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º
- Condition Index > 30 ‡¶π‡¶≤‡ßá multicollinearity ‡¶Ü‡¶õ‡ßá

---

## ‚ö†Ô∏è ‡¶Æ‡¶æ‡¶≤‡ßç‡¶ü‡¶ø‡¶ï‡¶≤‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞‡¶ø‡¶ü‡¶ø‡¶∞ ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ

### 1. **Coefficient-‡¶è‡¶∞ ‡¶Ö‡¶∏‡ßç‡¶•‡¶ø‡¶∞‡¶§‡¶æ**
- Coefficient-‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶Ö‡¶∏‡ßç‡¶•‡¶ø‡¶∞ ‡¶π‡¶Ø‡¶º‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º
- ‡¶õ‡ßã‡¶ü data ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶®‡ßá ‡¶¨‡¶°‡¶º coefficient ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶®

### 2. **Standard Error ‡¶¨‡ßÉ‡¶¶‡ßç‡¶ß‡¶ø**
- Coefficient-‡¶è‡¶∞ standard error ‡¶¨‡ßá‡¶°‡¶º‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º
- Statistical significance ‡¶ï‡¶Æ‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º

### 3. **‡¶≠‡ßÅ‡¶≤ Interpretation**
- Feature-‡¶è‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶™‡ßç‡¶∞‡¶≠‡¶æ‡¶¨ ‡¶¨‡ßÅ‡¶ù‡¶æ ‡¶ï‡¶†‡¶ø‡¶® ‡¶π‡¶Ø‡¶º
- Model interpretation ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ‡¶ú‡¶®‡¶ï

### 4. **Prediction ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ**
- Model overfitting ‡¶è‡¶∞ ‡¶ù‡ßÅ‡¶Å‡¶ï‡¶ø ‡¶¨‡¶æ‡¶°‡¶º‡ßá
- New data-‡¶è poor performance

---

## üõ†Ô∏è ‡¶∏‡¶Æ‡¶æ‡¶ß‡¶æ‡¶®‡ßá‡¶∞ ‡¶â‡¶™‡¶æ‡¶Ø‡¶º

### 1. **Feature Selection**
```python
# Highly correlated features ‡¶¨‡¶æ‡¶¶ ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ
threshold = 0.8
corr_matrix = df.corr().abs()
upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]
df_cleaned = df.drop(columns=to_drop)
```

### 2. **Principal Component Analysis (PCA)**
```python
from sklearn.decomposition import PCA

pca = PCA(n_components=0.95)  # 95% variance ‡¶∞‡¶æ‡¶ñ‡¶æ
X_pca = pca.fit_transform(X)
```

### 3. **Regularization Techniques**
- **Ridge Regression (L2)**: Coefficient shrinkage
- **Lasso Regression (L1)**: Feature selection + shrinkage
- **Elastic Net**: Ridge + Lasso combination

### 4. **Feature Engineering**
- Combined features ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ
- Derived features ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ

---

## üìà ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ (‡¶¨‡¶æ‡¶∏‡ßç‡¶§‡¶¨ ‡¶ú‡ßÄ‡¶¨‡¶®)

### **‡¶¨‡¶æ‡¶°‡¶º‡¶ø‡¶∞ ‡¶¶‡¶æ‡¶Æ ‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡¶æ‡¶≠‡¶æ‡¶∏:**

**Multicollinear Features:**
- ‡¶¨‡¶æ‡¶°‡¶º‡¶ø‡¶∞ ‡¶Ü‡¶Ø‡¶º‡¶§‡¶® (Square feet)
- ‡¶ï‡¶ï‡ßç‡¶∑‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ (Number of rooms)
- ‡¶¨‡¶æ‡¶•‡¶∞‡ßÅ‡¶Æ‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ (Number of bathrooms)

‡¶è‡¶ó‡ßÅ‡¶≤‡ßã ‡¶™‡¶∞‡¶∏‡ßç‡¶™‡¶∞ ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡¶ø‡¶§ ‡¶ï‡¶æ‡¶∞‡¶£:
- ‡¶¨‡¶°‡¶º ‡¶¨‡¶æ‡¶°‡¶º‡¶ø‡¶§‡ßá ‡¶¨‡ßá‡¶∂‡¶ø ‡¶ï‡¶ï‡ßç‡¶∑ ‡¶•‡¶æ‡¶ï‡ßá
- ‡¶¨‡ßá‡¶∂‡¶ø ‡¶ï‡¶ï‡ßç‡¶∑ ‡¶Æ‡¶æ‡¶®‡ßá ‡¶¨‡ßá‡¶∂‡¶ø ‡¶¨‡¶æ‡¶•‡¶∞‡ßÅ‡¶Æ

**‡¶∏‡¶Æ‡¶æ‡¶ß‡¶æ‡¶®:**
- ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶Ü‡¶Ø‡¶º‡¶§‡¶® ‡¶∞‡¶æ‡¶ñ‡¶æ
- ‡¶Ö‡¶•‡¶¨‡¶æ "‡¶ï‡¶ï‡ßç‡¶∑ ‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡¶ó‡¶°‡¶º ‡¶Ü‡¶Ø‡¶º‡¶§‡¶®" ‡¶®‡¶§‡ßÅ‡¶® feature ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ

---

## üéØ ‡¶ï‡¶ñ‡¶® Multicollinearity ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶®‡¶Ø‡¶º?

### 1. **Prediction Focus**
- ‡¶Ø‡¶¶‡¶ø ‡¶∂‡ßÅ‡¶ß‡ßÅ prediction accuracy ‡¶ö‡¶æ‡¶®
- Interpretation ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶®‡¶Ø‡¶º

### 2. **Control Variables**
- ‡¶Ø‡¶¶‡¶ø multicollinear variables control variable ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡¶Ø‡¶º

### 3. **Large Dataset**
- ‡¶ñ‡ßÅ‡¶¨ ‡¶¨‡¶°‡¶º dataset-‡¶è ‡¶ï‡¶Æ ‡¶™‡ßç‡¶∞‡¶≠‡¶æ‡¶¨

---

## üìù ‡¶ö‡ßá‡¶ï‡¶≤‡¶ø‡¶∏‡ßç‡¶ü

**Multicollinearity Detection:**
- [ ] Correlation matrix analysis
- [ ] VIF calculation (> 5 ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ‡¶ú‡¶®‡¶ï)
- [ ] Condition index check (> 30 ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ‡¶ú‡¶®‡¶ï)

**Solutions:**
- [ ] Remove highly correlated features
- [ ] Use PCA for dimensionality reduction
- [ ] Apply regularization (Ridge/Lasso)
- [ ] Create new engineered features

---

## üîó ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø Topics-‡¶è‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï

- **Ridge Regression**: Multicollinearity ‡¶∏‡¶Æ‡¶æ‡¶ß‡¶æ‡¶®‡ßá ‡¶ï‡¶æ‡¶∞‡ßç‡¶Ø‡¶ï‡¶∞
- **Feature Selection**: Correlation-based selection
- **PCA**: Dimensionality reduction technique
- **Model Validation**: Cross-validation importance

---

**‡¶Æ‡¶®‡ßá ‡¶∞‡¶æ‡¶ñ‡¶¨‡ßá‡¶®:** Multicollinearity ‡¶∏‡¶¨ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶®‡¶Ø‡¶º‡•§ ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ model-‡¶è‡¶∞ ‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶∏‡¶ø‡¶¶‡ßç‡¶ß‡¶æ‡¶®‡ßç‡¶§ ‡¶®‡¶ø‡¶®‡•§