
---

# üéØ Logistic Regression ‚Äî **OVR (One-Vs-Rest / One-Vs-All)** ‡¶ï‡ßÄ?

‡¶Ø‡¶ñ‡¶® Logistic Regression ‡¶¶‡¶ø‡ßü‡ßá **multi-class classification** ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡ßü, ‡¶§‡¶ñ‡¶® ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶∏‡¶π‡¶ú ‡¶è‡¶¨‡¶Ç ‡¶ú‡¶®‡¶™‡ßç‡¶∞‡¶ø‡ßü ‡¶™‡¶¶‡ßç‡¶ß‡¶§‡¶ø ‡¶π‡¶≤‡ßã **OVR**‡•§

‡¶è‡¶ü‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶π‡ßü ‡¶Ø‡¶ñ‡¶® ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‚â• 3 ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶Æ‡¶∞‡¶æ Binary Logistic Regression-‡¶è‡¶∞ ‡¶∂‡¶ï‡ßç‡¶§‡¶ø‡¶ï‡ßá ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá multi-class ‡¶ï‡¶∞‡¶§‡ßá ‡¶ö‡¶æ‡¶á‡•§

### ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:

‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ = **A, B, C**

OVR ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶ï‡¶∞‡ßá ‡ß©‡¶ü‡¶ø Logistic Regression model ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶¨‡ßá:

* Model 1 ‚Üí A vs (B, C)
* Model 2 ‚Üí B vs (A, C)
* Model 3 ‚Üí C vs (A, B)

‡¶∂‡ßá‡¶∑‡ßá ‡¶Ø‡ßá‡¶á ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßá‡¶∞ probability ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶¨‡ßá‡¶∂‡¶ø ‚Üí ‡¶∏‡ßá‡¶ü‡¶æ‡¶á prediction‡•§

---

![Image](https://media.geeksforgeeks.org/wp-content/uploads/20230330170812/Screenshot-2023-03-30-170740.png?utm_source=chatgpt.com)

![Image](https://miro.medium.com/v2/resize%3Afit%3A788/1%2Au9Kj9xXuGXiu8RJqwCGtig.png?utm_source=chatgpt.com)

---

# üß† ‡¶ï‡ßá‡¶® OVR ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶®?

Binary Logistic Regression ‡¶∂‡ßÅ‡¶ß‡ßÅ **‡¶¶‡ßÅ‡¶á ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏** ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§

OVR ‡¶∏‡ßá‡¶á Binary ‡¶Æ‡¶°‡ßá‡¶≤‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶∂‡¶ï‡ßç‡¶§‡¶ø‡¶ï‡ßá ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá multi-class classification ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá:

[
K \text{ classes } \Rightarrow K \text{ binary classifiers}
]

---

# üî• OVR ‡¶ï‡ßÄ‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá? (Step-by-Step)

‡¶ß‡¶∞‡ßÅ‡¶® ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ = 3 ‚Üí {A, B, C}

---

## **1Ô∏è‚É£ Step ‚Äî ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø Binary Model ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ**

### Model 1: A vs Not-A

* A ‚Üí 1
* B, C ‚Üí 0

### Model 2: B vs Not-B

* B ‚Üí 1
* A, C ‚Üí 0

### Model 3: C vs Not-C

* C ‚Üí 1
* A, B ‚Üí 0

Thus ‚Üí 3 logistic regression models‡•§

---

## **2Ô∏è‚É£ Step ‚Äî ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶è‡¶ï‡¶ü‡¶ø probability ‡¶¶‡ßá‡ßü**

Example prediction for a sample:

| Model     | Probability |
| --------- | ----------- |
| A vs rest | 0.10        |
| B vs rest | 0.62        |
| C vs rest | 0.28        |

---

## **3Ô∏è‚É£ Step ‚Äî Highest Probability = Predicted Class**

Prediction = **B**

---

# üìå Mathematical View (OVR)

For a test sample (x):

[
P(y=k|x) = \sigma(w_k^T x + b_k)
]

Prediction:

[
\hat{y} = \arg\max_k P(y=k|x)
]

‡¶è‡¶ñ‡¶æ‡¶®‡ßá (K) = total number of classes‡•§

---

# üîç OVR vs Softmax (Multinomial Logistic Regression)

| ‡¶¨‡¶ø‡¶∑‡ßü                 | OVR                                 | Multinomial (Softmax) |
| -------------------- | ----------------------------------- | --------------------- |
| Number of Models     | K binary models                     | One model             |
| Complexity           | Simple                              | More complex          |
| Output probabilities | Independent probabilities           | Sum to 1              |
| Best use case        | High-dimensional sparse data (text) | Small/medium datasets |
| Training time        | Faster                              | Slower                |
| Accuracy             | Lower (some cases)                  | Higher                |

Softmax model mathematically superior, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ OVR computation-wise ‡¶∏‡¶π‡¶ú ‡¶è‡¶¨‡¶Ç scalable‡•§

---

# üìö ‡¶¨‡¶æ‡¶∏‡ßç‡¶§‡¶¨ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ (Bangla)

‡¶ß‡¶∞‡ßÅ‡¶® ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶´‡¶≤ classify ‡¶ï‡¶∞‡¶õ‡¶ø:

‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ = {Apple, Banana, Mango}

‡¶§‡¶æ‡¶π‡¶≤‡ßá OVR ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá:

### Model 1: Apple vs Others

### Model 2: Banana vs Others

### Model 3: Mango vs Others

Prediction ‡¶è‡¶∞ output:

* Apple ‚Üí 0.25
* Banana ‚Üí 0.15
* Mango ‚Üí 0.60

‡¶§‡¶¨‡ßá ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ = **Mango**

---

# üß™ Python Example (sklearn) ‚Äî OVR Mode

Sklearn-‡¶è Logistic Regression default ‡¶≠‡¶æ‡¶¨‡ßá OVR ‡¶ï‡¶∞‡ßá‡•§

```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True)

model = LogisticRegression(multi_class='ovr', solver='liblinear')
model.fit(X, y)

print(model.predict([[5.2, 3.4, 1.5, 0.2]]))
```

‡¶Ø‡¶¶‡¶ø multi_class='ovr' ‡¶®‡¶æ ‡¶¶‡¶ø‡¶á ‚Üí ‡¶ï‡¶ø‡¶õ‡ßÅ solver automatically OVR ‡¶®‡ßá‡ßü‡•§

---

# ‚≠ê OVR-‡¶è‡¶∞ ‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ ‡¶ì ‡¶Ö‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ

## ‚úî ‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ

* Simple to implement
* Training fast
* Binary Logistic Regression reuse ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡ßü
* High-dimensional sparse data (like text classification)-‡¶è ‡¶ñ‡ßÅ‡¶¨ ‡¶ï‡¶æ‡¶∞‡ßç‡¶Ø‡¶ï‡¶∞
* Debugging ‡¶∏‡¶π‡¶ú (‡¶ï‡ßã‡¶® ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá ‡¶¨‡ßã‡¶ù‡¶æ ‡¶Ø‡¶æ‡ßü)

## ‚ùå ‡¶Ö‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ

* Probabilities sum to 1 ‡¶π‡ßü ‡¶®‡¶æ (softmax-‡¶è‡¶∞ ‡¶Æ‡¶§‡ßã proper probability distribution ‡¶®‡¶æ)
* ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶∏‡¶Æ‡ßü overlapping decision boundary ‡¶§‡ßà‡¶∞‡¶ø ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá
* Multinomial logistic regression ‡¶§‡ßÅ‡¶≤‡¶®‡¶æ‡ßü accuracy ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶ï‡¶Æ ‡¶π‡ßü

---

# üß† Summary (Short Notes)

* OVR = Multi-class classification-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø K binary logistic regressions
* ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø model predict ‡¶ï‡¶∞‡ßá P(class=k | x)
* Final prediction = highest probability
* Simple, fast, interpretable
* ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ softmax-‡¶è‡¶∞ ‡¶Æ‡¶§‡ßã mathematically elegant ‡¶®‡ßü

---


