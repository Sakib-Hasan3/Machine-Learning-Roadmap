

---

# ЁЯза **Logistic Regression тАФ Deep Mathematical Intuition**

Logistic Regression ржорзВрж▓ржд ржПржХржЯрж┐ **probabilistic classification model**, ржпрж╛ **log-odds** (logit) ржХрзЗ linear function ржПрж░ рж╕рж╛ржерзЗ рж╕ржВржпрзБржХрзНржд ржХрж░рзЗред
ржПрж░ ржнрзЗрждрж░рзЗрж░ ржЧржгрж┐ржд ржмрзБржЭрж▓рзЗ ржкрзБрж░рзЛ ржоржбрзЗрж▓ржЯрж┐ crystal clear рж╣рзЯрзЗ ржпрж╛рзЯред

---

# ЁЯФе PART 1 тАФ ржХрзЗржи Logistic Regression рж▓рж╛ржЧрзЗ?

ржЖржорж╛ржжрзЗрж░ рж▓ржХрзНрж╖рзНржп:
[
P(y = 1 \mid x)
]

ржПржЦржи ржпржжрж┐ Linear Regression ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж┐:
[
y = w^Tx + b
]

ржХрж┐ржирзНрждрзБ ржПржЯрж┐ probability рж╣рждрзЗ ржкрж╛рж░рзЗ ржирж╛, ржХрж╛рж░ржг:

* output negative рж╣рждрзЗ ржкрж╛рж░рзЗ
* 1 ржПрж░ ржмрзЗрж╢рж┐ рж╣рждрзЗ ржкрж╛рж░рзЗ
* probability interpretation ржирзЗржЗ

тЮб **Probability тЖТ 0 ржерзЗржХрзЗ 1 ржПрж░ ржоржзрзНржпрзЗ ржЪрж╛ржЗред**

рждрж╛ржЗ ржЖржорж░рж╛ Sigmoid ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж┐:

---

# ЁЯФе PART 2 тАФ Sigmoid Function ржЖрж╕рж▓рзЗ ржХрзА?

Sigmoid рж╣рж▓ probability-ржПрж░ perfect mapping:
[
\sigma(z) = \frac{1}{1+e^{-z}}
]

ржпрзЗржЦрж╛ржирзЗ
[
z = w^Tx + b
]

### ржХрзЗржи Sigmoid ржирж┐рж░рзНржмрж╛ржЪржи ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗ?

тЬФ Monotonic (ржзрзАрж░рзЗ ржзрзАрж░рзЗ ржмрж╛рзЬрзЗ)
тЬФ Output тИИ (0,1)
тЬФ Differentiable тЖТ gradient descent-ржП рж╕рзБржмрж┐ржзрж╛
тЬФ Good probabilistic interpretation
тЬФ Log-odds ржПрж░ inverse function

ржПржЗ "log-odds" ржЗ рж╣рж▓рзЛ Logistic Regression-ржПрж░ ржорзВрж▓ intuitionЁЯСЗ

---

# ЁЯФе PART 3 тАФ Odds & Log-Odds (Logit Function)

**Odds** рж╣рж▓рзЛ:
[
\text{Odds} = \frac{P}{1-P}
]

Odds = probability of success / probability of failure

ржПржЦржи Log(odds):
[
\text{Logit}(P) = \log\left(\frac{P}{1-P}\right)
]

тЭЧ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржмрж┐рж╖рзЯ:
**ржПржЗ logit function-ржПрж░ range тИТтИЮ ржерзЗржХрзЗ +тИЮ** тЖТ рждрж╛ржЗ ржПржЯрж┐ржХрзЗ linear ржХрж░рждрзЗ рж╕рзБржмрж┐ржзрж╛ред

ржПржЬржирзНржп Logistic Regression ржзрж░рзЗ ржирзЗрзЯ:
[
\log\left(\frac{P}{1-P}\right) = w^Tx + b
]

тЮб ржПржЯрж┐ржЗ Logistic Regression-ржПрж░ ржорзВрж▓ equation
тЮб ржПржХрзЗ ржмрж▓рзЗ **Log-Odds = Linear Function**

---

# ЁЯФе PART 4 тАФ Model Equation ржерзЗржХрзЗ Sigmoid ржмрзЗрж░ рж╣рзЯ ржХрзАржнрж╛ржмрзЗ?

Logit form ржерзЗржХрзЗ P ржмрзЗрж░ ржХрж░рж▓рзЗ logistic (sigmoid) function ржкрж╛ржУрзЯрж╛ ржпрж╛рзЯред

**Start:**
[
\log\left(\frac{P}{1-P}\right) = z
]

Exponentiate:
[
\frac{P}{1-P} = e^z
]

Cross multiply:
[
P = e^z(1-P)
]
[
P = e^z - P e^z
]
[
P + P e^z = e^z
]
[
P(1 + e^z) = e^z
]
[
P = \frac{e^z}{1 + e^z}
]

Now divide numerator & denominator by (e^z):
[
P = \frac{1}{1 + e^{-z}}
]

ЁЯОЙ ржжрзЗржЦрж╛ ржпрж╛ржЪрзНржЫрзЗ тАФ Sigmoid Equation ржирж┐ржЬрзЗ ржерзЗржХрзЗржЗ ржмрзЗрж░ рж╣рзЯрзЗ ржЧрзЗрж▓!

---

# ЁЯФе PART 5 тАФ Cost Function ржХрзЗржи MSE ржирзЯ?

ржпржжрж┐ MSE cost ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж┐:
[
\text{MSE} = (y - h(x))^2
]

рждрж╛рж╣рж▓рзЗ gradient descent ржЦрзБржм slow рж╣рзЯ тЖТ sigmoid derivative ржХрж╛рж░ржгрзЗ vanishing gradient рждрзИрж░рж┐ рж╣рзЯред

Probability modeling-ржПрж░ ржЬржирзНржп ржЖржжрж░рзНрж╢ cost рж╣рж▓рзЛ:

---

# ЁЯФе PART 6 тАФ Cross Entropy (Log-Loss)

Binary logistic regression-ржПрж░ cost:
[
J = -\left[y\log(h(x)) + (1-y)\log(1-h(x))\right]
]

Why this cost?

### ржпржжрж┐ y = 1

Cost = тИТlog(h(x))

* h(x) = 1 тЖТ cost = 0
* h(x) = 0 тЖТ cost тЖТ тИЮ

Perfect!

### ржпржжрж┐ y = 0

Cost = тИТlog(1 тИТ h(x))

ржПржЗ cost ржирж┐рж╢рзНржЪрж┐ржд ржХрж░рзЗ:

тЬФ ржнрзБрж▓ рж╣рж▓рзЗ ржмрзЬ ржкрзЗржирж╛рж▓рзНржЯрж┐
тЬФ рж╕ржарж┐ржХ рж╣рж▓рзЗ ржХржо ржкрзЗржирж╛рж▓рзНржЯрж┐
тЬФ convex shape тАФ optimization рж╕рж╣ржЬ
тЬФ probabilistic modeling consistent ржерж╛ржХрзЗ

---

# ЁЯФе PART 7 тАФ Gradient Descent Update

We minimize J by updating weights:

Weights update rule:
[
w := w - \alpha \frac{\partial J}{\partial w}
]

Derivative:
[
\frac{\partial J}{\partial w} = (h(x) - y)x
]

Bias:
[
b := b - \alpha(h(x)-y)
]

ржжрзЗржЦрзБржи:
**h(x) тИТ y** рж╣рж▓рзЛ error term тЖТ prediction тИТ reality

---

# ЁЯФе PART 8 тАФ Decision Boundary ржХрзА?

Logistic regression decision boundary рж▓рж┐ржирж┐рзЯрж╛рж░ ржХрж╛рж░ржг:
[
z = w^Tx + b = 0
]

ржпрзЗржЦрж╛ржирзЗ:

* z тЙе 0 тЖТ P тЙе 0.5 тЖТ class = 1
* z < 0 тЖТ P < 0.5 тЖТ class = 0

Hence a **linear boundary**.

---

# ЁЯза SUMMARY: Logistic Regression Intuition in 5 Lines

1. Probability ржмрзЗрж░ ржХрж░рждрзЗ ржЪрж╛ржЗ тЖТ Sigmoid ржжрж░ржХрж╛рж░
2. Sigmoid ржЖрж╕рж▓рзЗ log-odds-ржПрж░ inverse
3. Log-odds ржХрзЗ linear ржзрж░рж╛рзЯ model simple ржерж╛ржХрзЗ
4. Cross entropy probability modeling-ржПрж░ ржЬржирзНржп perfect cost
5. Gradient descent error ржЕржирзБржпрж╛рзЯрзА weights ржЖржкржбрзЗржЯ ржХрж░рзЗ

