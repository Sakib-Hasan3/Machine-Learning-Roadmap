# ЁЯФД Convergence Algorithm (ржХржиржнрж╛рж░рзНржЬрзЗржирзНрж╕ ржЕрзНржпрж╛рж▓ржЧрж░рж┐ржжржо) тАФ ржмрж╛ржВрж▓рж╛ржпрж╝ рж╕рж╣ржЬ ржмрзНржпрж╛ржЦрзНржпрж╛

---

## ЁЯУШ ржХржиржнрж╛рж░рзНржЬрзЗржирзНрж╕ ржХрзА?

ржорзЗрж╢рж┐ржи рж▓рж╛рж░рзНржирж┐ржВ-ржП **ржХржиржнрж╛рж░рзНржЬрзЗржирзНрж╕** (Convergence) ржорж╛ржирзЗ рж╣рж▓рзЛ тАФ ржпржЦржи ржХрзЛржирзЛ ржЕрзНржпрж╛рж▓ржЧрж░рж┐ржжржо (ржпрзЗржоржи: Gradient Descent) ржмрж╛рж░ржмрж╛рж░ ржЖржкржбрзЗржЯ рж╣рждрзЗ рж╣рждрзЗ ржПржоржи ржПржХ ржЕржмрж╕рзНржерж╛ржпрж╝ ржкрзМржБржЫрж╛ржпрж╝, ржпрзЗржЦрж╛ржирзЗ Cost Function ржЖрж░ ржЦрзБржм ржПржХржЯрж╛ ржХржоржЫрзЗ ржирж╛, ржЕрж░рзНржерж╛рзО рж╕ржорж╛ржзрж╛ржи рж╕рзНржерж┐рждрж┐рж╢рзАрж▓ (Stable) рж╣ржпрж╝рзЗ ржЧрзЗржЫрзЗред

---

## ЁЯФ╣ ржХржиржнрж╛рж░рзНржЬрзЗржирзНрж╕ ржЕрзНржпрж╛рж▓ржЧрж░рж┐ржжржорзЗрж░ ржзрж╛ржк

1. **рж╢рзБрж░рзБ:** ржкрзНрж░рж╛ржержорж┐ржХ ржорж╛ржи (Initial value) ржжрж┐ржпрж╝рзЗ рж╢рзБрж░рзБ
2. **Cost Function рж╣рж┐рж╕рж╛ржм:** ржмрж░рзНрждржорж╛ржи ржорж╛ржирзЗ Cost Function ржХржд, рждрж╛ ржмрзЗрж░ ржХрж░рж╛
3. **Parameter ржЖржкржбрзЗржЯ:** ржЕрзНржпрж╛рж▓ржЧрж░рж┐ржжржо ржЕржирзБржпрж╛ржпрж╝рзА (ржпрзЗржоржи: Gradient Descent) ржирждрзБржи ржорж╛ржи рж╣рж┐рж╕рж╛ржм
4. **Check:** Cost Function ржЖржЧрзЗрж░ ржЪрзЗржпрж╝рзЗ ржХржорзЗржЫрзЗ ржХрж┐ржирж╛ ржжрзЗржЦрзЗ
5. **Repeat:** Cost Function ржЦрзБржм ржХржо ржкрж░рж┐ржмрж░рзНрждржи рж╣рж▓рзЗ ржерзЗржорзЗ ржпрж╛ржпрж╝ (Converged)

---

## ЁЯзо ржЙржжрж╛рж╣рж░ржг: Gradient Descent-ржП ржХржиржнрж╛рж░рзНржЬрзЗржирзНрж╕

- ржЖржорж░рж╛ ржЪрж╛ржЗ Cost Function ржпрждржЯрж╛ рж╕ржорзНржнржм ржХржорж╛ржирзЛ
- ржкрзНрж░рждрж┐ржмрж╛рж░ ржкрзНржпрж╛рж░рж╛ржорж┐ржЯрж╛рж░ (╬╕) ржЖржкржбрзЗржЯ ржХрж░рж┐:

$$
\theta_{new} = \theta_{old} - \alpha \cdot \nabla J(\theta_{old})
$$

- ржПржЦрж╛ржирзЗ $\alpha$ рж╣рж▓рзЛ Learning Rate
- $\nabla J(\theta)$ рж╣рж▓рзЛ Cost Function-ржПрж░ Gradient

### **ржХржиржнрж╛рж░рзНржЬрзЗржирзНрж╕ ржХржЦржи рж╣ржпрж╝?**
- ржпржЦржи $|J(\theta_{new}) - J(\theta_{old})| < \epsilon$ (ржЦрзБржм ржЫрзЛржЯ)
- ржЕрж░рзНржерж╛рзО Cost Function ржЖрж░ ржХржоржЫрзЗ ржирж╛

---

## ЁЯУИ ржХржиржнрж╛рж░рзНржЬрзЗржирзНрж╕ ржЪрзЗржирж╛рж░ ржЙржкрж╛ржпрж╝

- **Cost Function-ржПрж░ ржорж╛ржи ржкрзНрж▓ржЯ ржХрж░рж▓рзЗ:**
  - рж╢рзБрж░рзБрждрзЗ ржжрзНрж░рзБржд ржХржорзЗ
  - рж╢рзЗрж╖рзЗ рж╕ржорждрж▓ (flat) рж╣ржпрж╝рзЗ ржпрж╛ржпрж╝
- **Gradient (ржврж╛рж▓) ржЦрзБржм ржЫрзЛржЯ рж╣рж▓рзЗ:**
  - $|\nabla J(\theta)|$ тЙИ 0
- **Iteration рж╕ржВржЦрзНржпрж╛:**
  - ржирж┐рж░рзНржжрж┐рж╖рзНржЯ рж╕ржВржЦрзНржпржХ iteration ржкрж░ ржерзЗржорзЗ ржпрж╛ржпрж╝

---

## ЁЯОп ржХржиржнрж╛рж░рзНржЬрзЗржирзНрж╕рзЗ ржХрзА рж╕ржорж╕рзНржпрж╛ рж╣рждрзЗ ржкрж╛рж░рзЗ?

- **Local Minima:** Cost Function ржирзНржпрзВржирждржо рж╣рж▓рзЗржУ рж╕рзЗржЯрж┐ Global ржирж╛ржУ рж╣рждрзЗ ржкрж╛рж░рзЗ
- **Overshooting:** Learning Rate ржмрзЗрж╢рж┐ рж╣рж▓рзЗ рж╕ржорж╛ржзрж╛ржи рж▓рж╛ржлрж┐ржпрж╝рзЗ ржпрж╛ржпрж╝
- **Slow Convergence:** Learning Rate ржХржо рж╣рж▓рзЗ ржЕржирзЗржХ рж╕ржоржпрж╝ рж▓рж╛ржЧрзЗ

---

## ЁЯЫая╕П ржмрзНржпржмрж╣рж╛рж░

- **Gradient Descent**
- **Newton's Method**
- **Stochastic Gradient Descent (SGD)**
- **Optimization Algorithms**

---

## тЬЕ ржЙржкрж╕ржВрж╣рж╛рж░

- ржХржиржнрж╛рж░рзНржЬрзЗржирзНрж╕ ржорж╛ржирзЗ Cost Function рж╕рзНржерж┐рждрж┐рж╢рзАрж▓ рж╣ржУржпрж╝рж╛
- ржнрж╛рж▓рзЛ ржЕрзНржпрж╛рж▓ржЧрж░рж┐ржжржо ржжрзНрж░рзБржд ржУ рж╕ржарж┐ржХржнрж╛ржмрзЗ ржХржиржнрж╛рж░рзНржЬ ржХрж░рзЗ
- Learning Rate, Initial Value, Cost Function-ржПрж░ ржзрж░ржи тАФ рж╕ржмржХрж┐ржЫрзБ ржХржиржнрж╛рж░рзНржЬрзЗржирзНрж╕рзЗ ржкрзНрж░ржнрж╛ржм ржлрзЗрж▓рзЗ

---

*┬й 2025 - Convergence Algorithm ржмрж╛ржВрж▓рж╛ ржЯрж┐ржЙржЯрзЛрж░рж┐ржпрж╝рж╛рж▓*
