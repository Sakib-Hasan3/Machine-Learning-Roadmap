# Random Forest Regression Problem

## ğŸŒ³ Random Forest Regression â€” Diagram (Intuition)

![Image](https://cdn.prod.website-files.com/5e6f9b297ef3941db2593ba1/5f6315207ab68b113cf57b1c_Screenshot%202020-09-17%20at%2009.49.20.png)

![Image](https://www.researchgate.net/publication/303835073/figure/fig3/AS%3A377949833449472%401467121670301/The-flowchart-of-random-forest-RF-for-regression-adapted-from-Rodriguez-Galiano-et.png)

[Image](https://miro.medium.com/v2/resize%3Afit%3A1400/0%2AYEwFetXQGPB8aDFV)

---

# 1ï¸âƒ£ Random Forest Regression â€” Python / Sklearn Code

### ğŸ”¹ à¦‰à¦¦à¦¾à¦¹à¦°à¦£ à¦¸à¦®à¦¸à§à¦¯à¦¾

ğŸ‘‰ **House Price Prediction** (continuous value â†’ regression)

---

## ğŸ“Œ Step-1: Libraries Import

```python
import numpy as np
import pandas as pd

from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

```

---

## ğŸ“Œ Step-2: Dataset à¦¤à§ˆà¦°à¦¿

```python
# Synthetic regression dataset
X, y = make_regression(
    n_samples=1000,
    n_features=6,
    noise=20,
    random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)

X_train.shape, X_test.shape

```

**Output**

```
((750, 6), (250, 6))

```

---

## ğŸ“Œ Step-3: Random Forest Regression (Default)

```python
rf = RandomForestRegressor(
    n_estimators=200,
    random_state=42,
    n_jobs=-1
)

rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)

print("MAE:", mean_absolute_error(y_test, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))
print("R2 Score:", r2_score(y_test, y_pred))

```

**Example Output**

```
MAE: 14.8
RMSE: 18.9
R2 Score: 0.92

```

---

# 2ï¸âƒ£ Hyperparameter Tuning (Bangla Explanation + Code)

## ğŸ”§ Hyperparameter Tuning à¦•à§€?

ğŸ‘‰ Model train à¦•à¦°à¦¾à¦° **à¦†à¦—à§‡** à¦¯à§‡à¦¸à¦¬ à¦¸à§‡à¦Ÿà¦¿à¦‚ à¦ à¦¿à¦• à¦•à¦°à¦¾ à¦¹à§Ÿ

ğŸ‘‰ à¦¸à§‡à¦—à§à¦²à§‹à¦° **best combination** à¦¬à§‡à¦° à¦•à¦°à¦¾à¦° à¦ªà§à¦°à¦•à§à¦°à¦¿à§Ÿà¦¾à¦‡ **Hyperparameter Tuning**

---

## ğŸ›ï¸ Random Forest Regression-à¦à¦° à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£ Hyperparameters

| Hyperparameter | à¦•à¦¾à¦œ |
| --- | --- |
| `n_estimators` | à¦•à§Ÿà¦Ÿà¦¾ tree à¦¹à¦¬à§‡ |
| `max_depth` | tree à¦•à¦¤ à¦—à¦­à§€à¦° |
| `min_samples_split` | split à¦•à¦°à¦¾à¦° minimum data |
| `min_samples_leaf` | leaf-à¦ minimum data |
| `max_features` | à¦ªà§à¦°à¦¤à¦¿ split-à¦ feature à¦¸à¦‚à¦–à§à¦¯à¦¾ |

---

## ğŸ“Œ GridSearchCV à¦¦à¦¿à§Ÿà§‡ Tuning

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    "n_estimators": [100, 200],
    "max_depth": [None, 10, 20],
    "min_samples_leaf": [1, 3]
}

grid = GridSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_grid=param_grid,
    cv=3,
    scoring="r2",
    n_jobs=-1
)

grid.fit(X_train, y_train)

grid.best_params_, grid.best_score_

```

**Example Output**

```
({'max_depth': 20, 'min_samples_leaf': 1, 'n_estimators': 200}, 0.94)

```

---

## ğŸ“Œ Tuned Model à¦¦à¦¿à§Ÿà§‡ Test

```python
best_rf = grid.best_estimator_

y_pred_tuned = best_rf.predict(X_test)

print("R2 Score (Tuned):", r2_score(y_test, y_pred_tuned))

```

**Output**

```
R2 Score (Tuned): 0.94

```

ğŸ“Œ **Tuning à¦•à¦°à¦¾à¦° à¦ªà¦° performance à¦¬à§‡à§œà§‡à¦›à§‡ âœ…**

---

# 3ï¸âƒ£ Numerical Example (à¦–à§à¦¬ à¦¸à¦¹à¦œ)

à¦§à¦°à¦¾ à¦¯à¦¾à¦• **à¦à¦•à¦Ÿà¦¾ à¦¬à¦¾à§œà¦¿à¦° à¦¦à¦¾à¦®** predict à¦•à¦°à¦¤à§‡ 5à¦Ÿà¦¾ tree à¦¨à¦¿à¦šà§‡à¦° à¦®à¦¾à¦¨ à¦¦à¦¿à¦²à§‹:

| Tree | Prediction |
| --- | --- |
| Tree-1 | 52,000 |
| Tree-2 | 54,000 |
| Tree-3 | 51,000 |
| Tree-4 | 53,000 |
| Tree-5 | 55,000 |

### ğŸ‘‰ Final Prediction

[

\frac{52000 + 54000 + 51000 + 53000 + 55000}{5} = 53,000

]

ğŸ“Œ à¦à¦Ÿà¦¾à¦‡ **Random Forest Regression**

---

```
Random Forest Regression
------------------------
â€¢ Ensemble learning technique
â€¢ Uses multiple regression trees
â€¢ Final prediction = average of all tree predictions
â€¢ Based on Bagging (Bootstrap Aggregating)

Why use?
â€¢ Reduces overfitting
â€¢ Handles non-linear data
â€¢ Robust to noise & outliers
â€¢ No feature scaling required

Key Hyperparameters
â€¢ n_estimators
â€¢ max_depth
â€¢ min_samples_split
â€¢ min_samples_leaf
â€¢ max_features

Evaluation Metrics
â€¢ MAE
â€¢ RMSE
â€¢ RÂ² Score

Use Cases
â€¢ House price prediction
â€¢ Salary prediction
â€¢ Sales forecasting
â€¢ Medical cost estimation

```

---

# ğŸ¤ Viva / Interview One-Liners

- **Random Forest Regression à¦•à§€?**
    
    ğŸ‘‰ Multiple regression tree à¦à¦° average à¦¦à¦¿à§Ÿà§‡ prediction
    
- **Bagging à¦¨à¦¾ Boosting?**
    
    ğŸ‘‰ Bagging
    
- **Overfitting à¦•à¦® à¦•à§‡à¦¨?**
    
    ğŸ‘‰ Bootstrap sampling + feature randomness
    
- **Scaling à¦¦à¦°à¦•à¦¾à¦°?**
    
    ğŸ‘‰ à¦¨à¦¾
    

---