![Image](https://miro.medium.com/0%2A9dMWQGuWz5SiiBw7)

![Image](https://media.licdn.com/dms/image/v2/C5612AQEJQE6b_4Q7TQ/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1527370562133?e=2147483647\&t=jlH0HuahY7TlmQC8IVTucm_32_seGwpjrVYNaQjIyg4\&v=beta)

![Image](https://miro.medium.com/v2/resize%3Afit%3A790/1%2ApyOhNcusifrGxaOUgQz-bg.png)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2AF7oim6JAbEHGB9-wrU6TfA.png)

## üîê Regularization ‚Äî In-Depth Intuition (‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡ßü, ‡¶è‡¶ï‡¶¶‡¶Æ ‡¶™‡¶∞‡¶ø‡¶∑‡ßç‡¶ï‡¶æ‡¶∞)

Regularization ‡¶¨‡ßÅ‡¶ù‡¶≤‡ßá **XGBoost / Gradient Boosting / ML overfitting**‚Äî‡¶∏‡¶¨ ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá clear ‡¶π‡ßü‡ßá ‡¶Ø‡¶æ‡ßü‡•§

---

## üß† Regularization ‡¶Ü‡¶∏‡¶≤‡ßá ‡¶ï‡ßÄ?

**Regularization** ‡¶π‡¶≤‡ßã ‡¶è‡¶Æ‡¶® ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ï‡ßå‡¶∂‡¶≤ ‡¶Ø‡¶æ ‡¶Æ‡¶°‡ßá‡¶≤‡¶ï‡ßá ‡¶¨‡¶≤‡ßá:

> ‚ùù ‡¶¨‡ßá‡¶∂‡¶ø clever ‡¶π‡¶§‡ßá ‡¶Ø‡ßá‡¶ì ‡¶®‡¶æ‚Äî‡¶∏‡¶π‡¶ú ‡¶•‡¶æ‡¶ï‡ßã ‚ùû

‡¶Ö‡¶∞‡ßç‡¶•‡¶æ‡ßé,

* Training data ‡¶Æ‡ßÅ‡¶ñ‡¶∏‡ßç‡¶• ‡¶ï‡¶∞‡¶æ ‚ùå
* General pattern ‡¶∂‡ßá‡¶ñ‡¶æ ‚úÖ

---

## üî• ‡¶ï‡ßá‡¶® Regularization ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞?

‡¶ï‡¶æ‡¶∞‡¶£ ML ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶∏‡ßç‡¶¨‡¶≠‡¶æ‡¶¨‡¶§‡¶á ‡¶ö‡¶æ‡ßü:

> ‚Äú‡¶Ü‡¶Æ‡¶ø ‡¶∏‡¶¨ ‡¶°‡ßá‡¶ü‡¶æ perfectly fit ‡¶ï‡¶∞‡¶¨‡ßã‚Äù

‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶è‡¶∞ ‡¶´‡¶≤:

* Training accuracy ‚Üë
* Test accuracy ‚Üì
  ‚û°Ô∏è **Overfitting**

üìå Regularization ‡¶è‡¶á overconfidence ‡¶≠‡¶æ‡¶ô‡ßá‡•§

---

## üß© Intuition (Real-life Analogy)

‡¶ß‡¶∞‡ßÅ‡¶® ‡¶è‡¶ï‡¶ú‡¶® ‡¶õ‡¶æ‡¶§‡ßç‡¶∞:

* ‡¶∏‡¶¨ ‡¶¨‡¶á ‡¶Æ‡ßÅ‡¶ñ‡¶∏‡ßç‡¶• ‡¶ï‡¶∞‡ßá‡¶õ‡ßá
* ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ concept ‡¶¨‡ßã‡¶ù‡ßá ‡¶®‡¶æ

üìå Exam-‡¶è ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶ò‡ßÅ‡¶∞‡¶≤‡ßá‡¶á fail ‚ùå

‚û°Ô∏è Regularization =

> ‚ÄúConcept ‡¶∂‡ßá‡¶ñ‡ßã, ‡¶Æ‡ßÅ‡¶ñ‡¶∏‡ßç‡¶• ‡¶®‡¶æ‚Äù

---

## üìà Overfitting vs Regularization (Visual intuition)

* Overfitting ‚Üí ‡¶Ö‡¶®‡ßá‡¶ï ‡¶¨‡¶æ‡¶Å‡¶ï‡¶æ‡¶®‡ßã curve
* Regularized model ‚Üí smooth, simple curve

---

## üß† Regularization ‡¶ï‡ßÄ‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá?

Regularization ‡¶Æ‡ßÇ‡¶≤‡¶§ **Penalty ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßá**:

[
Loss = Error + Penalty
]

Penalty ‡¶¨‡¶≤‡ßá:

> ‚ÄúModel ‡¶¨‡ßá‡¶∂‡¶ø ‡¶ú‡¶ü‡¶ø‡¶≤ ‡¶π‡¶≤‡ßá loss ‡¶¨‡¶æ‡ßú‡¶¨‡ßá‚Äù

---

## üîπ Regularization-‡¶è‡¶∞ ‡¶ß‡¶∞‡¶® (Core ML)

### 1Ô∏è‚É£ L2 Regularization (Ridge)

[
Penalty = \lambda \sum w^2
]

**Intuition**:

* ‡¶¨‡ßú weight ‚Üí ‡¶¨‡ßá‡¶∂‡¶ø penalty
* Weight ‡¶õ‡ßã‡¶ü ‡¶∞‡¶æ‡¶ñ‡ßá
* Smooth solution ‡¶¶‡ßá‡ßü

üìå XGBoost-‡¶è:

* `lambda` = L2 penalty

---

### 2Ô∏è‚É£ L1 Regularization (Lasso)

[
Penalty = \lambda \sum |w|
]

**Intuition**:

* ‡¶Ö‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶®‡ßÄ‡ßü feature ‚Üí weight = 0
* Feature selection ‡¶ï‡¶∞‡ßá

üìå XGBoost-‡¶è:

* `alpha` = L1 penalty

---

## üå≤ XGBoost-Specific Regularization (‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ üî•)

XGBoost ‡¶∂‡ßÅ‡¶ß‡ßÅ weight ‡¶®‡¶æ, **tree structure-‡¶ï‡ßá‡¶ì penalize ‡¶ï‡¶∞‡ßá**‡•§

### üîπ 1. Lambda (Œª) ‚Äî Leaf Weight Control

> ‚ÄúLeaf value ‡¶¨‡ßá‡¶∂‡¶ø ‡¶¨‡ßú ‡¶π‡¶≤‡ßá penalty‚Äù

üìå Overconfidence ‡¶ï‡¶Æ‡¶æ‡ßü

---

### üîπ 2. Alpha (Œ±) ‚Äî Sparsity Control

> ‚Äú‡¶Ö‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶®‡ßÄ‡ßü correction ‡¶¨‡¶æ‡¶¶ ‡¶¶‡¶æ‡¶ì‚Äù

üìå Feature selection-‡¶è‡¶∞ ‡¶Æ‡¶§‡ßã ‡¶ï‡¶æ‡¶ú

---

### üîπ 3. Gamma (Œ≥) ‚Äî Split Control

> ‚ÄúSplit ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶≤‡ßá ‡¶≤‡¶æ‡¶≠ ‡¶¶‡ßá‡¶ñ‡¶æ‡¶ì‚Äù

üìå Tree ‡¶Ö‡¶Ø‡¶•‡¶æ deep ‡¶π‡ßü ‡¶®‡¶æ

---

### üîπ 4. Max Depth

> ‚ÄúTree ‡¶¨‡ßá‡¶∂‡¶ø ‡¶ó‡¶≠‡ßÄ‡¶∞ ‡¶π‡¶≤‡ßá complexity ‡¶¨‡¶æ‡ßú‡ßá‚Äù

---

### üîπ 5. Subsample / Colsample

> ‚Äú‡¶∏‡¶¨ data / feature ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßã ‡¶®‡¶æ‚Äù

üìå Noise ‡¶ï‡¶Æ‡ßá

---

## üß† XGBoost Loss (with Regularization)

[
Loss = \sum l(y, \hat{y}) + \sum \Omega(Tree)
]

[
\Omega(Tree) = \gamma T + \frac{1}{2}\lambda \sum w^2
]

üìå ‡¶Æ‡¶æ‡¶®‡ßá:

* Tree ‡¶¨‡ßá‡¶∂‡¶ø ‡¶π‡¶≤‡ßá ‚Üí penalty
* Leaf weight ‡¶¨‡ßá‡¶∂‡¶ø ‡¶π‡¶≤‡ßá ‚Üí penalty

---

## üî• Regularization vs Learning Rate (Intuition)

| ‡¶¨‡¶ø‡¶∑‡ßü             | Learning Rate | Regularization   |
| ---------------- | ------------- | ---------------- |
| ‡¶ï‡ßÄ ‡¶®‡¶ø‡ßü‡¶®‡ßç‡¶§‡ßç‡¶∞‡¶£ ‡¶ï‡¶∞‡ßá | Update speed  | Model complexity |
| ‡¶ï‡¶æ‡¶ú              | ‡¶ß‡ßÄ‡¶∞‡ßá ‡¶∂‡ßá‡¶ñ‡¶æ     | ‡¶ï‡¶Æ clever ‡¶π‡¶ì‡ßü‡¶æ   |
| Overfitting      | ‡¶ï‡¶Æ‡¶æ‡ßü          | ‡¶∂‡¶ï‡ßç‡¶§‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶Æ‡¶æ‡ßü    |

---

## üéØ ‡¶ï‡¶ñ‡¶® Regularization ‡¶¨‡¶æ‡ßú‡¶æ‡¶¨‡ßá‡¶®?

* Training score ‚â´ Test score
* Model ‡¶ñ‡ßÅ‡¶¨ unstable
* Noise ‡¶¨‡ßá‡¶∂‡¶ø
* Tree ‡¶¨‡ßá‡¶∂‡¶ø deep

---

## üß† Golden Line (‡¶Æ‡¶®‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø)

> **Regularization =
> Model-‡¶è‡¶∞ ego control system**

---

## ‚ú® Ultra-Short Summary

* Overfitting = ‡¶¨‡ßá‡¶∂‡¶ø clever
* Regularization = discipline
* XGBoost = smartest disciplined model

---

