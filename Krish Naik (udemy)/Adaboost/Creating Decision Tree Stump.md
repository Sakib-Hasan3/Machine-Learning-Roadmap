## ðŸŒ³ Creating a **Decision Tree Stump**

*(AdaBoost-à¦à¦° à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£ building block â€” à¦¬à¦¾à¦‚à¦²à¦¾à§Ÿ à¦¸à¦®à§à¦ªà§‚à¦°à§à¦£ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾)*

---

## 1ï¸âƒ£ Decision Tree Stump à¦•à§€?

**Decision Tree Stump** à¦¹à¦²à§‹ à¦à¦•à¦Ÿà¦¿ **à¦–à§à¦¬à¦‡ à¦›à§‹à¦Ÿ Decision Tree** à¦¯à§‡à¦–à¦¾à¦¨à§‡:

* âœ… à¦®à¦¾à¦¤à§à¦° **à§§à¦Ÿà¦¿ feature**
* âœ… à¦®à¦¾à¦¤à§à¦° **à§§à¦Ÿà¦¿ split**
* âœ… à¦®à¦¾à¦¤à§à¦° **à§§à¦Ÿà¦¿ decision rule**
* âŒ à¦•à§‹à¦¨à§‹ child split à¦¨à§‡à¦‡ (depth = 1)

ðŸ“Œ à¦à¦•à§‡ à¦¬à¦²à¦¾ à¦¹à§Ÿ **Weak Learner**, à¦•à¦¾à¦°à¦£ à¦à¦•à¦¾ à¦à¦•à¦¾ à¦à¦Ÿà¦¾ à¦–à§à¦¬ à¦¶à¦•à§à¦¤à¦¿à¦¶à¦¾à¦²à§€ à¦¨à¦¾
à¦•à¦¿à¦¨à§à¦¤à§ **AdaBoost-à¦à¦° à¦œà¦¨à§à¦¯ à¦à¦Ÿà¦¿à¦‡ perfect**à¥¤

---

## 2ï¸âƒ£ Decision Stump à¦•à§‡à¦¨ AdaBoost-à¦ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à§Ÿ?

AdaBoost-à¦à¦° à¦®à§‚à¦² à¦§à¦¾à¦°à¦£à¦¾:

> â€œà¦…à¦¨à§‡à¦• à¦¦à§à¦°à§à¦¬à¦² à¦®à¦¡à§‡à¦² + à¦¸à¦ à¦¿à¦• weight = à¦¶à¦•à§à¦¤à¦¿à¦¶à¦¾à¦²à§€ à¦®à¦¡à§‡à¦²â€

Decision stump:

* à¦–à§à¦¬ à¦¸à¦¹à¦œ
* à¦¦à§à¦°à§à¦¤ train à¦¹à§Ÿ
* bias à¦¬à§‡à¦¶à¦¿, variance à¦•à¦®
* sequential boosting-à¦à¦° à¦œà¦¨à§à¦¯ à¦†à¦¦à¦°à§à¦¶

ðŸ‘‰ à¦¤à¦¾à¦‡ AdaBoost à¦ªà§à¦°à¦¾à§Ÿ à¦¸à¦¬à¦¸à¦®à§Ÿ **decision stump à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡**

---

## 3ï¸âƒ£ Decision Tree vs Decision Stump

| à¦¬à¦¿à¦·à§Ÿ         | Decision Tree | Decision Stump |
| ------------ | ------------- | -------------- |
| Depth        | à¦¯à§‡à¦•à§‹à¦¨à§‹        | à¦®à¦¾à¦¤à§à¦° 1        |
| Split à¦¸à¦‚à¦–à§à¦¯à¦¾ | à¦…à¦¨à§‡à¦•          | 1              |
| Complexity   | à¦¬à§‡à¦¶à¦¿          | à¦–à§à¦¬ à¦•à¦®         |
| Overfitting  | à¦¹à¦¤à§‡ à¦ªà¦¾à¦°à§‡      | à¦ªà§à¦°à¦¾à§Ÿ à¦¨à¦¾       |
| AdaBoost     | âŒ             | âœ…              |

---

## 4ï¸âƒ£ Decision Stump à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦¤à§ˆà¦°à¦¿ à¦¹à§Ÿ? (Concept)

à¦§à¦°à¦¾ à¦¯à¦¾à¦• à¦†à¦®à¦¾à¦¦à§‡à¦° dataset:

| Age | Income | Buy (Label) |
| --- | ------ | ----------- |
| 22  | Low    | No          |
| 35  | High   | Yes         |
| 40  | Medium | Yes         |
| 28  | Low    | No          |

### Step 1ï¸âƒ£ à¦à¦•à¦Ÿà¦¿ feature à¦¬à§‡à¦›à§‡ à¦¨à§‡à¦“à§Ÿà¦¾

à¦§à¦°à¦¾ à¦¯à¦¾à¦• â†’ **Age**

---

### Step 2ï¸âƒ£ à¦à¦•à¦Ÿà¦¿ threshold à¦ à¦¿à¦• à¦•à¦°à¦¾

à¦‰à¦¦à¦¾à¦¹à¦°à¦£:

```
Age â‰¤ 30 ?
```

---

### Step 3ï¸âƒ£ à¦¦à§à¦‡à¦Ÿà¦¾ leaf à¦¤à§ˆà¦°à¦¿

* Left leaf â†’ Yes / No
* Right leaf â†’ Yes / No

ðŸ“Œ à¦¯à§‡ label à¦¬à§‡à¦¶à¦¿ à¦¥à¦¾à¦•à§‡, à¦¸à§‡à¦Ÿà¦¾à¦‡ prediction

---

## 5ï¸âƒ£ Decision Stump à¦•à§€à¦­à¦¾à¦¬à§‡ â€œbestâ€ à¦¹à§Ÿ?

Stump à¦¨à¦¿à¦°à§à¦¬à¦¾à¦šà¦¨ à¦•à¦°à¦¾ à¦¹à§Ÿ **Error à¦•à¦®à¦¾à¦¨à§‹à¦° à¦­à¦¿à¦¤à§à¦¤à¦¿à¦¤à§‡**
AdaBoost-à¦ à¦à¦Ÿà¦¿ à¦¹à§Ÿ **weighted error** à¦¦à¦¿à§Ÿà§‡à¥¤

### ðŸ”¹ Weighted Error Formula:

```
Error = Î£ (weight_i Ã— wrong_prediction_i)
```

ðŸ‘‰ à¦¯à§‡à¦‡ stump-à¦à¦° error à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦•à¦®, à¦¸à§‡à¦Ÿà¦¾à¦‡ à¦¨à¦¿à¦°à§à¦¬à¦¾à¦šà¦¿à¦¤ à¦¹à§Ÿ

---

## 6ï¸âƒ£ AdaBoost-à¦ Decision Stump à¦¤à§ˆà¦°à¦¿ à¦•à¦°à¦¾à¦° à¦§à¦¾à¦ª

### ðŸ”¹ Step 1: à¦¸à¦¬ data weight à¦¦à¦¿à§Ÿà§‡ à¦¶à§à¦°à§

```
w1 = w2 = ... = wn = 1/n
```

---

### ðŸ”¹ Step 2: à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ feature à¦¨à¦¿à§Ÿà§‡ à¦šà§‡à¦·à§à¦Ÿà¦¾

* Feature 1 â†’ threshold test
* Feature 2 â†’ threshold test
* â€¦

---

### ðŸ”¹ Step 3: à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ stump-à¦à¦° error à¦¬à§‡à¦° à¦•à¦°à¦¾

---

### ðŸ”¹ Step 4: à¦¯à§‡à¦Ÿà¦¾à¦° error à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦•à¦® â†’ à¦¸à§‡à¦Ÿà¦¾à¦‡ stump

---

## 7ï¸âƒ£ Decision Stump-à¦à¦° Mathematical Form

à¦à¦•à¦Ÿà¦¿ stump à¦®à§‚à¦²à¦¤ à¦à¦‡ form-à¦à¦°:

```
h(x) = 
  +1  if x_j â‰¤ Î¸
  -1  otherwise
```

à¦¯à§‡à¦–à¦¾à¦¨à§‡:

* `x_j` = chosen feature
* `Î¸` = threshold

---

## 8ï¸âƒ£ Decision Stump + AdaBoost (Connection)

AdaBoost-à¦:
1ï¸âƒ£ Stump train à¦¹à§Ÿ
2ï¸âƒ£ Error à¦¬à§‡à¦° à¦¹à§Ÿ
3ï¸âƒ£ Alpha à¦¹à¦¿à¦¸à¦¾à¦¬ à¦¹à§Ÿ
4ï¸âƒ£ Data weight update à¦¹à§Ÿ

ðŸ“Œ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ round-à¦ **à¦à¦•à¦Ÿà¦¿ à¦¨à¦¤à§à¦¨ stump**

---

## 9ï¸âƒ£ Python à¦¦à¦¿à§Ÿà§‡ Decision Stump à¦¤à§ˆà¦°à¦¿ (Sklearn)

```python
from sklearn.tree import DecisionTreeClassifier

stump = DecisionTreeClassifier(
    max_depth=1,
    criterion='gini'
)

stump.fit(X_train, y_train)
```

ðŸ‘‰ `max_depth=1` à¦®à¦¾à¦¨à§‡à¦‡ Decision Stump

---

## ðŸ”Ÿ Interview / Viva à¦ªà§à¦°à¦¶à§à¦¨à§‹à¦¤à§à¦¤à¦° (Bangla)

**Q1:** Decision stump à¦•à§‡à¦¨ weak learner?
ðŸ‘‰ à¦•à¦¾à¦°à¦£ à¦à¦¤à§‡ à¦®à¦¾à¦¤à§à¦° à¦à¦•à¦Ÿà¦¿ split à¦¥à¦¾à¦•à§‡

**Q2:** AdaBoost à¦•à¦¿ deep tree à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡?
ðŸ‘‰ à¦¨à¦¾, à¦¸à¦¾à¦§à¦¾à¦°à¦£à¦¤ stump à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡

**Q3:** Decision stump à¦•à¦¿à¦­à¦¾à¦¬à§‡ select à¦¹à§Ÿ?
ðŸ‘‰ Minimum weighted error-à¦à¦° à¦­à¦¿à¦¤à§à¦¤à¦¿à¦¤à§‡

**Q4:** Stump overfitting à¦•à¦°à§‡?
ðŸ‘‰ à¦¨à¦¾, à¦•à¦¾à¦°à¦£ model à¦–à§à¦¬ simple

---

## ðŸ§  à¦®à¦¨à§‡ à¦°à¦¾à¦–à¦¾à¦° shortcut

> **Decision Stump = One feature + One split + One rule**

---


