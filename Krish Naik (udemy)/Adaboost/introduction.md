## ЁЯФ░ Introduction to **AdaBoost (Adaptive Boosting)** тАУ Machine Learning

*(ржмрж╛ржВрж▓рж╛рзЯ рж╕ржорзНржкрзВрж░рзНржг ржмрзНржпрж╛ржЦрзНржпрж╛)*

![Image](https://almablog-media.s3.ap-south-1.amazonaws.com/image_28_7cf514b000.png)

![Image](https://doimages.nyc3.cdn.digitaloceanspaces.com/010AI-ML/content/images/2019/12/WhatsApp-Image-2019-12-30-at-11.55.02-AM.jpeg)

![Image](https://substackcdn.com/image/fetch/%24s_%21RxGQ%21%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe32def8b-6361-40fb-8e41-180ba002ef1e_2501x1467.png)

![Image](https://daxg39y63pxwu.cloudfront.net/images/blog/adaboost-algorithm/AdaBoost_Algorithm_Explained_in_Depth.webp)

---

## 1я╕ПтГг AdaBoost ржХрзА?

**AdaBoost (Adaptive Boosting)** рж╣рж▓рзЛ ржПржХржЯрж┐ **Ensemble Machine Learning algorithm**,
ржпрзЗржЦрж╛ржирзЗ **ржПржХрж╛ржзрж┐ржХ ржжрзБрж░рзНржмрж▓ (weak) ржоржбрзЗрж▓** ржПржХрж╕рж╛ржерзЗ ржХрж╛ржЬ ржХрж░рзЗ **ржПржХржЯрж┐ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА (strong) ржоржбрзЗрж▓** рждрзИрж░рж┐ ржХрж░рзЗред

ЁЯСЙ рж╕рж╣ржЬ ржнрж╛рж╖рж╛рзЯ:

> тАЬржЕржирзЗржХржЧрзБрж▓рзЛ ржжрзБрж░рзНржмрж▓ ржЫрж╛рждрзНрж░ ржорж┐рж▓рзЗ ржПржХрж╕рж╛ржерзЗ ржкрзЬрж╛рж╢рзЛржирж╛ ржХрж░рзЗ ржПржХржЬржи ржЯржкрж╛рж░ ржЫрж╛рждрзНрж░ ржмрж╛ржирж╛ржирзЛтАЭ ЁЯШД

---

## 2я╕ПтГг Weak Learner ржорж╛ржирзЗ ржХрзА?

* **Weak learner** = ржПржоржи ржоржбрзЗрж▓ ржпрж╛ рж╕рж╛ржорж╛ржирзНржп ржнрж╛рж▓рзЛ ржХрж╛ржЬ ржХрж░рзЗ
* рж╕рж╛ржзрж╛рж░ржгржд **Decision Stump** ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯ
  (Decision Tree ржпрж╛рж░ depth = 1)

ЁЯУМ Decision stump:

* ржорж╛рждрзНрж░ **ржПржХржЯрж┐ feature**
* ржорж╛рждрзНрж░ **ржПржХржЯрж┐ split**
* ржЦрзБржм рж╕рж╣ржЬ ржХрж┐ржирзНрждрзБ ржПржХрж╛ ржПржХрж╛ ржЦрзБржм рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА ржирж╛

---

## 3я╕ПтГг AdaBoost ржХрзЗржи ржжрж░ржХрж╛рж░?

ржЕржирзЗржХ рж╕ржорзЯ:

* ржПржХржЯрж┐ржорж╛рждрзНрж░ ржоржбрзЗрж▓ ржпржерзЗрж╖рзНржЯ ржнрж╛рж▓рзЛ accuracy ржжрзЗрзЯ ржирж╛
* ржХрж┐ржЫрзБ data point ржмрж╛рж░ржмрж╛рж░ ржнрзБрж▓ рж╣рзЯ

ЁЯСЙ AdaBoost:

* **ржнрзБрж▓ рж╣ржУрзЯрж╛ data ржЧрзБрж▓рзЛржХрзЗ ржмрзЗрж╢рж┐ ржЧрзБрж░рзБрждрзНржм ржжрзЗрзЯ**
* ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ ржЖржЧрзЗрж░ ржнрзБрж▓ ржерзЗржХрзЗ рж╢рзЗржЦрзЗ

---

## 4я╕ПтГг AdaBoost ржХрзАржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░рзЗ? (Step-by-step)

### ЁЯФ╣ Step 1: рж╢рзБрж░рзБрждрзЗ рж╕ржм data рж╕ржорж╛ржи ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг

ржзрж░рж╛ ржпрж╛ржХ рззрзжржЯрж╛ data point ржЖржЫрзЗ
тЮбя╕П ржкрзНрж░рждрзНржпрзЗржХржЯрж╛рж░ weight = 1/10

---

### ЁЯФ╣ Step 2: ржкрзНрж░ржержо weak learner train ржХрж░рж╛

* ржкрзНрж░ржержо decision stump train рж╣рзЯ
* ржХрж┐ржЫрзБ data **ржнрзБрж▓ржнрж╛ржмрзЗ classify** рж╣рзЯ

---

### ЁЯФ╣ Step 3: ржнрзБрж▓ data-рж░ weight ржмрж╛рзЬрж╛ржирзЛ

* ржпрзЗржЧрзБрж▓рзЛ ржнрзБрж▓ рж╣рзЯрзЗржЫрзЗ тЮЬ **weight ржмрж╛рзЬрж╛ржирзЛ рж╣рзЯ**
* ржпрзЗржЧрзБрж▓рзЛ ржарж┐ржХ рж╣рзЯрзЗржЫрзЗ тЮЬ **weight ржХржорж╛ржирзЛ рж╣рзЯ**

ЁЯУМ ржХрж╛рж░ржг:

> тАЬржкрж░рзЗрж░ ржоржбрзЗрж▓ ржпрзЗржи ржПржЗ ржнрзБрж▓ржЧрзБрж▓рзЛрж░ ржжрж┐ржХрзЗ ржмрзЗрж╢рж┐ ржиржЬрж░ ржжрзЗрзЯтАЭ

---

### ЁЯФ╣ Step 4: ржЖржмрж╛рж░ ржирждрзБржи weak learner train

* ржПржмрж╛рж░ ржоржбрзЗрж▓ ржмрзЗрж╢рж┐ ржЧрзБрж░рзБрждрзНржм ржжрзЗрзЯ ржЖржЧрзЗрж░ ржнрзБрж▓ data-ржХрзЗ
* ржЖржмрж╛рж░ ржХрж┐ржЫрзБ ржнрзБрж▓, ржХрж┐ржЫрзБ ржарж┐ржХ

---

### ЁЯФ╣ Step 5: ржкрзНрж░рждрж┐ржЯрж┐ weak learner-ржПрж░ ржЧрзБрж░рзБрждрзНржм ржирж┐рж░рзНржзрж╛рж░ржг

ржкрзНрж░рждрж┐ржЯрж┐ weak learner-ржПрж░ ржЬржирзНржп ржПржХржЯрж╛ ржорж╛ржи ржмрзЗрж░ ржХрж░рж╛ рж╣рзЯ:

```
alpha = ┬╜ ├Ч ln((1 - error) / error)
```

* Error ржХржо тЮЬ alpha ржмрзЗрж╢рж┐ тЮЬ model ржмрзЗрж╢рж┐ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА
* Error ржмрзЗрж╢рж┐ тЮЬ alpha ржХржо

---

### ЁЯФ╣ Step 6: Final Prediction (Weighted Voting)

рж╕ржм weak learner ржПржХрж╕рж╛ржерзЗ vote ржжрзЗрзЯ:

```
Final Prediction = sign( ╬г (alpha ├Ч prediction) )
```

ЁЯСЙ ржпрж╛рж░ alpha ржмрзЗрж╢рж┐, рждрж╛рж░ vote-ржПрж░ ржЧрзБрж░рзБрждрзНржм ржмрзЗрж╢рж┐

---

## 5я╕ПтГг AdaBoost-ржПрж░ ржПржХржЯрж┐ рж╕рж╣ржЬ ржЙржжрж╛рж╣рж░ржг

ржзрж░рж╛ ржпрж╛ржХ:

* 3ржЯрж┐ decision stump ржЖржЫрзЗ
* рждрж╛ржжрзЗрж░ alpha: 0.8, 0.5, 0.2

рждрж╛рж╣рж▓рзЗ:

* 1st model-ржПрж░ ржХржерж╛ рж╕ржмржЪрзЗрзЯрзЗ ржмрзЗрж╢рж┐ рж╢рзЛржирж╛ рж╣ржмрзЗ
* 3rd model-ржПрж░ ржХржерж╛ рж╕ржмржЪрзЗрзЯрзЗ ржХржо

---

## 6я╕ПтГг AdaBoost ржХрзЛржерж╛рзЯ ржмрзНржпржмрж╣рж╛рж░ рж╣рзЯ?

* тЬЕ Classification (рж╕ржмржЪрзЗрзЯрзЗ ржмрзЗрж╢рж┐)
* ЁЯФ╣ Face detection (ViolaтАУJones)
* ЁЯФ╣ Spam detection
* ЁЯФ╣ Medical diagnosis
* ЁЯФ╣ Credit scoring

ЁЯУМ Regression-ржПрж░ ржЬржирзНржпржУ ржЖржЫрзЗ тЖТ **AdaBoostRegressor**

---

## 7я╕ПтГг AdaBoost-ржПрж░ рж╕рзБржмрж┐ржзрж╛ (Advantages)

тЬЕ High accuracy
тЬЕ Overfitting ржХржо (ржЕржирзЗржХ ржХрзНрж╖рзЗрждрзНрж░рзЗ)
тЬЕ Feature selection ржирж┐ржЬрзЗ ржирж┐ржЬрзЗ ржХрж░рзЗ
тЬЕ Simple idea ржХрж┐ржирзНрждрзБ powerful

---

## 8я╕ПтГг AdaBoost-ржПрж░ ржЕрж╕рзБржмрж┐ржзрж╛ (Disadvantages)

тЭМ Noise ржУ outlier-ржПрж░ ржкрзНрж░рждрж┐ sensitive
тЭМ ржЦрзБржм noisy data рж╣рж▓рзЗ performance ржХржорзЗ
тЭМ Sequential training тЖТ parallel ржХрж░рж╛ ржпрж╛рзЯ ржирж╛

---

## 9я╕ПтГг Bagging vs Boosting (рж╕рж╣ржЬ рждрзБрж▓ржирж╛)

| ржмрж┐рж╖рзЯ       | Bagging         | Boosting          |
| ---------- | --------------- | ----------------- |
| Focus      | Variance ржХржорж╛ржирзЛ  | Bias ржХржорж╛ржирзЛ        |
| Data usage | Random sampling | Weighted sampling |
| Learning   | Independent     | Sequential        |
| Example    | Random Forest   | AdaBoost          |

---

## ЁЯФЯ AdaBoost тАУ Interview / Viva ржкрзНрж░рж╢рзНржирзЛрждрзНрждрж░ (Bangla)

**Q1:** AdaBoost ржХрзЗржи тАЬAdaptiveтАЭ?
ЁЯСЙ ржХрж╛рж░ржг ржкрзНрж░рждрж┐ржЯрж┐ ржзрж╛ржкрзЗ data-ржПрж░ weight ржкрж░рж┐ржмрж░рзНрждржи рж╣рзЯ

**Q2:** Weak learner рж╣рж┐рж╕рзЗржмрзЗ ржХрзА ржмрзНржпржмрж╣рж╛рж░ рж╣рзЯ?
ЁЯСЙ рж╕рж╛ржзрж╛рж░ржгржд Decision Stump

**Q3:** AdaBoost overfitting ржХрж░рзЗ?
ЁЯСЙ ржХржо ржХрж░рзЗ, рждржмрзЗ noisy data рж╣рж▓рзЗ рж╕ржорж╕рзНржпрж╛ рж╣рждрзЗ ржкрж╛рж░рзЗ

**Q4:** AdaBoost classification ржирж╛ regression?
ЁЯСЙ ржжрзБржЯрзЛржЗ (AdaBoostClassifier, AdaBoostRegressor)

---

## ЁЯФЪ рж╕ржВржХрзНрж╖рзЗржкрзЗ ржоржирзЗ рж░рж╛ржЦрж╛рж░ ржЯрзНрж░рж┐ржХ ЁЯза

> **AdaBoost = Weak learner + Weight update + Weighted voting**

---

ржЪрж╛ржУ рждрзЛ ржЖржорж┐ ржкрж░рзЗрж░ ржзрж╛ржкрзЗ ржжрж┐рждрзЗ ржкрж╛рж░рж┐ ЁЯСЗ

* тЬЕ **Python code (Sklearn) рж╕рж╣ example**
* тЬЕ **Mathematical intuition (рж╕рж╣ржЬржнрж╛ржмрзЗ)**
* тЬЕ **Notion / PDF notes**
* тЬЕ **XGBoost / Gradient Boosting-ржПрж░ рж╕рж╛ржерзЗ рждрзБрж▓ржирж╛**

ЁЯСЙ ржмрж▓рзЛ, ржкрж░рзЗрж░ржЯрж╛ ржХрзЛржиржЯрж╛ ржЪрж╛ржУ?

---

## тЬЕ Python Code (Scikit-learn) тАУ Hands-on

ржирж┐ржЪрзЗ ржПржХржЯрж┐ ржЫрзЛржЯ ржХрж┐ржирзНрждрзБ рж╕ржорзНржкрзВрж░рзНржг sklearn ржЙржжрж╛рж╣рж░ржг ржжрзЗржУрзЯрж╛ рж╣рж▓рзЛ, ржпрж╛рждрзЗ ржЖржкржирж┐ AdaBoostClassifier ржХрзАржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░рзЗ рждрж╛ ржжрзНрж░рзБржд ржЪрж╛рж▓рж┐рзЯрзЗ ржжрзЗржЦрждрзЗ ржкрж╛рж░рзЗржиред

### ржХрзА рж╣ржмрзЗ ржПржЗ ржХрзЛржбрзЗ?
- ржЫрзЛржЯ synthetic dataset ржмрж╛ржирж╛ржмрзЗ
- Decision Stump (`max_depth=1`) ржжрж┐рзЯрзЗ AdaBoost train ржХрж░ржмрзЗ
- Test accuracy ржУ classification report ржжрзЗржЦрж╛ржмрзЗ

### ржХрзЛржб

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score, classification_report


def main():
	# 1) Data рждрзИрж░рж┐
	X, y = make_classification(
		n_samples=1000,
		n_features=10,
		n_informative=5,
		n_redundant=0,
		random_state=42,
	)

	X_train, X_test, y_train, y_test = train_test_split(
		X, y, test_size=0.25, random_state=42, stratify=y
	)

	# 2) Weak learner = Decision Stump
	stump = DecisionTreeClassifier(max_depth=1, random_state=42)

	# 3) AdaBoost model
	clf = AdaBoostClassifier(
		estimator=stump,       # sklearn >= 1.2: 'estimator' ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи
		n_estimators=50,
		learning_rate=1.0,
		random_state=42,
	)

	# 4) Train
	clf.fit(X_train, y_train)

	# 5) Evaluate
	y_pred = clf.predict(X_test)
	acc = accuracy_score(y_test, y_pred)

	print(f"Accuracy: {acc:.4f}")
	print("\nClassification Report:\n")
	print(classification_report(y_test, y_pred, digits=4))

	# (ржРржЪрзНржЫрж┐ржХ) ржХрзЛржи feature ржХрждржЯрж╛ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг тАУ ржпржжрж┐ base estimators tree рж╣рзЯ
	if hasattr(clf, "feature_importances_"):
		print("Feature importances:", clf.feature_importances_)


if __name__ == "__main__":
	main()
```

### ржЯрж┐ржЙржи ржХрж░рж╛рж░ ржЯрж┐ржкрж╕
- `n_estimators`: ржмрзЗрж╢рж┐ ржжрж┐рж▓рзЗ рж╕рж╛ржзрж╛рж░ржгржд ржнрж╛рж▓рзЛ рж╣рзЯ, ржХрж┐ржирзНрждрзБ ржЦрзБржм ржмрзЗрж╢рж┐ рж╣рж▓рзЗ рж╕ржорзЯ ржмрж╛рзЬрзЗ
- `learning_rate`: ржЫрзЛржЯ ржХрж░рж▓рзЗ ржкрзНрж░рждрж┐ржЯрж┐ learner-ржПрж░ ржкрзНрж░ржнрж╛ржм ржХржорзЗ, ржХрж┐ржирзНрждрзБ ржмрзЗрж╢рж┐ learner рж▓рж╛ржЧрждрзЗ ржкрж╛рж░рзЗ
- `max_depth=1` ржжрж┐рзЯрзЗ рж╢рзБрж░рзБ ржХрж░рзБржи; data ржЬржЯрж┐рж▓ рж╣рж▓рзЗ ржПржХржЯрзБ ржмрж╛рзЬрж╛рждрзЗ ржкрж╛рж░рзЗржи

### ржжрзНрж░рзБржд ржЪрж╛рж▓рж┐рзЯрзЗ ржжрзЗржЦрзБржи
- Local-ржП run ржХрж░рждрзЗ ржЪрж╛ржЗрж▓рзЗ:

```bash
pip install -r requirements.txt
python adaboost_demo.py
```

