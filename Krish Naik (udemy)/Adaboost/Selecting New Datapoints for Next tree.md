## ğŸ”„ **Selecting New Datapoints for Next Tree (AdaBoost)**

*(à¦¬à¦¾à¦‚à¦²à¦¾à§Ÿ Step-by-Step à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾ + Code + Output à¦¸à¦¹)*

![Image](https://datamapu.com/images/adaboost/adaboost.png)

![Image](https://vitalflux.com/wp-content/uploads/2020/09/Screenshot-2020-09-09-at-8.17.33-AM.png)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2AHImcqWdiiQnr84PBGWjf5Q.png)

---

## 1ï¸âƒ£ Selecting New Datapoints à¦®à¦¾à¦¨à§‡ à¦•à§€?

AdaBoost-à¦ **à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ iteration-à¦à¦° à¦ªà¦°**
à¦ªà¦°à§‡à¦° weak learner (Decision Tree Stump) **à¦à¦•à¦‡ dataset à¦¦à§‡à¦–à§‡ à¦¨à¦¾**à¥¤

ğŸ‘‰ à¦¬à¦°à¦‚:

* **Weight à¦¬à§‡à¦¶à¦¿ à¦¯à¦¾à¦¦à§‡à¦°** â†’ à¦¤à¦¾à¦°à¦¾ **à¦¬à§‡à¦¶à¦¿ chance** à¦ªà¦¾à§Ÿ à¦†à¦¬à¦¾à¦° à¦¨à¦¿à¦°à§à¦¬à¦¾à¦šà¦¿à¦¤ à¦¹à¦“à§Ÿà¦¾à¦°
* **Weight à¦•à¦® à¦¯à¦¾à¦¦à§‡à¦°** â†’ à¦¤à¦¾à¦°à¦¾ **à¦•à¦® chance** à¦ªà¦¾à§Ÿ

ğŸ“Œ à¦à¦Ÿà¦¾à¦•à§‡à¦‡ à¦¬à¦²à§‡
ğŸ‘‰ **Weighted Sampling / Resampling**

---

## 2ï¸âƒ£ à¦•à§‡à¦¨ à¦¨à¦¤à§à¦¨ datapoints à¦¨à¦¿à¦°à§à¦¬à¦¾à¦šà¦¨ à¦•à¦°à¦¾ à¦¹à§Ÿ?

à¦•à¦¾à¦°à¦£ AdaBoost à¦šà¦¾à§Ÿ:

> â€œà¦ªà¦°à§‡à¦° tree à¦¯à§‡à¦¨ à¦†à¦—à§‡à¦° tree-à¦à¦° à¦­à§à¦²à¦—à§à¦²à§‹à¦° à¦‰à¦ªà¦° à¦¬à§‡à¦¶à¦¿ focus à¦•à¦°à§‡â€

à¦¯à¦¦à¦¿ à¦à¦•à¦‡ dataset à¦à¦•à¦‡à¦­à¦¾à¦¬à§‡ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¾ à¦¹à§Ÿ:

* Boosting à¦•à¦¾à¦œ à¦•à¦°à¦¬à§‡ à¦¨à¦¾
* à¦¸à¦¬ learner à¦à¦•à¦‡ à¦­à§à¦² à¦•à¦°à¦¬à§‡

---

## 3ï¸âƒ£ à¦†à¦—à§‡à¦° à¦§à¦¾à¦ªà¦—à§à¦²à§‹ à¦¸à¦‚à¦•à§à¦·à§‡à¦ªà§‡ à¦®à¦¨à§‡ à¦•à¦°à¦¿

à¦†à¦®à¦¾à¦¦à§‡à¦° à¦›à¦¿à¦²:

### ğŸ”¹ Normalized Weights

```
[0.166, 0.500, 0.166, 0.166]
```

### ğŸ”¹ Bins (Cumulative)

```
[0.166, 0.666, 0.832, 1.000]
```

ğŸ‘‰ 2nd datapoint (index 1) = à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦­à§à¦² â†’ à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦¬à§œ bin

---

## 4ï¸âƒ£ New Datapoints à¦•à§€à¦­à¦¾à¦¬à§‡ select à¦¹à§Ÿ? (Concept)

### ğŸ”¹ Step 1: Random number generate

```
r âˆˆ [0, 1]
```

### ğŸ”¹ Step 2: Check bin

à¦¯à§‡à¦‡ bin-à¦à¦° à¦®à¦§à§à¦¯à§‡ `r` à¦ªà§œà§‡ â†’ à¦¸à§‡à¦‡ datapoint selected

---

## 5ï¸âƒ£ Manual Example (à¦–à§à¦¬ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£)

à¦§à¦°à¦¾ à¦¯à¦¾à¦• random numbers:

```
[0.10, 0.40, 0.90, 0.60]
```

| Random r | Falls in bin | Selected datapoint |
| -------- | ------------ | ------------------ |
| 0.10     | 0.000â€“0.166  | D1                 |
| 0.40     | 0.166â€“0.666  | D2 âŒ               |
| 0.90     | 0.832â€“1.00   | D4                 |
| 0.60     | 0.166â€“0.666  | D2 âŒ               |

ğŸ‘‰ New dataset:

```
[D1, D2, D4, D2]
```

ğŸ“Œ à¦²à¦•à§à¦·à§à¦¯ à¦•à¦°à§‹:

* **D2 à¦¦à§à¦‡à¦¬à¦¾à¦° à¦à¦¸à§‡à¦›à§‡**
* à¦•à¦¾à¦°à¦£ à¦“à¦Ÿà¦¾à¦° weight à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦¬à§‡à¦¶à¦¿

---

## 6ï¸âƒ£ Python Code: Selecting New Datapoints

```python
import numpy as np

# Original dataset
X = np.array([[1], [2], [3], [4]])
y = np.array([0, 0, 1, 1])

# Normalized weights
weights = np.array([0.166, 0.5, 0.166, 0.166])

# Create bins (cumulative distribution)
bins = np.cumsum(weights)

# Fixed random numbers (for understanding)
random_numbers = [0.10, 0.40, 0.90, 0.60]

selected_indices = []

for r in random_numbers:
    idx = np.where(bins >= r)[0][0]
    selected_indices.append(idx)

X_new = X[selected_indices]
y_new = y[selected_indices]

print("Random Numbers:", random_numbers)
print("Selected Indices:", selected_indices)
print("New X:", X_new.flatten())
print("New y:", y_new)
```

---

## 7ï¸âƒ£ Output (Expected)

```
Random Numbers: [0.1, 0.4, 0.9, 0.6]
Selected Indices: [0, 1, 3, 1]
New X: [1 2 4 2]
New y: [0 0 1 0]
```

---

## 8ï¸âƒ£ à¦à¦‡ Output à¦¥à§‡à¦•à§‡ à¦•à§€ à¦¬à§‹à¦à¦¾ à¦¯à¦¾à§Ÿ?

* Index **1 (datapoint 2)** à¦¦à§à¦‡à¦¬à¦¾à¦° à¦à¦¸à§‡à¦›à§‡
* à¦•à¦¾à¦°à¦£ à¦¤à¦¾à¦° weight = **0.5 (à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦¬à§‡à¦¶à¦¿)**
* à¦ªà¦°à§‡à¦° tree à¦à¦‡ datapoint-à¦•à§‡ à¦¬à§‡à¦¶à¦¿ à¦¦à§‡à¦–à§‡ à¦¶à¦¿à¦–à¦¬à§‡

ğŸ‘‰ **Boosting à¦¸à¦«à¦²à¦­à¦¾à¦¬à§‡ à¦•à¦¾à¦œ à¦•à¦°à¦›à§‡** âœ…

---

## 9ï¸âƒ£ Next Tree à¦•à§€à¦­à¦¾à¦¬à§‡ à¦†à¦²à¦¾à¦¦à¦¾ à¦¹à§Ÿ?

à¦•à¦¾à¦°à¦£:

* Dataset-à¦ duplication à¦†à¦›à§‡
* Distribution à¦¬à¦¦à¦²à§‡ à¦—à§‡à¦›à§‡
* Error-prone datapoint dominate à¦•à¦°à¦›à§‡

ğŸ‘‰ à¦«à¦²à§‡:

* Next stump à¦†à¦—à§‡à¦° à¦­à§à¦² à¦ à¦¿à¦• à¦•à¦°à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à§‡

---

## ğŸ”Ÿ AdaBoost Loop-à¦ à¦à¦‡ à¦§à¦¾à¦ª à¦•à§‹à¦¥à¦¾à§Ÿ?

```
Train stump
â†“
Calculate error
â†“
Calculate alpha
â†“
Update weights
â†“
Normalize weights
â†“
Assign bins
â†“
Select new datapoints   â† âœ… (THIS STEP)
â†“
Train next stump
```

---

## 1ï¸âƒ£1ï¸âƒ£ Interview / Viva à¦–à§à¦¬ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£ à¦ªà§à¦°à¦¶à§à¦¨

**Q1:** Datapoint à¦à¦•à¦¾à¦§à¦¿à¦•à¦¬à¦¾à¦° à¦•à§‡à¦¨ à¦†à¦¸à§‡?
ğŸ‘‰ Weight à¦¬à§‡à¦¶à¦¿ à¦¹à¦²à§‡ probability à¦¬à§‡à¦¶à¦¿

**Q2:** AdaBoost à¦•à¦¿ data delete à¦•à¦°à§‡?
ğŸ‘‰ à¦¨à¦¾, re-sampling à¦•à¦°à§‡

**Q3:** Sampling à¦›à¦¾à§œà¦¾ AdaBoost à¦•à¦°à¦¾ à¦¯à¦¾à§Ÿ?
ğŸ‘‰ à¦¹à§à¦¯à¦¾à¦, weight-based loss à¦¦à¦¿à§Ÿà§‡à¦“ à¦•à¦°à¦¾ à¦¯à¦¾à§Ÿ (implementation dependent)

**Q4:** à¦à¦‡ à¦§à¦¾à¦ª à¦¨à¦¾ à¦¥à¦¾à¦•à¦²à§‡ à¦•à§€ à¦¹à¦¬à§‡?
ğŸ‘‰ Boosting à¦•à¦¾à¦œ à¦•à¦°à¦¬à§‡ à¦¨à¦¾

---

## ğŸ§  One-line Memory Trick

> **Wrong datapoints shout louder in the next tree** ğŸ”ŠğŸŒ³

---


