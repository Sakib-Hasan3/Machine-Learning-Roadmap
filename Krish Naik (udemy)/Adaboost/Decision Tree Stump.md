ржПржХржжржо ржарж┐ржХ ЁЯСН
ржПржЦржи ржЖржорж┐ **Decision Tree Stump**тАУржПрж░ ржЬржирзНржп

тЬЕ **Code**
тЬЕ **Concrete example dataset**
тЬЕ **Actual output (print ржХрж░рзЗ ржжрзЗржЦрж╛ржирзЛ)**

рж╕ржмржХрж┐ржЫрзБ **ржПржХрж╕рж╛ржерзЗ ржмрж╛ржВрж▓рж╛рзЯ** ржжрзЗржЦрж╛ржЪрзНржЫрж┐, ржпрзЗржи рждрзБржорж┐ ржХрзЛржб рж░рж╛ржи ржирж╛ ржХрж░рж▓рзЗржУ ржкрзБрж░рзЛржЯрж╛ ржкрж░рж┐рж╖рзНржХрж╛рж░ ржмрзБржЭрждрзЗ ржкрж╛рж░рзЛред

---

## ЁЯМ│ Decision Tree Stump тАУ Code + Example + Output

---

## 1я╕ПтГг Example Dataset (рж╕рж╣ржЬ)

ржзрж░рж╛ ржпрж╛ржХ ржЖржорж╛ржжрзЗрж░ ржПржХржЯрж┐ ржЫрзЛржЯ dataset ржЖржЫрзЗ:

| Feature (X) | Label (y) |
| ----------- | --------- |
| 1           | 0         |
| 2           | 0         |
| 3           | 1         |
| 4           | 1         |

ЁЯСЙ Task: Binary Classification
ЁЯСЙ Model: **Decision Tree Stump (depth = 1)**

---

## 2я╕ПтГг Python Code (Decision Tree Stump)

```python
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_text

# Step 1: Dataset
X = np.array([[1], [2], [3], [4]])
y = np.array([0, 0, 1, 1])

# Step 2: Create Decision Tree Stump
stump = DecisionTreeClassifier(
    max_depth=1,
    criterion='gini'
)

# Step 3: Train model
stump.fit(X, y)

# Step 4: Predictions
predictions = stump.predict(X)

# Step 5: Show tree structure
tree_rules = export_text(stump, feature_names=["X"])

print("Predictions:", predictions)
print("\nDecision Tree Stump Structure:\n")
print(tree_rules)
```

---

## 3я╕ПтГг Output (ржПржЗ ржХрзЛржб ржЪрж╛рж▓рж╛рж▓рзЗ ржпрж╛ ржжрзЗржЦрж╛ржмрзЗ)

### ЁЯФ╣ Predictions Output

```
Predictions: [0 0 1 1]
```

ЁЯУМ ржмрзНржпрж╛ржЦрзНржпрж╛:

* Input: `[1, 2, 3, 4]`
* Model рж╢рж┐ржЦрзЗржЫрзЗ:

  * 1, 2 тЖТ Class 0
  * 3, 4 тЖТ Class 1

---

### ЁЯФ╣ Decision Tree Stump Structure Output

```
|--- X <= 2.50
|   |--- class: 0
|--- X >  2.50
|   |--- class: 1
```

---

## 4я╕ПтГг ржПржЗ Output ржХрзА ржмрзЛржЭрж╛ржЪрзНржЫрзЗ? (ржЦрзБржм ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг)

### ЁЯФН Rule ржмрзНржпрж╛ржЦрзНржпрж╛:

```
if X <= 2.5 тЖТ class 0
else        тЖТ class 1
```

ЁЯСЙ ржПржЯрж╛ржЗ ржПржХржЯрж┐ **Decision Tree Stump**

* тЬФя╕П ржорж╛рждрзНрж░ **ржПржХржЯрж┐ feature (X)**
* тЬФя╕П ржорж╛рждрзНрж░ **ржПржХржЯрж┐ split (2.5)**
* тЬФя╕П depth = 1

---

## 5я╕ПтГг Visualize ржХрж░рзЗ ржнрж╛ржмрж▓рзЗ (Mind Map)

```
          X <= 2.5?
         /         \
      Yes           No
   (Class 0)     (Class 1)
```

---

## 6я╕ПтГг AdaBoost-ржПрж░ рж╕рж╛ржерзЗ Connection (Important)

AdaBoost:

* ржПрж░ржХржо **ржЕржирзЗржХржЧрзБрж▓рзЛ stump рждрзИрж░рж┐ ржХрж░рзЗ**
* ржкрзНрж░рждрзНржпрзЗржХржЯрж╛рж░ ржЬржирзНржп **alpha (importance)** рж╣рж┐рж╕рж╛ржм ржХрж░рзЗ
* рж╢рзЗрж╖рзЗ **weighted voting** ржХрж░рзЗ

ЁЯСЙ ржПржЗ stump-ржЯрж╛ржЗ AdaBoost-ржПрж░ **basic soldier** ЁЯТк

---

## 7я╕ПтГг Interview / Viva ржкрзНрж░рж╢рзНржи (with this example)

**Q:** Decision stump-ржПрж░ split value 2.5 ржХрзЗржи?
ЁЯСЙ ржХрж╛рж░ржг 2 ржЖрж░ 3-ржПрж░ ржорж╛ржЭржЦрж╛ржирзЗ split ржХрж░рж▓рзЗ error рж╕ржмржЪрзЗрзЯрзЗ ржХржо рж╣рзЯ

**Q:** max_depth=1 ржжрж┐рж▓рзЗ ржХрзА рж╣рзЯ?
ЁЯСЙ Tree рж╢рзБржзрзБ root + 2 leaf рждрзИрж░рж┐ ржХрж░рждрзЗ ржкрж╛рж░рзЗ

---

## ЁЯза One-line Summary

> **Decision Tree Stump = One rule тЖТ Fast тЖТ Weak тЖТ Perfect for AdaBoost**

---


