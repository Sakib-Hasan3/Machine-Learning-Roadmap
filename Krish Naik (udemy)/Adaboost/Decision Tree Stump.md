---

## üå≥ Decision Tree Stump ‚Äì Code + Example + Output

---

## 1Ô∏è‚É£ Example Dataset (‡¶∏‡¶π‡¶ú)

‡¶ß‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶ï ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶õ‡ßã‡¶ü dataset ‡¶Ü‡¶õ‡ßá:

| Feature (X) | Label (y) |
| ----------- | --------- |
| 1           | 0         |
| 2           | 0         |
| 3           | 1         |
| 4           | 1         |

üëâ Task: Binary Classification
üëâ Model: **Decision Tree Stump (depth = 1)**

---

## 2Ô∏è‚É£ Python Code (Decision Tree Stump)

```python
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_text

# Step 1: Dataset
X = np.array([[1], [2], [3], [4]])
y = np.array([0, 0, 1, 1])

# Step 2: Create Decision Tree Stump
stump = DecisionTreeClassifier(
    max_depth=1,
    criterion='gini'
)

# Step 3: Train model
stump.fit(X, y)

# Step 4: Predictions
predictions = stump.predict(X)

# Step 5: Show tree structure
tree_rules = export_text(stump, feature_names=["X"])

print("Predictions:", predictions)
print("\nDecision Tree Stump Structure:\n")
print(tree_rules)
```

---

## 3Ô∏è‚É£ Output (‡¶è‡¶á ‡¶ï‡ßã‡¶° ‡¶ö‡¶æ‡¶≤‡¶æ‡¶≤‡ßá ‡¶Ø‡¶æ ‡¶¶‡ßá‡¶ñ‡¶æ‡¶¨‡ßá)

### üîπ Predictions Output

```
Predictions: [0 0 1 1]
```

üìå ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ:

* Input: `[1, 2, 3, 4]`
* Model ‡¶∂‡¶ø‡¶ñ‡ßá‡¶õ‡ßá:

  * 1, 2 ‚Üí Class 0
  * 3, 4 ‚Üí Class 1

---

### üîπ Decision Tree Stump Structure Output

```
|--- X <= 2.50
|   |--- class: 0
|--- X >  2.50
|   |--- class: 1
```

---

## 4Ô∏è‚É£ ‡¶è‡¶á Output ‡¶ï‡ßÄ ‡¶¨‡ßã‡¶ù‡¶æ‡¶ö‡ßç‡¶õ‡ßá? (‡¶ñ‡ßÅ‡¶¨ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£)

### üîç Rule ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ:

```
if X <= 2.5 ‚Üí class 0
else        ‚Üí class 1
```

üëâ ‡¶è‡¶ü‡¶æ‡¶á ‡¶è‡¶ï‡¶ü‡¶ø **Decision Tree Stump**

* ‚úîÔ∏è ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ **‡¶è‡¶ï‡¶ü‡¶ø feature (X)**
* ‚úîÔ∏è ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ **‡¶è‡¶ï‡¶ü‡¶ø split (2.5)**
* ‚úîÔ∏è depth = 1

---

## 5Ô∏è‚É£ Visualize ‡¶ï‡¶∞‡ßá ‡¶≠‡¶æ‡¶¨‡¶≤‡ßá (Mind Map)

```
          X <= 2.5?
         /         \
      Yes           No
   (Class 0)     (Class 1)
```

---

## 6Ô∏è‚É£ AdaBoost-‡¶è‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá Connection (Important)

AdaBoost:

* ‡¶è‡¶∞‡¶ï‡¶Æ **‡¶Ö‡¶®‡ßá‡¶ï‡¶ó‡ßÅ‡¶≤‡ßã stump ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá**
* ‡¶™‡ßç‡¶∞‡¶§‡ßç‡¶Ø‡ßá‡¶ï‡¶ü‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø **alpha (importance)** ‡¶π‡¶ø‡¶∏‡¶æ‡¶¨ ‡¶ï‡¶∞‡ßá
* ‡¶∂‡ßá‡¶∑‡ßá **weighted voting** ‡¶ï‡¶∞‡ßá

üëâ ‡¶è‡¶á stump-‡¶ü‡¶æ‡¶á AdaBoost-‡¶è‡¶∞ **basic soldier** üí™

---

## 7Ô∏è‚É£ Interview / Viva ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® (with this example)

**Q:** Decision stump-‡¶è‡¶∞ split value 2.5 ‡¶ï‡ßá‡¶®?
üëâ ‡¶ï‡¶æ‡¶∞‡¶£ 2 ‡¶Ü‡¶∞ 3-‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶ù‡¶ñ‡¶æ‡¶®‡ßá split ‡¶ï‡¶∞‡¶≤‡ßá error ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶ï‡¶Æ ‡¶π‡ßü

**Q:** max_depth=1 ‡¶¶‡¶ø‡¶≤‡ßá ‡¶ï‡ßÄ ‡¶π‡ßü?
üëâ Tree ‡¶∂‡ßÅ‡¶ß‡ßÅ root + 2 leaf ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá

---

## üß† One-line Summary

> **Decision Tree Stump = One rule ‚Üí Fast ‚Üí Weak ‚Üí Perfect for AdaBoost**

---



