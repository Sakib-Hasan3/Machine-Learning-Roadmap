## âš–ï¸ **Normalising Weights & Assigning Bins (AdaBoost)**

*(à¦¬à¦¾à¦‚à¦²à¦¾à§Ÿ à¦ªà¦°à¦¿à¦·à§à¦•à¦¾à¦° à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾ + Code + Output à¦¸à¦¹)*

![Image](https://datamapu.com/images/adaboost/adaboost.png)

![Image](https://datamapu.com/images/adaboost/ab_example_clf_bins1.png)

![Image](https://daxg39y63pxwu.cloudfront.net/images/blog/adaboost-algorithm/AdaBoost_Algorithm_Explained_in_Depth.webp)

---

## 1ï¸âƒ£ Normalising Weights à¦®à¦¾à¦¨à§‡ à¦•à§€?

**Normalising weights** à¦®à¦¾à¦¨à§‡ à¦¹à¦²à§‹
à¦¸à¦¬ updated weight à¦à¦®à¦¨à¦­à¦¾à¦¬à§‡ à¦ à¦¿à¦• à¦•à¦°à¦¾ à¦¯à§‡à¦¨â€”

```
w1 + w2 + w3 + ... + wn = 1
```

ðŸ‘‰ à¦…à¦°à§à¦¥à¦¾à§Ž weight à¦—à§à¦²à§‹à¦•à§‡ **probability distribution** à¦¬à¦¾à¦¨à¦¾à¦¨à§‹à¥¤

ðŸ“Œ à¦•à§‡à¦¨ à¦¦à¦°à¦•à¦¾à¦°?

* Sampling à¦•à¦°à¦¾à¦° à¦œà¦¨à§à¦¯
* à¦ªà¦°à§‡à¦° weak learner à¦•à§‹à¦¨ data à¦•à¦¤à¦¬à¦¾à¦° à¦¦à§‡à¦–à¦¬à§‡ à¦¤à¦¾ à¦¨à¦¿à¦°à§à¦§à¦¾à¦°à¦£à§‡à¦° à¦œà¦¨à§à¦¯
* Mathematical stability à¦°à¦¾à¦–à¦¾à¦° à¦œà¦¨à§à¦¯

---

## 2ï¸âƒ£ Normalisation à¦¨à¦¾ à¦•à¦°à¦²à§‡ à¦•à§€ à¦¸à¦®à¦¸à§à¦¯à¦¾?

à¦§à¦°à¦¾ à¦¯à¦¾à¦• updated weights à¦¹à¦²à§‹:

```
[0.144, 0.433, 0.144, 0.144]
```

âŒ à¦¸à¦®à¦¸à§à¦¯à¦¾:

* Sum = 0.865 (â‰  1)
* à¦à¦—à§à¦²à§‹ probability à¦¹à¦¿à¦¸à§‡à¦¬à§‡ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¾ à¦¯à¦¾à§Ÿ à¦¨à¦¾

ðŸ‘‰ à¦¤à¦¾à¦‡ **normalisation à¦¬à¦¾à¦§à§à¦¯à¦¤à¦¾à¦®à§‚à¦²à¦•**

---

## 3ï¸âƒ£ Normalisation Formula

```
w_i(new) = w_i / Î£ w
```

---

## 4ï¸âƒ£ Example: Normalising Weights (Manual)

### ðŸ”¹ Before Normalisation

```
weights = [0.144, 0.433, 0.144, 0.144]
sum = 0.865
```

### ðŸ”¹ After Normalisation

```
[0.166, 0.500, 0.166, 0.166]
```

ðŸ“Œ à¦à¦–à¦¨ sum = 1.0 âœ…

---

## 5ï¸âƒ£ Assigning Bins à¦®à¦¾à¦¨à§‡ à¦•à§€?

**Bins** à¦¹à¦²à§‹ weight à¦…à¦¨à§à¦¯à¦¾à§Ÿà§€ à¦¤à§ˆà¦°à¦¿ à¦•à¦°à¦¾ **interval / range**,
à¦¯à¦¾à¦° à¦®à¦¾à¦§à§à¦¯à¦®à§‡ à¦†à¦®à¦°à¦¾ à¦¬à§à¦à¦¿â€”

> à¦•à§‹à¦¨ data point à¦•à¦¤ probability à¦¨à¦¿à§Ÿà§‡ à¦ªà¦°à§‡à¦° round-à¦ à¦†à¦¸à¦¬à§‡

ðŸ‘‰ à¦à¦Ÿà¦¾ à¦®à§‚à¦²à¦¤ **Weighted Sampling**-à¦à¦° à¦ªà§à¦°à¦¸à§à¦¤à§à¦¤à¦¿à¥¤

---

## 6ï¸âƒ£ Bins à¦•à§€à¦­à¦¾à¦¬à§‡ à¦¤à§ˆà¦°à¦¿ à¦¹à§Ÿ? (Concept)

à¦§à¦°à¦¾ à¦¯à¦¾à¦• normalized weights:

| Data | Weight |
| ---- | ------ |
| D1   | 0.166  |
| D2   | 0.500  |
| D3   | 0.166  |
| D4   | 0.166  |

### ðŸ”¹ Cumulative Sum (Bins)

| Data | Weight | Bin Range     |
| ---- | ------ | ------------- |
| D1   | 0.166  | 0.000 â€“ 0.166 |
| D2   | 0.500  | 0.166 â€“ 0.666 |
| D3   | 0.166  | 0.666 â€“ 0.832 |
| D4   | 0.166  | 0.832 â€“ 1.000 |

ðŸ“Œ à¦­à§à¦² data (D2) à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦¬à§œ bin à¦ªà§‡à§Ÿà§‡à¦›à§‡

---

## 7ï¸âƒ£ Random Sampling à¦¦à¦¿à§Ÿà§‡ Data à¦¨à¦¿à¦°à§à¦¬à¦¾à¦šà¦¨

à¦§à¦°à¦¾ à¦¯à¦¾à¦• random number = **0.4**

* 0.4 à¦ªà§œà§‡ â†’ **0.166 â€“ 0.666**
* ðŸ‘‰ **D2 selected**

ðŸ‘‰ à¦à¦­à¦¾à¦¬à§‡ à¦­à§à¦² data à¦¬à¦¾à¦°à¦¬à¦¾à¦° à¦¨à¦¿à¦°à§à¦¬à¦¾à¦šà¦¿à¦¤ à¦¹à§Ÿ

---

## 8ï¸âƒ£ Python Code: Normalising Weights + Assigning Bins

```python
import numpy as np

# Updated weights (before normalization)
weights = np.array([0.144, 0.433, 0.144, 0.144])

# Step 1: Normalize
norm_weights = weights / np.sum(weights)

# Step 2: Create cumulative bins
bins = np.cumsum(norm_weights)

print("Normalized Weights:", np.round(norm_weights, 3))
print("Bins:", np.round(bins, 3))
```

---

## 9ï¸âƒ£ Output (Expected)

```
Normalized Weights: [0.166 0.5   0.166 0.166]
Bins: [0.166 0.666 0.832 1.   ]
```

---

## ðŸ”Ÿ Random Sampling Example (Optional but Important)

```python
# Random number between 0 and 1
r = 0.4

selected_index = np.where(bins >= r)[0][0]
print("Random number:", r)
print("Selected data index:", selected_index)
```

### ðŸ”¹ Output

```
Random number: 0.4
Selected data index: 1
```

ðŸ‘‰ Index 1 = à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦­à§à¦² data point ðŸ˜Ž

---

## 1ï¸âƒ£1ï¸âƒ£ AdaBoost Loop-à¦ à¦à¦‡ à¦§à¦¾à¦ª à¦•à§‹à¦¥à¦¾à§Ÿ?

```
Train weak learner
â†“
Calculate error
â†“
Calculate alpha
â†“
Update weights
â†“
Normalize weights   â† âœ…
â†“
Assign bins         â† âœ…
â†“
Weighted sampling
â†“
Next iteration
```

---

## 1ï¸âƒ£2ï¸âƒ£ Interview / Viva à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£ à¦ªà§à¦°à¦¶à§à¦¨

**Q1:** Normalisation à¦•à§‡à¦¨ à¦¦à¦°à¦•à¦¾à¦°?
ðŸ‘‰ Probability distribution à¦¬à¦¾à¦¨à¦¾à¦¨à§‹à¦° à¦œà¦¨à§à¦¯

**Q2:** Bins à¦•à§€ represent à¦•à¦°à§‡?
ðŸ‘‰ Data point-à¦à¦° selection probability

**Q3:** Wrong data à¦•à§‡à¦¨ à¦¬à§œ bin à¦ªà¦¾à§Ÿ?
ðŸ‘‰ à¦¯à§‡à¦¨ à¦ªà¦°à§‡à¦° learner à¦¬à§‡à¦¶à¦¿ focus à¦•à¦°à§‡

**Q4:** AdaBoost à¦•à¦¿ à¦¸à¦¬à¦¸à¦®à§Ÿ sampling à¦•à¦°à§‡?
ðŸ‘‰ Conceptually à¦¹à§à¦¯à¦¾à¦, implementation à¦­à§‡à¦¦à§‡ à¦­à¦¿à¦¨à§à¦¨ à¦¹à¦¤à§‡ à¦ªà¦¾à¦°à§‡

---

## ðŸ§  One-line Memory Trick

> **Normalize â†’ Make bins â†’ Sample â†’ Focus on mistakes**

---

