![Image](https://miro.medium.com/1%2AqG4PEnoSWQRLoEL_P1ruhw.jpeg)

![Image](https://www.visiondummy.com/wp-content/uploads/2014/04/eigenvectors_covariance.png)

![Image](https://georgemdallas.wordpress.com/wp-content/uploads/2013/10/pca13.jpg)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/0%2AfSfgExsETK72Si3-)

## ðŸ”· **Eigen Decomposition on Covariance Matrix** â€” à¦¸à¦¹à¦œ à¦•à¦¿à¦¨à§à¦¤à§ à¦—à¦­à§€à¦° à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾ (PCA-à¦à¦° à¦¹à§ƒà¦¦à§Ÿ)

à¦à¦Ÿà¦¾ à¦¬à§à¦à¦²à§‡ PCA à¦ªà§à¦°à§‹ à¦ªà¦°à¦¿à¦·à§à¦•à¦¾à¦° à¦¹à§Ÿà§‡ à¦¯à¦¾à§Ÿà¥¤ à¦¨à¦¿à¦šà§‡ **à¦•à¦¿ â†’ à¦•à§‡à¦¨ â†’ à¦•à¦¿à¦­à¦¾à¦¬à§‡ â†’ à¦‰à¦¦à¦¾à¦¹à¦°à¦£**â€”à¦¸à¦¬ à¦§à¦¾à¦ªà§‡ à¦§à¦¾à¦ªà§‡à¥¤

---

## ðŸ§  à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡à¦° à¦¸à¦¾à¦°à¦•à¦¥à¦¾

> **Covariance matrix à¦¬à¦²à§‡ à¦¦à§‡à§Ÿ à¦¡à§‡à¦Ÿà¦¾ à¦•à§‹à¦¨ à¦¦à¦¿à¦•à¦—à§à¦²à§‹à¦¤à§‡ à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦›à§œà¦¾à§Ÿ;
> Eigen decomposition à¦¸à§‡à¦‡ à¦¦à¦¿à¦•à¦—à§à¦²à§‹ (eigenvectors) à¦“ à¦¤à¦¾à¦¦à§‡à¦° à¦¶à¦•à§à¦¤à¦¿ (eigenvalues) à¦¬à§‡à¦° à¦•à¦°à§‡à¥¤**

---

# 1ï¸âƒ£ Covariance Matrix à¦•à§‡à¦¨ à¦¦à¦°à¦•à¦¾à¦°?

à¦§à¦°à¦¾ à¦¯à¦¾à¦• à¦†à¦ªà¦¨à¦¾à¦° à¦¡à§‡à¦Ÿà¦¾ (X) (mean-centered à¦•à¦°à¦¾ à¦¹à§Ÿà§‡à¦›à§‡)à¥¤

Covariance matrix:
[
\Sigma = \frac{1}{n-1} X^T X
]

à¦à¦Ÿà¦¾ à¦¬à¦²à§‡:

* à¦•à§‹à¦¨ feature à¦œà§‹à§œà¦¾ **à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦¬à¦¾à§œà§‡/à¦•à¦®à§‡** (covariance)
* à¦•à§‹à¦¨ à¦¦à¦¿à¦• à¦¬à¦°à¦¾à¦¬à¦° **variance à¦¬à§‡à¦¶à¦¿**

ðŸ“Œ PCA-à¦° à¦ªà§à¦°à¦¶à§à¦¨:

> â€œà¦¡à§‡à¦Ÿà¦¾ à¦•à§‹à¦¨ **direction**-à¦ à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦¬à§‡à¦¶à¦¿ à¦›à§œà¦¾à¦¨à§‹?â€

---

# 2ï¸âƒ£ Eigen Decomposition à¦•à§€?

Covariance matrix (\Sigma) à¦•à§‡ à¦­à¦¾à¦™à¦¾ à¦¹à§Ÿ:

[
\Sigma = Q \Lambda Q^T
]

à¦¯à§‡à¦–à¦¾à¦¨à§‡â€”

* **(Q)** = eigenvectors (columns) â†’ **à¦¦à¦¿à¦• (axes)**
* **(\Lambda)** = diagonal eigenvalues â†’ **à¦ à¦¦à¦¿à¦•à§‡à¦° variance**

ðŸ‘‰ à¦à¦‡ à¦­à¦¾à¦™à¦¾à¦Ÿà¦¾à¦‡ **Eigen Decomposition**à¥¤

---

# 3ï¸âƒ£ Eigenvector & Eigenvalue-à¦à¦° à¦…à¦°à§à¦¥ (Intuition)

### ðŸ”¹ Eigenvector (à¦¦à¦¿à¦•)

* à¦¨à¦¤à§à¦¨ axis (PC direction)
* à¦¯à§‡à¦¦à¦¿à¦•à§‡ à¦¡à§‡à¦Ÿà¦¾ à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦¸à§à¦¨à§à¦¦à¦°à¦­à¦¾à¦¬à§‡ line up à¦•à¦°à§‡

### ðŸ”¹ Eigenvalue (à¦¶à¦•à§à¦¤à¦¿)

* à¦ axis à¦¬à¦°à¦¾à¦¬à¦° à¦¡à§‡à¦Ÿà¦¾ **à¦•à¦¤à¦Ÿà¦¾ à¦›à§œà¦¾à¦¨à§‹ (variance)**

ðŸ“Œ Largest eigenvalue â†” **PC1**
ðŸ“Œ 2nd largest â†” **PC2** (PC1-à¦à¦° à¦¸à¦¾à¦¥à§‡ 90Â°)

---

# 4ï¸âƒ£ à¦•à§‡à¦¨ Covariance matrix-à¦à¦‡ decomposition?

à¦•à¦¾à¦°à¦£ PCA solve à¦•à¦°à§‡:
[
\max_{|w|=1} ; w^T \Sigma w
]

à¦à¦‡ optimization-à¦à¦° solution à¦¹à¦²à§‹:
[
\Sigma w = \lambda w
]

ðŸ‘‰ à¦à¦Ÿà¦¿à¦‡ **Eigenvalue Equation**
ðŸ‘‰ à¦¤à¦¾à¦‡ covariance matrix-à¦à¦° eigenvectors-à¦‡ PCA-à¦à¦° principal directionsà¥¤

---

# 5ï¸âƒ£ à¦›à§‹à¦Ÿ 2D Numeric Example (à¦¹à¦¾à¦¤à§‡-à¦•à¦²à¦®à§‡ intuition)

à¦§à¦°à¦¾ à¦¯à¦¾à¦• mean-centered à¦¡à§‡à¦Ÿà¦¾ à¦¥à§‡à¦•à§‡ covariance matrix:

[
\Sigma =
\begin{bmatrix}
4 & 3 \
3 & 4
\end{bmatrix}
]

### Eigenvalues:

[
\lambda_1 = 7,\quad \lambda_2 = 1
]

### Eigenvectors (normalized):

* (w_1 = \frac{1}{\sqrt{2}}(1,1))
* (w_2 = \frac{1}{\sqrt{2}}(1,-1))

ðŸ§  à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾:

* (w_1) à¦¦à¦¿à¦• à¦¬à¦°à¦¾à¦¬à¦° variance = **7** (à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦¬à§‡à¦¶à¦¿) â†’ **PC1**
* (w_2) à¦¦à¦¿à¦• à¦¬à¦°à¦¾à¦¬à¦° variance = **1** â†’ **PC2**

ðŸ‘‰ à¦¡à§‡à¦Ÿà¦¾ diagonal ((1,1)) à¦¦à¦¿à¦•à§‡à¦‡ à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦¬à§‡à¦¶à¦¿ à¦›à§œà¦¾à¦¨à§‹à¥¤

---

# 6ï¸âƒ£ PCA-à¦¤à§‡ à¦•à§€à¦­à¦¾à¦¬à§‡ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à§Ÿ? (Step-by-step)

1. **Standardize** à¦¡à§‡à¦Ÿà¦¾
2. **Covariance matrix** à¦¬à¦¾à¦¨à¦¾à¦“
3. **Eigen decomposition** à¦•à¦°à§‹
4. **Eigenvalues** à¦¬à§œ à¦¥à§‡à¦•à§‡ à¦›à§‹à¦Ÿ à¦¸à¦¾à¦œà¦¾à¦“
5. Corresponding **Eigenvectors** à¦¨à¦¾à¦“
6. Top-(k) eigenvectors à¦¦à¦¿à§Ÿà§‡ data **project** à¦•à¦°à§‹

Projection:
[
Z = X W_k
]

* (W_k) = top-(k) eigenvectors

---

# 7ï¸âƒ£ Orthogonality à¦•à§‡à¦¨ à¦†à¦¸à§‡?

Covariance matrix symmetric â‡’

* Eigenvectors **orthogonal** (90Â°)

ðŸ“Œ à¦®à¦¾à¦¨à§‡:

* PCs à¦à¦•à§‡ à¦…à¦ªà¦°à§‡à¦° à¦¤à¦¥à§à¦¯ à¦ªà§à¦¨à¦°à¦¾à¦¬à§ƒà¦¤à§à¦¤à¦¿ à¦•à¦°à§‡ à¦¨à¦¾
* à¦ªà§à¦°à¦¤à§à¦¯à§‡à¦•à¦Ÿà¦¾ PC à¦¨à¦¤à§à¦¨ variance à¦¯à§‹à¦— à¦•à¦°à§‡

---

# 8ï¸âƒ£ Variance Explained (à¦•à§‡à¦¨ eigenvalues à¦¦à¦°à¦•à¦¾à¦°)

Total variance:
[
\sum_i \lambda_i
]

Explained variance ratio:
[
\frac{\lambda_k}{\sum_i \lambda_i}
]

ðŸ‘‰ Top PCs à¦°à¦¾à¦–à¦¿ à¦¯à¦¤à¦•à§à¦·à¦£ à¦¨à¦¾ 90â€“95% variance à¦§à¦°à¦¾ à¦ªà§œà§‡à¥¤

---

# 9ï¸âƒ£ Geometry-à¦° à¦¸à¦¾à¦¥à§‡ à¦®à¦¿à¦²

* **Eigenvectors** = à¦¨à¦¤à§à¦¨ à¦˜à§‹à¦°à¦¾à¦¨à§‹ axes
* **Eigenvalues** = axes-à¦à¦° à¦²à¦®à§à¦¬à¦¾/à¦›à§‹à¦Ÿ à¦¹à¦“à§Ÿà¦¾
* PCA = axes à¦à¦®à¦¨à¦­à¦¾à¦¬à§‡ à¦˜à§‹à¦°à¦¾à¦¨à§‹ à¦¯à¦¾à¦¤à§‡ à¦²à¦®à§à¦¬à¦¾ à¦¦à¦¿à¦•à¦—à§à¦²à§‹ à¦†à¦—à§‡ à¦§à¦°à¦¾ à¦ªà§œà§‡

---

## ðŸ”‘ Golden Answer (Exam/Viva)

> **Eigen decomposition of the covariance matrix yields eigenvectors that define the principal component directions and eigenvalues that quantify the variance captured along those directions.**

---


