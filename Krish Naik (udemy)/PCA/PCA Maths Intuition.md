![Image](https://miro.medium.com/v2/resize%3Afit%3A1168/1%2A_zzCYEUm1KSjBMgEDJCS0A.png)

![Image](https://miro.medium.com/1%2AqG4PEnoSWQRLoEL_P1ruhw.jpeg)

![Image](https://miro.medium.com/1%2AAGm7H5hp32qTH4hoOhamcw.jpeg)

![Image](https://i.sstatic.net/XFngC.png)

## ðŸ”¢ PCA â€” **Maths Intuition** (à¦à¦•à¦¦à¦® à¦¸à¦¹à¦œà¦­à¦¾à¦¬à§‡, step-by-step)

à¦à¦–à¦¾à¦¨à§‡ à¦†à¦®à¦°à¦¾ PCA-à¦° **à¦—à¦¾à¦£à¦¿à¦¤à¦¿à¦• à¦§à¦¾à¦°à¦£à¦¾** à¦¬à§à¦à¦¬
ðŸ‘‰ à¦•à¦¿à¦¨à§à¦¤à§ à¦­à¦¾à¦°à§€ à¦®à§à¦¯à¦¾à¦¥ à¦¨à¦¾
ðŸ‘‰ à¦¶à§à¦§à§ *à¦•à§‡à¦¨ à¦à¦‡ à¦«à¦°à§à¦®à§à¦²à¦¾ à¦†à¦¸à§‡* à¦¸à§‡à¦Ÿà¦¾ à¦¬à§‹à¦à¦¾à¦° à¦œà¦¨à§à¦¯

---

## ðŸ§  à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡à¦° Maths Idea

> **PCA à¦à¦®à¦¨ à¦à¦•à¦Ÿà¦¿ direction à¦–à§‹à¦à¦œà§‡, à¦¯à§‡à¦¦à¦¿à¦•à§‡ à¦¡à§‡à¦Ÿà¦¾ project à¦•à¦°à¦²à§‡ variance à¦¸à¦°à§à¦¬à§‹à¦šà§à¦š à¦¹à§Ÿà¥¤**

à¦à¦‡ à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡à¦° à¦­à§‡à¦¤à¦°à§‡à¦‡ PCA-à¦° à¦ªà§à¦°à§‹ à¦®à§à¦¯à¦¾à¦¥ à¦²à§à¦•à¦¾à¦¨à§‹à¥¤

---

# 1ï¸âƒ£ PCA à¦†à¦¸à¦²à§‡ à¦•à§‹à¦¨ problem solve à¦•à¦°à§‡?

à¦§à¦°à¦¾ à¦¯à¦¾à¦•:

* à¦†à¦ªà¦¨à¦¾à¦° à¦•à¦¾à¦›à§‡ data matrix à¦†à¦›à§‡:

[
X \in \mathbb{R}^{n \times d}
]

* (n) = number of samples
* (d) = number of features

ðŸ‘‰ à¦†à¦®à¦°à¦¾ à¦šà¦¾à¦‡:

* à¦¨à¦¤à§à¦¨ à¦à¦•à¦Ÿà¦¿ direction (w)
* à¦¯à§‡à¦¦à¦¿à¦•à§‡ data project à¦•à¦°à¦²à§‡ **variance à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦¬à§‡à¦¶à¦¿**

---

# 2ï¸âƒ£ Projection (à¦—à¦¾à¦£à¦¿à¦¤à¦¿à¦•à¦­à¦¾à¦¬à§‡)

à¦à¦•à¦Ÿà¦¾ data point (x_i) à¦•à§‡ direction (w)-à¦à¦° à¦‰à¦ªà¦° project à¦•à¦°à¦²à§‡ à¦ªà¦¾à¦‡:

[
z_i = w^T x_i
]

à¦à¦Ÿà¦¾à¦‡ **PC score**

---

# 3ï¸âƒ£ Variance of projection

à¦†à¦®à¦°à¦¾ à¦šà¦¾à¦‡ à¦à¦‡ projected values-à¦à¦° variance à¦¸à¦°à§à¦¬à§‹à¦šà§à¦š à¦¹à§‹à¦•:

[
\text{Var}(z) = \text{Var}(w^T X)
]

Math à¦¬à¦²à§‡:

[
\text{Var}(w^T X) = w^T \Sigma w
]

à¦¯à§‡à¦–à¦¾à¦¨à§‡
(\Sigma) = **Covariance Matrix**

---

# 4ï¸âƒ£ PCA-à¦° à¦®à§‚à¦² Optimization Problem

ðŸ‘‰ PCA solve à¦•à¦°à§‡ à¦à¦‡ problem:

[
\max_w \quad w^T \Sigma w
]

à¦¶à¦°à§à¦¤ (constraint):

[
|w| = 1
]

ðŸ“Œ à¦•à§‡à¦¨ constraint?

* à¦¨à¦¾ à¦¦à¦¿à¦²à§‡ (w) à¦…à¦¸à§€à¦® à¦¬à§œ à¦•à¦°à§‡ variance à¦¬à¦¾à§œà¦¾à¦¨à§‹ à¦¯à§‡à¦¤ (cheating ðŸ˜„)

---

# 5ï¸âƒ£ à¦à¦‡ optimization solve à¦•à¦°à¦²à§‡ à¦•à§€ à¦¹à§Ÿ?

à¦à¦‡ problem-à¦à¦° solution à¦¹à¦²à§‹:

[
\Sigma w = \lambda w
]

ðŸ‘‰ à¦à¦Ÿà¦¾ à¦¹à¦²à§‹ **Eigenvalue Equation**

à¦à¦–à¦¾à¦¨ à¦¥à§‡à¦•à§‡à¦‡ à¦†à¦¸à§‡:

* **Eigenvector** (w)
* **Eigenvalue** (\lambda)

---

# 6ï¸âƒ£ Eigenvalue & Eigenvector à¦à¦° Maths Meaning

### ðŸ”¹ Eigenvector

* Direction (axis)
* à¦¯à§‡à¦¦à¦¿à¦•à§‡ data à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦­à¦¾à¦²à§‹à¦­à¦¾à¦¬à§‡ à¦›à§œà¦¾à¦¨à§‹

### ðŸ”¹ Eigenvalue

* à¦ direction-à¦ variance à¦•à¦¤

ðŸ“Œ à¦¤à¦¾à¦‡:

* Largest eigenvalue â†’ **PC1**
* 2nd largest â†’ **PC2**

---

# 7ï¸âƒ£ à¦›à§‹à¦Ÿ Numeric Intuition (2D example)

à¦§à¦°à¦¾ à¦¯à¦¾à¦• covariance matrix:

[
\Sigma =
\begin{bmatrix}
4 & 3 \
3 & 4
\end{bmatrix}
]

Eigenvalues:
[
\lambda_1 = 7,\quad \lambda_2 = 1
]

ðŸ‘‰ à¦®à¦¾à¦¨à§‡:

* à¦à¦• à¦¦à¦¿à¦• à¦¦à¦¿à§Ÿà§‡ variance = 7 (à¦¬à§œ)
* à¦†à¦°à§‡à¦• à¦¦à¦¿à¦• à¦¦à¦¿à§Ÿà§‡ variance = 1 (à¦›à§‹à¦Ÿ)

ðŸ“Œ PCA à¦¬à¦²à§‡:

> â€œà§­ à¦¯à§‡à¦¦à¦¿à¦•à§‡, à¦¸à§‡à¦Ÿà¦¾à¦‡ PC1â€

---

# 8ï¸âƒ£ Why PCs are Orthogonal? (à§¯à§¦Â° à¦•à§‡à¦¨?)

Math constraint à¦¥à§‡à¦•à§‡ à¦†à¦¸à§‡:

[
w_1^T w_2 = 0
]

ðŸ‘‰ à¦®à¦¾à¦¨à§‡:

* PC1 âŸ‚ PC2

ðŸ“Œ à¦•à¦¾à¦°à¦£:

* à¦à¦•à¦‡ variance à¦¦à§à¦¬à¦¾à¦° à¦—à¦£à¦¨à¦¾ à¦•à¦°à¦¾ à¦¯à¦¾à¦¬à§‡ à¦¨à¦¾
* à¦¨à¦¤à§à¦¨ PC-à¦•à§‡ **à¦¨à¦¤à§à¦¨ information** à¦†à¦¨à¦¤à§‡ à¦¹à¦¬à§‡

---

# 9ï¸âƒ£ Dimensionality Reduction (Math view)

à¦§à¦°à¦¾ à¦¯à¦¾à¦•:

* Total variance = (\lambda_1 + \lambda_2 + \dots + \lambda_d)

à¦†à¦®à¦°à¦¾ à¦ªà§à¦°à¦¥à¦® (k)à¦Ÿà¦¾ PC à¦°à¦¾à¦–à¦¿ à¦¯à¦¦à¦¿:

[
\frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^d \lambda_i} \ge 0.9
]

ðŸ‘‰ à¦®à¦¾à¦¨à§‡: **90% information retained**

---

# ðŸ”„ PCA Transformation Formula

à¦¨à¦¤à§à¦¨ data:

[
Z = X W
]

à¦¯à§‡à¦–à¦¾à¦¨à§‡:

* (W = [w_1, w_2, \dots, w_k])
* (Z) = reduced dataset

---

# ðŸ§  Maths Intuition à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡ à¦†à¦¬à¦¾à¦°

* **Covariance** â†’ à¦•à§‹à¦¨ direction-à¦ data à¦›à§œà¦¾à§Ÿ
* **Eigenvector** â†’ à¦¸à§‡à¦‡ direction
* **Eigenvalue** â†’ à¦›à§œà¦¾à¦¨à§‹à¦° à¦ªà¦°à¦¿à¦®à¦¾à¦£

ðŸ‘‰ PCA = **Variance maximization under unit-length constraint**

---

## ðŸ”‘ Exam / Viva Golden Answer

> **Mathematically, PCA finds the eigenvectors of the covariance matrix corresponding to the largest eigenvalues, which maximize the variance of the projected data.**

---

