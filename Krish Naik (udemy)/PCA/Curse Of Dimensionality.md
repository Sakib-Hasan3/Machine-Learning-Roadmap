![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2A2_IV9Mg3rDmzL3ssCnW0UA.png)

![Image](https://people.eecs.berkeley.edu/~jrs/highd/counterintuitive-properties-of-high-dimensional-space/Figure2.png)

![Image](https://i.sstatic.net/DUZKm.png)

![Image](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/images/c2/cursefigure.png)

## ðŸ”¥ Curse of Dimensionality â€” Machine Learning à¦ (à¦¬à¦¾à¦‚à¦²à¦¾à§Ÿ à¦¬à¦¿à¦¸à§à¦¤à¦¾à¦°à¦¿à¦¤ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾)

### ðŸ“Œ Curse of Dimensionality à¦•à§€?

**Curse of Dimensionality** à¦®à¦¾à¦¨à§‡ à¦¹à¦²à§‹â€”
ðŸ‘‰ **feature (dimension) à¦¯à¦¤ à¦¬à¦¾à§œà§‡, à¦¡à§‡à¦Ÿà¦¾ à¦¤à¦¤ sparse (à¦›à§œà¦¾à¦¨à§‹) à¦¹à§Ÿà§‡ à¦¯à¦¾à§Ÿ**,
ðŸ‘‰ à¦«à¦²à§‡ **distance, density, learningâ€”à¦¸à¦¬ à¦•à¦¿à¦›à§ à¦­à§‡à¦™à§‡ à¦ªà§œà§‡**à¥¤

à¦¸à¦¹à¦œ à¦•à¦¥à¦¾à§Ÿ:

> **à¦¬à§‡à¦¶à¦¿ feature â‰  à¦­à¦¾à¦²à§‹ model**
> à¦…à¦¨à§‡à¦• à¦¸à¦®à§Ÿ à¦‰à¦²à§à¦Ÿà§‹à¦Ÿà¦¾ à¦¹à§Ÿà¥¤

---

## ðŸ§  à¦•à§‡à¦¨ à¦à¦•à§‡ â€œCurseâ€ à¦¬à¦²à¦¾ à¦¹à§Ÿ?

à¦•à¦¾à¦°à¦£ dimension à¦¬à¦¾à§œà¦¾à¦° à¦¸à¦¾à¦¥à§‡ à¦¸à¦¾à¦¥à§‡ à¦¸à¦®à¦¸à§à¦¯à¦¾à¦—à§à¦²à§‹ **exponential** à¦¹à¦¾à¦°à§‡ à¦¬à¦¾à§œà§‡à¥¤

---

# ðŸ§© 1ï¸âƒ£ Distance Meaningless à¦¹à§Ÿà§‡ à¦¯à¦¾à§Ÿ

### 1D (à¦à¦•à¦Ÿà¦¾ feature)

```
â€¢â€”â€”â€¢â€”â€”â€¢â€”â€”â€¢
```

Point à¦—à§à¦²à§‹ à¦•à¦¾à¦›à¦¾à¦•à¦¾à¦›à¦¿â€”distance à¦¬à§‹à¦à¦¾ à¦¯à¦¾à§Ÿà¥¤

### 2D

```
â€¢     â€¢
   â€¢
        â€¢
```

### 100D ðŸ˜µ

à¦¸à¦¬ point à¦ªà§à¦°à¦¾à§Ÿ **à¦¸à¦®à¦¾à¦¨ à¦¦à§‚à¦°à§‡**!

ðŸ“Œ à¦«à¦²à¦¾à¦«à¦²:

* Nearest Neighbor â‰ˆ Far Neighbor
* KNN, K-Means à¦­à§‡à¦™à§‡ à¦ªà§œà§‡

ðŸ‘‰ **Distance-based algorithm fail à¦•à¦°à§‡**

---

# ðŸ§© 2ï¸âƒ£ Data Sparse à¦¹à§Ÿà§‡ à¦¯à¦¾à§Ÿ (à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦¬à§œ à¦¸à¦®à¦¸à§à¦¯à¦¾)

à¦§à¦°à¦¾ à¦¯à¦¾à¦•:

* 1D â†’ 10 point à¦¦à¦¿à¦²à§‡à¦‡ à¦­à¦¾à¦²à§‹ coverage
* 10D â†’ à¦à¦•à¦‡ coverage à¦ªà§‡à¦¤à§‡ à¦²à¦¾à¦—à¦¬à§‡ **10Â¹â° point** ðŸ˜±

ðŸ“Œ à¦¬à¦¾à¦¸à§à¦¤à¦¬à§‡ à¦à¦¤ à¦¡à§‡à¦Ÿà¦¾ à¦¥à¦¾à¦•à§‡ à¦¨à¦¾
ðŸ‘‰ Model **generalize à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à§‡ à¦¨à¦¾**

---

# ðŸ§© 3ï¸âƒ£ Overfitting à¦­à§Ÿà¦‚à¦•à¦°à¦­à¦¾à¦¬à§‡ à¦¬à¦¾à§œà§‡

High dimension à¦®à¦¾à¦¨à§‡:

* Feature à¦…à¦¨à§‡à¦•
* Data à¦•à¦®

ðŸ‘‰ Model noise à¦®à§à¦–à¦¸à§à¦¥ à¦•à¦°à§‡ à¦«à§‡à¦²à§‡

ðŸ“Œ Training accuracy â†‘
ðŸ“Œ Test accuracy â†“â†“â†“

---

# ðŸ§© 4ï¸âƒ£ Computation à¦–à¦°à¦š à¦†à¦•à¦¾à¦¶à¦šà§à¦®à§à¦¬à§€

* Distance calculation = à¦§à§€à¦°
* Memory = à¦¬à§‡à¦¶à¦¿
* Training = Slow

ðŸ“Œ Big data + High dimension = ðŸš¨

---

# ðŸ§ª Real-life Example (Student Dataset)

### Dataset:

```
Math, Physics, Chemistry, Biology, English
Attendance, Sleep, PhoneUsage, Stress, Diet
```

âž¡ï¸ 10 features

à¦•à¦¿à¦¨à§à¦¤à§:

* Mathâ€“Physicsâ€“Chemistry correlated
* Biologyâ€“English correlated

ðŸ‘‰ Effective information à¦¹à§Ÿà¦¤à§‹ **3â€“4 dimension**

ðŸ“Œ à¦¬à¦¾à¦•à¦¿ feature à¦—à§à¦²à§‹ **curse à¦¤à§ˆà¦°à¦¿ à¦•à¦°à¦›à§‡**

---

# ðŸ§ª Image Dataset Example (à¦¸à¦¬à¦šà§‡à§Ÿà§‡ classic)

* Image size: 64Ã—64 = **4096 features**
* Data points: 2000 image

ðŸ“Œ 4096D space-à¦ 2000 point = à¦ªà§à¦°à¦¾à§Ÿ à¦•à¦¿à¦›à§à¦‡ à¦¨à¦¾
ðŸ‘‰ Distance meaningless
ðŸ‘‰ Model overfit

---

# ðŸ§  à¦•à§‡à¦¨ Human intuition à¦•à¦¾à¦œ à¦•à¦°à§‡ à¦¨à¦¾?

à¦†à¦®à¦°à¦¾ 3D-à¦à¦° à¦¬à§‡à¦¶à¦¿ à¦•à¦²à§à¦ªà¦¨à¦¾ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¿ à¦¨à¦¾
à¦•à¦¿à¦¨à§à¦¤à§ ML algorithm-à¦•à§‡ 1000D-à¦¤à§‡ à¦•à¦¾à¦œ à¦•à¦°à¦¤à§‡ à¦¹à§Ÿ

ðŸ‘‰ à¦¸à§‡à¦–à¦¾à¦¨à§‡à¦‡ curse

---

# ðŸ› ï¸ Curse à¦¥à§‡à¦•à§‡ à¦¬à¦¾à¦à¦šà¦¾à¦° à¦‰à¦ªà¦¾à§Ÿ (à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£ à¦…à¦‚à¦¶)

## âœ… 1ï¸âƒ£ Dimensionality Reduction

* **PCA**
* Autoencoder
* t-SNE / UMAP (visualization)

ðŸ‘‰ Feature â†“
ðŸ‘‰ Information â‰ˆ same

---

## âœ… 2ï¸âƒ£ Feature Selection

* Correlation-based
* Tree-based importance
* Mutual Information

ðŸ‘‰ à¦…à¦ªà§à¦°à§Ÿà§‹à¦œà¦¨à§€à§Ÿ feature à¦¬à¦¾à¦¦

---

## âœ… 3ï¸âƒ£ Regularization

* L1 (Lasso)
* L2 (Ridge)

ðŸ‘‰ Overfitting à¦•à¦®à¦¾à§Ÿ

---

## âœ… 4ï¸âƒ£ More Data (à¦¸à¦¬à¦¸à¦®à§Ÿ à¦¸à¦®à§à¦­à¦¬ à¦¨à¦¾)

* Data augmentation
* Synthetic data

---

# ðŸ“Š Algorithm-wise Effect

| Algorithm         | Curse-à¦à¦° à¦ªà§à¦°à¦­à¦¾à¦¬        |
| ----------------- | ---------------------- |
| KNN               | âŒ à¦–à§à¦¬ à¦–à¦¾à¦°à¦¾à¦ª            |
| K-Means           | âŒ Distance meaningless |
| Linear / Logistic | âš ï¸ Overfit             |
| Tree-based        | âœ… à¦¤à§à¦²à¦¨à¦¾à¦®à§‚à¦²à¦• à¦­à¦¾à¦²à§‹       |
| PCA + Any model   | âœ… Best practice        |

---

# ðŸ§  à¦–à§à¦¬ à¦¸à¦¹à¦œ Analogy (à¦®à¦¾à¦¥à¦¾à§Ÿ à¦—à§‡à¦à¦¥à§‡ à¦¯à¦¾à¦¬à§‡)

### Feature = à¦°à¦¾à¦¸à§à¦¤à¦¾

### Data = à¦—à¦¾à§œà¦¿

* 2à¦Ÿà¦¾ à¦°à¦¾à¦¸à§à¦¤à¦¾ â†’ à¦—à¦¾à§œà¦¿ à¦šà¦²à¦¾à¦šà¦² à¦¬à§‹à¦à¦¾ à¦¯à¦¾à§Ÿ
* 100à¦Ÿà¦¾ à¦°à¦¾à¦¸à§à¦¤à¦¾ â†’ à¦—à¦¾à§œà¦¿ à¦›à§œà¦¿à§Ÿà§‡ à¦¯à¦¾à§Ÿ
  ðŸ‘‰ Traffic pattern à¦¬à§‹à¦à¦¾ à¦¯à¦¾à§Ÿ à¦¨à¦¾

---

## ðŸ”‘ Golden Sentence (Exam / Interview)

> **The curse of dimensionality refers to problems that arise when analyzing data in high-dimensional spaces, where data becomes sparse and distance measures lose meaning.**

---
