# ‡¶®‡¶æ‡¶á‡¶≠ ‡¶¨‡ßá‡¶á‡¶ú (Naive Bayes): ‡¶≠‡ßç‡¶Ø‡¶æ‡¶∞‡¶ø‡ßü‡ßá‡¶®‡ßç‡¶ü, ‡¶´‡¶∞‡ßç‡¶Æ‡ßÅ‡¶≤‡¶æ, ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ ‡¶ì ‡¶ï‡ßã‡¶°

‡¶®‡¶æ‡¶á‡¶≠ ‡¶¨‡ßá‡¶á‡¶ú ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ú‡¶®‡¶™‡ßç‡¶∞‡¶ø‡¶Ø‡¶º probabilistic classification algorithm ‡¶Ø‡¶æ Bayes‚Äô Theorem-‡¶è‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø ‡¶ï‡¶∞‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá ‡¶è‡¶¨‡¶Ç ‡¶ß‡¶∞‡ßá ‡¶®‡ßá‡ßü ‡¶Ø‡ßá ‡¶∏‡¶¨ feature ‡¶è‡¶ï‡ßá‚Äì‡¶Ö‡¶™‡¶∞‡ßá‡¶∞ ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡ßç‡¶¨‡¶æ‡¶ß‡ßÄ‡¶® (independent)‡•§ ‡¶è‡¶ü‡¶ø ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§, ‡¶≤‡¶æ‡¶á‡¶ü‡¶ì‡ßü‡ßá‡¶ü ‡¶è‡¶¨‡¶Ç ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡¶∏‡¶π ‡¶®‡¶æ‡¶®‡¶æ ‡¶ß‡¶∞‡¶®‡ßá‡¶∞ ‡¶°‡ßá‡¶ü‡¶æ‡ßü ‡¶≠‡¶æ‡¶≤‡ßã ‡¶´‡¶≤ ‡¶¶‡ßá‡ßü‡•§

---

## üîπ 1. Gaussian Naive Bayes

üëâ Continuous / ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ‡¶∏‡ßÇ‡¶ö‡¶ï ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø

### ‡¶ï‡¶ñ‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü?

* ‡¶Ø‡¶ñ‡¶® feature-‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® real number ‡¶π‡ßü (‡¶â‡¶ö‡ßç‡¶ö‡¶§‡¶æ, ‡¶ì‡¶ú‡¶®, ‡¶Æ‡¶æ‡¶∞‡ßç‡¶ï‡¶∏, ‡¶§‡¶æ‡¶™‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø)

### ‡¶ï‡ßÄ‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá?

* ‡¶ß‡¶∞‡ßá ‡¶®‡ßá‡ßü ‡¶Ø‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø feature Gaussian (Normal) Distribution ‡¶Ö‡¶®‡ßÅ‡¶∏‡¶∞‡¶£ ‡¶ï‡¶∞‡ßá

### ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:

* ‡¶è‡¶ï‡¶ú‡¶® ‡¶õ‡¶æ‡¶§‡ßç‡¶∞ ‡¶™‡¶æ‡¶∏ ‡¶ï‡¶∞‡¶¨‡ßá ‡¶®‡¶æ‡¶ï‡¶ø ‡¶´‡ßá‡¶≤ ‡¶ï‡¶∞‡¶¨‡ßá ‚Üí ‡¶á‡¶®‡¶™‡ßÅ‡¶ü: ‡¶Æ‡¶æ‡¶∞‡ßç‡¶ï‡¶∏, ‡¶â‡¶™‡¶∏‡ßç‡¶•‡¶ø‡¶§‡¶ø

### ‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:

‚úî ‡¶∏‡¶π‡¶ú  
‚úî ‡¶õ‡ßã‡¶ü ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü‡ßá ‡¶≠‡¶æ‡¶≤‡ßã ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá

---

## üîπ 2. Multinomial Naive Bayes

üëâ Count-based / Text Data-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø

### ‡¶ï‡¶ñ‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü?

* ‡¶Ø‡¶ñ‡¶® feature ‡¶π‡¶≤‡ßã frequency ‡¶¨‡¶æ count  
* ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£‡¶§ NLP / Text Classification-‡¶è

### ‡¶ï‡ßÄ‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá?

* ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶ï‡ßü‡¶¨‡¶æ‡¶∞ ‡¶è‡¶∏‡ßá‡¶õ‡ßá ‡¶∏‡ßá‡¶ü‡¶æ‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø ‡¶ï‡¶∞‡ßá probability ‡¶π‡¶ø‡¶∏‡¶æ‡¶¨ ‡¶ï‡¶∞‡ßá

### ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:

* Email ‚Üí Spam ‡¶®‡¶æ‡¶ï‡¶ø Not Spam  
* ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‚Üí Sports / Politics / Tech

### ‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:

‚úî Text classification-‡¶è ‡¶ñ‡ßÅ‡¶¨ ‡¶ï‡¶æ‡¶∞‡ßç‡¶Ø‡¶ï‡¶∞  
‚úî Bag of Words, TF-IDF-‡¶è‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶≠‡¶æ‡¶≤‡ßã ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá

---

## üîπ 3. Bernoulli Naive Bayes

üëâ Binary (0/1) ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø

### ‡¶ï‡¶ñ‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü?

* Feature ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶π‡ßç‡¶Ø‡¶æ‡¶Å/‡¶®‡¶æ ‡¶¨‡¶æ 0/1

### ‡¶ï‡ßÄ‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá?

* ‡¶ï‡ßã‡¶®‡ßã feature ‡¶Ü‡¶õ‡ßá ‡¶ï‡¶ø‡¶®‡¶æ ‡¶∏‡ßá‡¶ü‡¶æ‡¶á ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£, ‡¶ï‡¶§‡¶¨‡¶æ‡¶∞ ‡¶Ü‡¶õ‡ßá ‡¶§‡¶æ ‡¶®‡¶æ

### ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:

* ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶Ü‡¶õ‡ßá (1) / ‡¶®‡ßá‡¶á (0)  
* User ‡¶ï‡ßç‡¶≤‡¶ø‡¶ï ‡¶ï‡¶∞‡ßá‡¶õ‡ßá (1) / ‡¶ï‡¶∞‡ßá‡¶®‡¶ø (0)

### Multinomial vs Bernoulli:

| ‡¶¨‡¶ø‡¶∑‡ßü          | Multinomial     | Bernoulli    |
| ------------- | --------------- | ------------ |
| Feature ‡¶ü‡¶æ‡¶á‡¶™  | Count           | Binary       |
| ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ | ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£    | ‡¶®‡¶æ           |
| ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶Ü‡¶õ‡ßá ‡¶ï‡¶ø‡¶®‡¶æ | ‡¶ï‡¶Æ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ | ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ |

---

## üîπ 4. Complement Naive Bayes

üëâ Imbalanced Dataset-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶â‡¶®‡ßç‡¶®‡¶§ ‡¶≠‡¶æ‡¶∞‡ßç‡¶∏‡¶®

### ‡¶ï‡ßá‡¶® ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞?

* Multinomial Naive Bayes imbalanced data-‡¶§‡ßá ‡¶≠‡¶æ‡¶≤‡ßã ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá ‡¶®‡¶æ

### ‡¶ï‡ßÄ‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá?

* ‡¶®‡¶ø‡¶ú‡ßá‡¶∞ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ ‡¶¨‡¶æ‡¶¶ ‡¶¶‡¶ø‡ßü‡ßá ‡¶¨‡¶æ‡¶ï‡¶ø ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶§‡¶•‡ßç‡¶Ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá probability ‡¶π‡¶ø‡¶∏‡¶æ‡¶¨ ‡¶ï‡¶∞‡ßá (complement statistics)

### ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞:

* News classification  
* Sentiment analysis (positive/negative imbalance)

---

## üîπ 5. Categorical Naive Bayes

üëâ Categorical Feature-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø

### ‡¶ï‡¶ñ‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü?

* Feature ‡¶Ø‡¶¶‡¶ø ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø category ‡¶π‡ßü (‡¶Ø‡ßá‡¶Æ‡¶®: Gender, Color, Department)

### ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:

* Gender = Male / Female  
* Browser = Chrome / Firefox / Edge

### ‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ:

‚úî Encoding ‡¶õ‡¶æ‡ßú‡¶æ‡¶ì ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá  
‚úî Discrete categorical data-‡¶§‡ßá ‡¶≠‡¶æ‡¶≤‡ßã

---

## üßÆ ‡¶ó‡¶æ‡¶£‡¶ø‡¶§‡¶ø‡¶ï ‡¶´‡¶∞‡ßç‡¶Æ‡ßÅ‡¶≤‡¶æ (Bayes + Naive ‡¶Ö‡¶®‡ßÅ‡¶Æ‡¶æ‡¶®)

Bayes‚Äô Theorem:

$$
P(C \mid \mathbf{x}) = \frac{P(\mathbf{x} \mid C)\,P(C)}{P(\mathbf{x})}
$$

Naive Independence Assumption:

$$
P(\mathbf{x} \mid C) = \prod_{i=1}^{d} P(x_i \mid C)
$$

Decision Rule (denominator ‡¶∏‡ßç‡¶•‡¶ø‡¶∞ ‡¶ß‡¶∞‡ßá):

$$
\hat{C} = \arg\max_C\; P(\mathbf{x}\mid C)\,P(C)
$$

Gaussian likelihood (feature \(x_i\) normal ‡¶ß‡¶∞‡¶æ):

$$
P(x_i \mid C) = \frac{1}{\sqrt{2\pi\,\sigma^2_{C,i}}}\,\exp\Big( -\frac{(x_i - \mu_{C,i})^2}{2\,\sigma^2_{C,i}} \Big)
$$

Multinomial likelihood (+ Laplace smoothing):

$$
P(\mathbf{x}\mid C) \propto \prod_{i=1}^d \theta_{i\mid C}^{\;x_i},\quad
	heta_{i\mid C} = \frac{N_{i\mid C}+\alpha}{\sum_{j}N_{j\mid C}+\alpha d}
$$

Bernoulli likelihood:

$$
P(\mathbf{x}\mid C) = \prod_{i=1}^d \theta_{i\mid C}^{\;x_i}\,(1-\theta_{i\mid C})^{\;1-x_i}
$$

Complement NB: \(\theta\) ‡¶Ö‡¶®‡ßÅ‡¶Æ‡¶æ‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü class complement ‡¶•‡ßá‡¶ï‡ßá, ‡¶Ø‡¶æ‡¶§‡ßá rare/imbalanced ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶≠‡¶æ‡¶¨ ‡¶∏‡ßÅ‡¶∑‡¶Æ ‡¶•‡¶æ‡¶ï‡ßá‡•§

---

## üìò Worked Example (Multinomial NB, ‡¶õ‡ßã‡¶ü ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü)

‡¶ß‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶ï ‡¶¶‡ßÅ‡¶á‡¶ü‡¶ø ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏: Spam (S) ‡¶è‡¶¨‡¶Ç Ham (H)‡•§ Vocabulary: [offer, win].

Training counts (Laplace smoothing \(\alpha=1\)):

* S: offer=8, win=6, total words=20  
* H: offer=2, win=1, total words=15

Probability estimates:

$$
	heta_{offer\mid S} = \frac{8+1}{20+1\cdot 2} = \frac{9}{22},\quad
	heta_{win\mid S} = \frac{6+1}{22} = \frac{7}{22}
$$

$$
	heta_{offer\mid H} = \frac{2+1}{15+2} = \frac{3}{17},\quad
	heta_{win\mid H} = \frac{1+1}{17} = \frac{2}{17}
$$

Prior (‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£‡¶∏‡ßç‡¶¨‡¶∞‡ßÇ‡¶™): \(P(S)=P(H)=0.5\).

‡¶®‡¶§‡ßÅ‡¶® ‡¶á‡¶Æ‡ßá‡¶á‡¶≤: "offer win win" ‚áí counts: offer=1, win=2.

Log-score (numerator):

$$
\log P(\mathbf{x}\mid S) + \log P(S) = \log\Big((9/22)^1\,(7/22)^2\Big) + \log 0.5
$$

$$
\log P(\mathbf{x}\mid H) + \log P(H) = \log\Big((3/17)^1\,(2/17)^2\Big) + \log 0.5
$$

‡¶è‡¶ñ‡¶æ‡¶®‡ßá Spam-‡¶è‡¶∞ ‡¶∏‡ßç‡¶ï‡ßã‡¶∞ ‡¶¨‡ßá‡¶∂‡¶ø ‚áí ‡¶∂‡ßç‡¶∞‡ßá‡¶£‡ßÄ‡¶ï‡¶∞‡¶£ ‡¶´‡¶≤: Spam.

---

## üêç Python (scikit-learn) ‡¶ï‡ßã‡¶° ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£

‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§ ‡¶ü‡ßç‡¶∞‡¶æ‡¶á ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶§‡ßà‡¶∞‡¶ø‡•§ Python 3.x ‡¶è‡¶¨‡¶Ç scikit-learn ‡¶á‡¶®‡ßç‡¶∏‡¶ü‡¶≤ ‡¶•‡¶æ‡¶ï‡¶æ ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞‡•§

### 1) GaussianNB (‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ‡¶∏‡ßÇ‡¶ö‡¶ï)

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = GaussianNB()
clf.fit(X_train, y_train)
pred = clf.predict(X_test)
print("GaussianNB accuracy:", accuracy_score(y_test, pred))
```

### 2) MultinomialNB (‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü/‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü)

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

texts = [
	"win a lottery now",
	"limited time offer",
	"meeting notes project",
	"family dinner plan",
	"special win offer today",
	"project update meeting"
]
labels = [1, 1, 0, 0, 1, 0]  # 1=Spam, 0=Ham

vec = CountVectorizer()
X = vec.fit_transform(texts)
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=7)

clf = MultinomialNB(alpha=1.0)
clf.fit(X_train, y_train)
pred = clf.predict(X_test)
print("MultinomialNB accuracy:", accuracy_score(y_test, pred))
```

### 3) BernoulliNB (‡¶¨‡¶æ‡¶á‡¶®‡¶æ‡¶∞‡¶ø ‡¶â‡¶™‡¶∏‡ßç‡¶•‡¶ø‡¶§‡¶ø)

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score

texts = [
	"win a lottery now",
	"limited time offer",
	"meeting notes project",
	"family dinner plan",
	"special win offer today",
	"project update meeting"
]
labels = [1, 1, 0, 0, 1, 0]

vec = CountVectorizer(binary=True)
X = vec.fit_transform(texts)
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=7)

clf = BernoulliNB(alpha=1.0)
clf.fit(X_train, y_train)
pred = clf.predict(X_test)
print("BernoulliNB accuracy:", accuracy_score(y_test, pred))
```

### 4) ComplementNB (‡¶á‡¶Æ‡ßç‡¶¨‡ßç‡¶Ø‡¶æ‡¶≤‡ßá‡¶®‡ßç‡¶∏ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü)

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import ComplementNB
from sklearn.metrics import accuracy_score

texts = [
	"rare category news about quantum",
	"common sports update football",
	"rare physics breakthrough",
	"sports match highlights",
	"quantum computing advances",
	"football transfer rumour"
]
labels = [1, 0, 1, 0, 1, 0]  # 1=Rare (e.g., science), 0=Common (sports)

vec = CountVectorizer()
X = vec.fit_transform(texts)
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)

clf = ComplementNB(alpha=1.0)
clf.fit(X_train, y_train)
pred = clf.predict(X_test)
print("ComplementNB accuracy:", accuracy_score(y_test, pred))
```

### 5) CategoricalNB (‡¶°‡¶ø‡¶∏‡¶ï‡ßç‡¶∞‡¶ø‡¶ü ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡ßá‡¶ó‡¶∞‡¶ø)

```python
import numpy as np
from sklearn.naive_bayes import CategoricalNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£: [Gender, Browser, Color] ‡¶∏‡¶¨‡¶ó‡ßÅ‡¶≤‡ßã integer-coded category
# Gender: 0=Male, 1=Female
# Browser: 0=Chrome, 1=Firefox, 2=Edge
# Color: 0=Red, 1=Blue, 2=Green

X = np.array([
	[0, 0, 1], [1, 1, 0], [0, 2, 2], [1, 0, 1], [0, 1, 0], [1, 2, 2],
	[0, 0, 0], [1, 1, 2], [0, 2, 1], [1, 0, 2]
])
y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])  # ‡¶¶‡ßÅ‡¶á‡¶ü‡¶ø ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)

clf = CategoricalNB(alpha=1.0)
clf.fit(X_train, y_train)
pred = clf.predict(X_test)
print("CategoricalNB accuracy:", accuracy_score(y_test, pred))
```

---

## üîπ ‡¶∏‡¶æ‡¶∞‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡ßá‡¶™ ‡¶ü‡ßá‡¶¨‡¶ø‡¶≤

| Variant        | Data ‡¶ü‡¶æ‡¶á‡¶™       | ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞ |
| -------------- | --------------- | --------------- |
| Gaussian NB    | Continuous      | Numeric data    |
| Multinomial NB | Count / Text    | NLP, Spam       |
| Bernoulli NB   | Binary          | Yes/No feature  |
| Complement NB  | Imbalanced Text | NLP             |
| Categorical NB | Categorical     | Category-based  |

---

## üîë Exam Short Notes

* Always use log-sum of probabilities to avoid underflow.  
* Laplace smoothing (Œ±) helps zero-count problems in text.  
* GaussianNB assumes normality per feature; check basic skew/outliers.  
* Multinomial vs Bernoulli: count vs presence‚ÄîNLP-‡¶§‡ßá ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶ß‡¶∞‡¶® ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶¨‡¶æ‡¶õ‡¶æ‡¶á‡•§  
* ComplementNB imbalanced ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡ßá ‡¶Ö‡¶®‡ßá‡¶ï ‡¶∏‡¶Æ‡ßü ‡¶¨‡ßá‡¶∂‡¶ø robust‡•§  
* CategoricalNB integer-coded discrete category-‡¶§‡ßá ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá‡•§

---

### ‡¶Æ‡¶®‡ßá ‡¶∞‡¶æ‡¶ñ‡¶¨‡ßá

* ‡¶∏‡¶¨ Naive Bayes fast & lightweight  
* Feature independence assumption ‡¶¨‡¶æ‡¶∏‡ßç‡¶§‡¶¨‡ßá ‡¶™‡ßÅ‡¶∞‡ßã‡¶™‡ßÅ‡¶∞‡¶ø ‡¶∏‡¶§‡ßç‡¶Ø ‡¶®‡¶æ ‡¶π‡¶≤‡ßá‡¶ì ‡¶≠‡¶æ‡¶≤‡ßã ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá

