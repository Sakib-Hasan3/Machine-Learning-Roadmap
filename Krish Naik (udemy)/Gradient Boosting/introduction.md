![Image](https://pythongeeks.org/wp-content/uploads/2022/03/working-of-gradient-boosting-algorithm.webp)

![Image](https://media.geeksforgeeks.org/wp-content/uploads/20250903173429506712/des.webp)

![Image](https://www.researchgate.net/publication/379187282/figure/fig5/AS%3A11431281246365527%401716346813625/Flow-chart-of-gradient-boosting-regression-model.png)

![Image](https://www.researchgate.net/publication/351542039/figure/fig1/AS%3A11431281172877200%401688685833363/Flow-diagram-of-gradient-boosting-machine-learning-method-The-ensemble-classifiers.png)

## ЁЯФ░ Gradient Boosting Regression тАУ ржмрж╛ржВрж▓рж╛рзЯ рж╕рж╣ржЬ ржУ ржЧржнрзАрж░ ржмрзНржпрж╛ржЦрзНржпрж╛

### ЁЯФ╣ Gradient Boosting Regression ржХрзА?

**Gradient Boosting Regression** рж╣рж▓рзЛ ржПржХржЯрж┐ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА **Ensemble Learning** ржкржжрзНржзрждрж┐, ржпрзЗржЦрж╛ржирзЗ ржЕржирзЗржХржЧрзБрж▓рзЛ **ржжрзБрж░рзНржмрж▓ рж░рж┐ржЧрзНрж░рзЗрж╢ржи ржоржбрзЗрж▓ (Weak Regressors)** ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ (sequentially) ржЯрзНрж░рзЗржи ржХрж░рзЗ ржПржХржЯрж┐ **ржЙржЪрзНржЪ-ржирж┐рж░рзНржнрзБрж▓ Regression Model** рждрзИрж░рж┐ ржХрж░рж╛ рж╣рзЯред

ЁЯСЙ ржПржЦрж╛ржирзЗ ржкрзНрж░рждрж┐ржЯрж┐ ржирждрзБржи ржоржбрзЗрж▓ ржЖржЧрзЗрж░ ржоржбрзЗрж▓рзЗрж░ **ржнрзБрж▓ (Residual / Error)** ржарж┐ржХ ржХрж░рж╛рж░ ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рзЗред

---

### ЁЯФ╣ Boosting + Regression = Gradient Boosting Regression

* **Boosting** тЖТ ржПржХрзЗрж░ ржкрж░ ржПржХ ржоржбрзЗрж▓ рж╢рзЗржЦрзЗ
* **Regression** тЖТ Continuous value (ржжрж╛ржо, рж╕рзНржХрзЛрж░, рждрж╛ржкржорж╛рждрзНрж░рж╛ ржЗрждрзНржпрж╛ржжрж┐) predict ржХрж░рзЗ
* **Gradient** тЖТ Loss function ржХржорж╛ржирзЛрж░ ржжрж┐ржХржирж┐рж░рзНржжрзЗрж╢ржирж╛ ржжрзЗрзЯ

ЁЯУМ ржЕрж░рзНржерж╛рзО:

> Gradient Boosting ржПржоржи ржПржХржЯрж┐ ржкржжрзНржзрждрж┐ ржпрзЗржЦрж╛ржирзЗ ржЖржорж░рж╛ **Loss Function ржХржорж╛ржирзЛрж░ ржЬржирзНржп ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ ржоржбрзЗрж▓ ржЖржкржбрзЗржЯ ржХрж░рж┐**ред

---

### ЁЯФ╣ Gradient Boosting Regression ржХрзАржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░рзЗ? (Step by Step)

#### ЁЯзй Step 1: Initial Prediction

рж╢рзБрж░рзБрждрзЗ ржПржХржЯрж┐ ржЦрзБржм рж╕рж╛ржзрж╛рж░ржг ржорж╛ржи ржжрж┐рзЯрзЗ prediction ржХрж░рж╛ рж╣рзЯ
ЁЯСЙ рж╕рж╛ржзрж╛рж░ржгржд **target variable-ржПрж░ mean**

[
\hat{y}_0 = \text{mean}(y)
]

---

#### ЁЯзй Step 2: Residual (ржнрзБрж▓) рж╣рж┐рж╕рж╛ржм ржХрж░рж╛

ржкрзНрж░рждрж┐ржЯрж┐ ржбрзЗржЯрж╛ ржкрзЯрзЗржирзНржЯрзЗрж░ ржЬржирзНржп:

[
\text{Residual} = y - \hat{y}
]

ЁЯСЙ ржПржЗ residual-ржЗ рж╣рж▓рзЛ тАЬржЖржЧрзЗрж░ ржоржбрзЗрж▓ ржХрзЛржерж╛рзЯ ржнрзБрж▓ ржХрж░рзЗржЫрзЗтАЭ

---

#### ЁЯзй Step 3: Weak Regressor ржЯрзНрж░рзЗржи

ржПржХржЯрж┐ **Decision Tree Regressor (shallow tree)** residual-ржЧрзБрж▓рзЛрж░ ржУржкрж░ ржЯрзНрж░рзЗржи ржХрж░рж╛ рж╣рзЯред

---

#### ЁЯзй Step 4: Prediction Update

ржирждрзБржи prediction ржЖржкржбрзЗржЯ ржХрж░рж╛ рж╣рзЯ:

[
\hat{y}*{new} = \hat{y}*{old} + \eta \times \text{Tree Prediction}
]

ржпрзЗржЦрж╛ржирзЗ

* (\eta) = **Learning Rate** (ржЫрзЛржЯ рж╣рж▓рзЗ рж╢рзЗржЦрж╛ ржзрзАрж░ ржХрж┐ржирзНрждрзБ stable)

---

#### ЁЯзй Step 5: ржмрж╛рж░ржмрж╛рж░ Repeat

ржПржЗ ржкрзНрж░ржХрзНрж░рж┐рзЯрж╛ржЯрж┐ ржЕржирзЗржХржЧрзБрж▓рзЛ tree ржкрж░рзНржпржирзНржд ржЪрж╛рж▓рж╛ржирзЛ рж╣рзЯред

---

### ЁЯФ╣ Residual рж╢рзЗржЦрж╛рж░ ржорзВрж▓ ржзрж╛рж░ржгрж╛ (Intuition)

> тЭЭ ржкрзНрж░рждрж┐ржмрж╛рж░ ржЖржорж┐ рж╢рзБржзрзБ ржЖржЧрзЗрж░ ржнрзБрж▓ржЯрж╛ржЗ рж╢рж┐ржЦржм тЭЮ

Gradient Boosting рж╕рж░рж╛рж╕рж░рж┐ target рж╢рзЗржЦрзЗ ржирж╛,
ЁЯСЙ рж╕рзЗ рж╢рзЗржЦрзЗ **тАЬржЖржорж┐ ржХрзЛржерж╛рзЯ ржнрзБрж▓ ржХрж░рзЗржЫрж┐рж▓рж╛ржотАЭ**

---

### ЁЯФ╣ Loss Function ржПрж░ ржнрзВржорж┐ржХрж╛

Gradient Boosting Regression рж╕рж╛ржзрж╛рж░ржгржд ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ:

* **Mean Squared Error (MSE)**
* **Mean Absolute Error (MAE)**
* **Huber Loss**

ЁЯУМ Gradient = Loss function-ржПрж░ derivative
ЁЯСЙ ржХрзЛржи ржжрж┐ржХрзЗ ржЧрзЗрж▓рзЗ loss ржХржоржмрзЗ, рж╕рзЗржЯрж╛ржЗ gradient ржжрзЗржЦрж╛рзЯ

---

### ЁЯФ╣ Gradient Boosting Regression ржХрзЛржерж╛рзЯ ржмрзНржпржмрж╣рж╛рж░ рж╣рзЯ?

* тЬЕ House price prediction
* тЬЕ Sales forecasting
* тЬЕ Demand prediction
* тЬЕ Risk modeling
* тЬЕ Weather prediction

---

### ЁЯФ╣ Gradient Boosting Regression-ржПрж░ рж╕рзБржмрж┐ржзрж╛ тЬЕ

тЬФ ржЦрзБржмржЗ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА prediction
тЬФ Complex non-linear relation ржзрж░рждрзЗ ржкрж╛рж░рзЗ
тЬФ Feature scaling рж▓рж╛ржЧрзЗ ржирж╛
тЬФ Bias ржХржорж╛рзЯ

---

### ЁЯФ╣ Gradient Boosting Regression-ржПрж░ рж╕рзАржорж╛ржмржжрзНржзрждрж╛ тЭМ

тЬШ Overfitting рж╣рждрзЗ ржкрж╛рж░рзЗ (tree ржмрзЗрж╢рж┐ рж╣рж▓рзЗ)
тЬШ Training ржзрзАрж░ (sequential nature)
тЬШ Hyperparameter tuning ржЬрж░рзБрж░рж┐

---

### ЁЯФ╣ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг Hyperparameters

| Parameter       | ржХрж╛ржЬ            |
| --------------- | -------------- |
| `n_estimators`  | ржХрзЯржЯрж╛ tree      |
| `learning_rate` | рж╢рзЗржЦрж╛рж░ ржЧрждрж┐      |
| `max_depth`     | tree-ржПрж░ ржЧржнрзАрж░рждрж╛ |
| `subsample`     | data sampling  |
| `loss`          | error measure  |

---

### ЁЯФ╣ Gradient Boosting Regression ржмржирж╛ржо Random Forest

| ржмрж┐рж╖рзЯ        | Gradient Boosting | Random Forest   |
| ----------- | ----------------- | --------------- |
| Training    | Sequential        | Parallel        |
| Focus       | Residual          | Random sampling |
| Overfitting | ржмрзЗрж╢рж┐ рж╕ржорзНржнрж╛ржмржирж╛     | ржХржо              |
| Accuracy    | ржЦрзБржм ржмрзЗрж╢рж┐          | ржнрж╛рж▓рзЛ            |

---

### ЁЯФ╣ ржПржХ рж▓рж╛ржЗржирзЗ рж╕рж╛рж░рж╛ржВрж╢

**Gradient Boosting Regression = Mean prediction тЖТ Residual рж╢рзЗржЦрж╛ тЖТ ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ Error ржХржорж╛ржирзЛ**

---

