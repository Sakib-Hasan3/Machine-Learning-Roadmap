![Image](https://miro.medium.com/1%2AREUu0LJytEt6kIOFQeB0Tw.png)

![Image](https://media.geeksforgeeks.org/wp-content/uploads/20250903173429506712/des.webp)

![Image](https://www.researchgate.net/publication/351542039/figure/fig1/AS%3A11431281172877200%401688685833363/Flow-diagram-of-gradient-boosting-machine-learning-method-The-ensemble-classifiers.png)

![Image](https://www.analytixlabs.co.in/wp-content/uploads/2022/10/Artboard-1-copy-11-100.jpg)

## ЁЯФ░ Gradient Boosting Classifier тАУ Training Process (ржмрж╛ржВрж▓рж╛рзЯ ржмрж┐рж╕рзНрждрж╛рж░рж┐ржд)

### ЁЯФ╣ Gradient Boosting Classifier ржХрзА?

**Gradient Boosting Classifier** рж╣рж▓рзЛ ржПржХржЯрж┐ **Ensemble Classification algorithm**, ржпрзЗржЦрж╛ржирзЗ ржЕржирзЗржХржЧрзБрж▓рзЛ **ржжрзБрж░рзНржмрж▓ classifier (weak learners)** ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ (sequentially) ржЯрзНрж░рзЗржи ржХрж░рзЗ ржПржХржЯрж┐ **рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА classifier** рждрзИрж░рж┐ ржХрж░рж╛ рж╣рзЯред

ЁЯУМ ржПржЦрж╛ржирзЗ ржкрзНрж░рждрж┐ржЯрж┐ ржирждрзБржи ржоржбрзЗрж▓ ржЖржЧрзЗрж░ ржоржбрзЗрж▓рзЗрж░ **ржнрзБрж▓ (error / residual)** ржХржорж╛ржирзЛрж░ ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рзЗред

---

## ЁЯза Gradient Boosting Classifier Training тАУ Step by Step

### ЁЯзй Step 1: Initial Model (рж╢рзБрж░рзБрж░ ржоржбрзЗрж▓)

ClassificationтАУржП рж╢рзБрж░рзБрждрзЗ ржПржХржЯрж┐ **constant prediction** ржирзЗржУрзЯрж╛ рж╣рзЯред

* Binary classification рж╣рж▓рзЗ:

  * рж╕рж╛ржзрж╛рж░ржгржд **log-odds (logit)** ржжрж┐рзЯрзЗ рж╢рзБрж░рзБ рж╣рзЯ
* рж╕рж╣ржЬржнрж╛ржмрзЗ ржнрж╛ржмрж▓рзЗ:

  * рж╢рзБрж░рзБрждрзЗ рж╕ржмрж╛рж░ ржЬржирзНржп ржПржХржЗ prediction

ЁЯСЙ ржЙржжрж╛рж╣рж░ржг:
ржзрж░рж┐ ржХрзНрж▓рж╛рж╕ = {0, 1}
рж╢рзБрж░рзБрждрзЗ model ржмрж▓рзЗ: тАЬрж╕ржмрж╛ржЗ 0 рж╣ржУрзЯрж╛рж░ рж╕ржорзНржнрж╛ржмржирж╛ 50%тАЭ

---

### ЁЯзй Step 2: Loss Function ржирж┐рж░рзНржзрж╛рж░ржг

ClassificationтАУржП рж╕ржмржЪрзЗрзЯрзЗ ржмрзЗрж╢рж┐ ржмрзНржпржмрж╣рзГржд loss:

* **Log Loss (Binary Cross Entropy)**

[
Loss = -[y\log(p) + (1-y)\log(1-p)]
]

ЁЯУМ Gradient Boosting ржПржЦрж╛ржирзЗ **loss function minimize** ржХрж░рждрзЗ рж╢рзЗржЦрзЗред

---

### ЁЯзй Step 3: Pseudo-Residual (Gradient) ржЧржгржирж╛

ржПржЯрж╛ржЗ Gradient BoostingтАУржПрж░ ржорзВрж▓ ржзрж╛ржк ЁЯФе

[
\text{Residual} = y - p
]

ЁЯСЙ ржПржЦрж╛ржирзЗ:

* (y) = ржЖрж╕рж▓ label
* (p) = model-ржПрж░ predicted probability

ЁЯУМ ржПржЗ residual ржЖрж╕рж▓рзЗ loss function-ржПрж░ **negative gradient**ред

---

### ЁЯзй Step 4: Weak Learner ржЯрзНрж░рзЗржи ржХрж░рж╛

ржПржХржЯрж┐ **Decision Tree Classifier (shallow tree)** ржЯрзНрж░рзЗржи ржХрж░рж╛ рж╣рзЯ ржПржЗ residual-ржПрж░ ржУржкрж░ред

* Tree рж╢рзЗржЦрзЗ:
  ЁЯСЙ тАЬржХрзЛржи feature ржжрзЗржЦрзЗ error ржмрзЗрж╢рж┐ рж╣ржЪрзНржЫрзЗ?тАЭ

---

### ЁЯзй Step 5: Prediction Update (Model Update)

Model ржЖржкржбрзЗржЯ рж╣рзЯ ржПржнрж╛ржмрзЗ:

[
F_{new}(x) = F_{old}(x) + \eta \times h(x)
]

ржпрзЗржЦрж╛ржирзЗ:

* (h(x)) = ржирждрзБржи tree-ржПрж░ output
* (\eta) = **Learning Rate**

---

### ЁЯзй Step 6: Probability Update

Updated score ржерзЗржХрзЗ ржЖржмрж╛рж░ probability ржмрзЗрж░ ржХрж░рж╛ рж╣рзЯ:

[
p = \frac{1}{1 + e^{-F(x)}}
]

---

### ЁЯзй Step 7: Iteration (Repeat)

ржПржЗ ржзрж╛ржкржЧрзБрж▓рзЛ ржмрж╛рж░ржмрж╛рж░ ржЪрж▓рзЗ:

* Residual рж╣рж┐рж╕рж╛ржм
* Tree train
* Model update

ЁЯСЙ ржпрждржмрж╛рж░ `n_estimators`, рждрждржмрж╛рж░

---

### ЁЯзй Step 8: Final Prediction

рж╢рзЗрж╖рзЗ:

* Probability тЙе threshold тЖТ Class 1
* Probability < threshold тЖТ Class 0

---

## ЁЯФ╣ TrainingтАУржПрж░ Intuition (рж╕рж╣ржЬ ржнрж╛рж╖рж╛рзЯ)

> тЭЭ ржкрзНрж░рждрж┐ржмрж╛рж░ ржЖржорж┐ рж╢рзБржзрзБ рж╕рзЗржЗ ржЬрж╛рзЯржЧрж╛рзЯ рж╢рж┐ржЦржм, ржпрзЗржЦрж╛ржирзЗ ржЖржорж┐ ржнрзБрж▓ ржХрж░рзЗржЫрж┐ тЭЮ

Gradient Boosting Classifier:

* ржЖржЧрзЗрж░ ржнрзБрж▓ sampleтАУржЧрзБрж▓рзЛрждрзЗ ржмрзЗрж╢рж┐ ржлрзЛржХрж╛рж╕ ржХрж░рзЗ
* ржзрзАрж░рзЗ ржзрзАрж░рзЗ decision boundary ржарж┐ржХ ржХрж░рзЗ

---

## ЁЯФ╣ ржХрзЗржи Gradient Boosting Classifier ржПржд рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА?

тЬФ Non-linear boundary рж╢рж┐ржЦрждрзЗ ржкрж╛рж░рзЗ
тЬФ Complex feature interaction ржзрж░рждрзЗ ржкрж╛рж░рзЗ
тЬФ Bias ржХржорж╛рзЯ
тЬФ Probability output ржжрзЗрзЯ

---

## ЁЯФ╣ Important Hyperparameters (Training Control)

| Parameter       | ржХрж╛ржЬ            |
| --------------- | -------------- |
| `n_estimators`  | ржХрзЯржЯрж╛ tree      |
| `learning_rate` | рж╢рзЗржЦрж╛рж░ ржЧрждрж┐      |
| `max_depth`     | tree-ржПрж░ ржЧржнрзАрж░рждрж╛ |
| `subsample`     | data sampling  |
| `loss`          | `log_loss`     |

---

## ЁЯФ╣ Overfitting ржХрж┐ржнрж╛ржмрзЗ ржХржорж╛ржмрзЗржи?

* Learning rate ржЫрзЛржЯ рж░рж╛ржЦрзБржи
* Tree depth ржХржорж╛ржи
* `subsample < 1.0` ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи
* Early stopping ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи

---

## ЁЯФ╣ ржПржХ рж▓рж╛ржЗржирзЗ рж╕рж╛рж░рж╛ржВрж╢

**Gradient Boosting Classifier Training = Initial guess тЖТ Gradient (error) рж╢рзЗржЦрж╛ тЖТ Tree ржпрзЛржЧ тЖТ Probability refine**

---
