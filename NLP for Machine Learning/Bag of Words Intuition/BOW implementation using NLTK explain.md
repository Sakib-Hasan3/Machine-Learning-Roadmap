ржЪрж▓рзБржи ржПржЦржи ржЖржЧрзЗрж░ **NLTK ржжрж┐рзЯрзЗ BoW implementation**тАУржПрж░ **ржкрзНрж░рждрж┐ржЯрж┐ cell ржПржХрзЗ ржПржХрзЗ ржмрзНржпрж╛ржЦрзНржпрж╛ ржХрж░рж┐**
ржПржХржжржо **exam + practical intuition** ржзрж░рзЗ ЁЯСЗ

---

# ЁЯза Bag of Words (BoW) using NLTK тАФ **Cell-by-Cell Explanation**

---

## тЬЕ **Cell 1 тАФ NLTK downloads + Imports**

```python
import nltk
nltk.download("punkt")
nltk.download("stopwords")
nltk.download("wordnet")
nltk.download("omw-1.4")

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

import re
import pandas as pd
```

### ЁЯФН ржПржЗ cell ржХрзА ржХрж░ржЫрзЗ?

ЁЯФ╣ `nltk.download(...)`

* **punkt** тЖТ tokenization (sentence/word ржнрж╛ржЩрждрзЗ)
* **stopwords** тЖТ is, the, and ржЯрж╛ржЗржк рж╢ржмрзНржж ржмрж╛ржж ржжрж┐рждрзЗ
* **wordnet + omw** тЖТ lemmatization ржПрж░ ржЬржирзНржп

ЁЯФ╣ Imports:

* `word_tokenize` тЖТ ржмрж╛ржХрзНржп тЖТ рж╢ржмрзНржж
* `stopwords` тЖТ common рж╢ржмрзНржж ржмрж╛ржж
* `WordNetLemmatizer` тЖТ рж╢ржмрзНржжржХрзЗ base form ржП ржЖржирж╛
* `re` тЖТ text cleaning (regex)
* `pandas` тЖТ table ржЖржХрж╛рж░рзЗ output ржжрзЗржЦрж╛рждрзЗ

ЁЯУМ **ржПржЗ cell ржЫрж╛рзЬрж╛ NLP preprocessing рж╣ржмрзЗ ржирж╛**

---

## тЬЕ **Cell 2 тАФ Corpus (Raw Text Dataset)**

```python
corpus = [
    "I love machine learning. Machine learning is fun!",
    "I love NLP and I love deep learning.",
    "NLP is about text processing and understanding language."
]
corpus
```

### ЁЯФН ржПржЗ cell ржХрзА ржХрж░ржЫрзЗ?

ЁЯФ╣ `corpus` = **documents-ржПрж░ list**

* ржкрзНрж░рждрж┐ржЯрж╛ element = ржПржХрзЗржХржЯрж╛ document / sentence

ЁЯУМ NLP-рждрзЗ:

* Corpus = рж╕ржорзНржкрзВрж░рзНржг text collection
* Document = corpus-ржПрж░ ржПржХржЯрж┐ element

---

## тЬЕ **Cell 3 тАФ Preprocessing Function**

```python
stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()
```

ЁЯФ╣ ржПржЦрж╛ржирзЗ:

* English stopwords load ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ
* Lemmatizer object рждрзИрж░рж┐

---

### ЁЯФ╣ Preprocessing Function

```python
def preprocess(text):
    text = text.lower()
    text = re.sub(r"[^a-z\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()

    tokens = word_tokenize(text)
    tokens = [t for t in tokens if t not in stop_words]
    tokens = [lemmatizer.lemmatize(t) for t in tokens]
    return tokens
```

### ЁЯза ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ ржХрзА рж╣ржЪрзНржЫрзЗ?

1я╕ПтГг `lower()`
тЖТ рж╕ржм рж╢ржмрзНржж ржЫрзЛржЯ рж╣рж╛рждрзЗрж░

```
Machine тЖТ machine
```

2я╕ПтГг Regex cleaning
тЖТ punctuation, number ржмрж╛ржж

```
learning. тЖТ learning
```

3я╕ПтГг `word_tokenize()`
тЖТ sentence тЖТ рж╢ржмрзНржжрзЗрж░ list

```
"I love NLP" тЖТ ["i","love","nlp"]
```

4я╕ПтГг Stopword removal
тЖТ is, the, and ржмрж╛ржж

5я╕ПтГг Lemmatization

```
learning тЖТ learn
```

ЁЯУМ Output = **clean tokens**

---

### ЁЯФ╣ Apply preprocessing

```python
processed = [preprocess(doc) for doc in corpus]
processed
```

тЮб ржкрзНрж░рждрж┐ржЯрж╛ document ржПржЦржи **clean word list**

---

## тЬЕ **Cell 4 тАФ Vocabulary рждрзИрж░рж┐**

```python
vocab = sorted(set(word for doc in processed for word in doc))
vocab
```

### ЁЯФН ржПржЗ cell ржХрзА ржХрж░ржЫрзЗ?

ЁЯФ╣ рж╕ржм document ржерзЗржХрзЗ:

* unique рж╢ржмрзНржж ржмрзЗрж░ ржХрж░рж╛
* duplicate ржмрж╛ржж
* alphabetically sort

ЁЯУМ Vocabulary = **BoW-ржПрж░ backbone**

Example:

```
["about", "deep", "fun", "language", "learn", ...]
```

---

## тЬЕ **Cell 5 тАФ Word тЖТ Index Mapping**

```python
word2idx = {word: i for i, word in enumerate(vocab)}
word2idx
```

### ЁЯФН ржХрзЗржи ржжрж░ржХрж╛рж░?

BoW vector ржмрж╛ржирж╛рждрзЗ:

* ржкрзНрж░рждрж┐ржЯрж╛ рж╢ржмрзНржжрзЗрж░ ржПржХржЯрж╛ **fixed position** ржжрж░ржХрж╛рж░

Example:

```
"learn" тЖТ index 4
```

ЁЯУМ ржПржЦржи vector-ржПрж░ 4 ржиржорзНржмрж░ position ржорж╛ржирзЗ тАЬlearnтАЭ

---

## тЬЕ **Cell 6 тАФ BoW Vector рждрзИрж░рж┐ (Count-based)**

```python
def bow_vector(tokens, word2idx):
    vec = [0] * len(word2idx)
    for w in tokens:
        vec[word2idx[w]] += 1
    return vec
```

### ЁЯза ржПржЗ function ржХрзА ржХрж░ржЫрзЗ?

* рж╢рзБрж░рзБрждрзЗ vector = `[0,0,0,0,...]`
* token ржЖрж╕рж▓рзЗ тЖТ рж╕рзЗржЗ index ржП count ржмрж╛рзЬрзЗ

Example:

```
tokens = ["love","nlp","love"]
vector тЖТ love index = 2
тЖТ [0,0,2,0,...]
```

---

### ЁЯФ╣ Apply to all documents

```python
bow_vectors = [bow_vector(doc_tokens, word2idx) for doc_tokens in processed]
bow_vectors
```

тЮб ржПржЦржи ржкрзНрж░рждрж┐ржЯрж╛ document = **numerical vector**

---

## тЬЕ **Cell 7 тАФ BoW Matrix (Readable Table)**

```python
bow_df = pd.DataFrame(bow_vectors, columns=vocab)
bow_df.index = [f"D{i+1}" for i in range(len(corpus))]
bow_df
```

### ЁЯФН ржПржЗ cell ржХрзА ржХрж░ржЫрзЗ?

* BoW vectors тЖТ DataFrame
* Columns = Vocabulary
* Rows = Documents (D1, D2, D3)

ЁЯУМ ржПржЯрж╛ржХрзЗржЗ ржмрж▓рзЗ:

> **DocumentтАУTerm Matrix**

---

## тЬЕ **Cell 8 тАФ Tokens + BoW ржПржХрж╕рж╛ржерзЗ ржжрзЗржЦрж╛**

```python
for i, (tokens, vec) in enumerate(zip(processed, bow_vectors), start=1):
    print(f"Document D{i}:")
    print("Tokens:", tokens)
    print("BoW:", vec)
    print("-" * 60)
```

### ЁЯФН ржПржЗ cell ржХрзЗржи?

* Learning purpose
* ржмрзЛржЭрж╛рж░ ржЬржирзНржп:

```
ржПржЗ tokens тЖТ ржПржЗ vector
```

ЁЯУМ Exam-ржП ржмрзЛржЭрж╛ржирзЛрж░ ржЬржирзНржп ржЦрзБржм useful

---

# ЁЯза Final Intuition (рж╕ржм ржорж┐рж▓рж┐рзЯрзЗ)

```
Raw Text
 тЖУ
Cleaning
 тЖУ
Tokenization
 тЖУ
Stopword Removal
 тЖУ
Lemmatization
 тЖУ
Vocabulary
 тЖУ
Count words
 тЖУ
BoW Vector
 тЖУ
BoW Matrix
```

---

## тЬНя╕П Exam-Ready One-Liner

> NLTK ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ Bag of Words implementation-ржП ржкрзНрж░ржержорзЗ text preprocessing ржХрж░рж╛ рж╣ржпрж╝, рждрж╛рж░ржкрж░ vocabulary рждрзИрж░рж┐ ржХрж░рзЗ ржкрзНрж░рждрж┐ржЯрж┐ document-ржХрзЗ рж╢ржмрзНржжрзЗрж░ frequency ржнрзЗржХрзНржЯрж░ рж╣рж┐рж╕рзЗржмрзЗ ржкрзНрж░ржХрж╛рж╢ ржХрж░рж╛ рж╣ржпрж╝ред

---
