---

## ‚úÖ Sentence (‡¶è‡¶ï‡¶á‡¶ü‡¶æ ‡¶¶‡ßÅ‚Äô‡¶ú‡¶æ‡ßü‡¶ó‡¶æ‡ßü)

**"Apple is looking at buying a startup in London in 2024."**

---

## 1) spaCy vs NLTK ‚Äî Full Comparison Code

```python
# ---------- NLTK ----------
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk import pos_tag

# (‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡¶¨‡¶æ‡¶∞ ‡¶ö‡¶æ‡¶≤‡¶æ‡¶≤‡ßá ‡¶≤‡¶æ‡¶ó‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá)
# nltk.download("punkt")
# nltk.download("averaged_perceptron_tagger")

sentence = "Apple is looking at buying a startup in London in 2024."

tokens_nltk = word_tokenize(sentence)
ps = PorterStemmer()
stems = [ps.stem(w) for w in tokens_nltk]
pos_nltk = pos_tag(tokens_nltk)

print("=== NLTK ===")
print("Tokens:", tokens_nltk)
print("Stems:", stems)
print("POS:", pos_nltk)


# ---------- spaCy ----------
import spacy

# (‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡¶¨‡¶æ‡¶∞ ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤ ‡¶ï‡¶∞‡¶≤‡ßá: python -m spacy download en_core_web_sm)
nlp = spacy.load("en_core_web_sm")

doc = nlp(sentence)

tokens_spacy = [t.text for t in doc]
lemmas_spacy = [t.lemma_ for t in doc]
pos_spacy = [(t.text, t.pos_) for t in doc]
entities_spacy = [(ent.text, ent.label_) for ent in doc.ents]

print("\n=== spaCy ===")
print("Tokens:", tokens_spacy)
print("Lemmas:", lemmas_spacy)
print("POS:", pos_spacy)
print("Entities (NER):", entities_spacy)
```

---

## 2) Expected Output (Example)

### ‚úÖ NLTK Output

```
=== NLTK ===
Tokens: ['Apple', 'is', 'looking', 'at', 'buying', 'a', 'startup', 'in', 'London', 'in', '2024', '.']
Stems:  ['appl', 'is', 'look', 'at', 'buy', 'a', 'startup', 'in', 'london', 'in', '2024', '.']
POS: [('Apple', 'NNP'), ('is', 'VBZ'), ('looking', 'VBG'), ('at', 'IN'), ('buying', 'VBG'),
      ('a', 'DT'), ('startup', 'NN'), ('in', 'IN'), ('London', 'NNP'), ('in', 'IN'), ('2024', 'CD'), ('.', '.')]
```

üìå ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶¶‡ßá‡¶ñ‡ßÅ‡¶®:

* **Stemming** ‡¶ï‡¶∞‡ßá ‚ÄúApple ‚Üí appl‚Äù (real word ‡¶®‡¶æ‚Äî‡¶ï‡ßá‡¶ü‡ßá ‡¶¶‡¶ø‡ßü‡ßá‡¶õ‡ßá)

---

### ‚úÖ spaCy Output

```
=== spaCy ===
Tokens: ['Apple', 'is', 'looking', 'at', 'buying', 'a', 'startup', 'in', 'London', 'in', '2024', '.']
Lemmas: ['Apple', 'be', 'look', 'at', 'buy', 'a', 'startup', 'in', 'London', 'in', '2024', '.']
POS: [('Apple', 'PROPN'), ('is', 'AUX'), ('looking', 'VERB'), ('at', 'ADP'), ('buying', 'VERB'),
      ('a', 'DET'), ('startup', 'NOUN'), ('in', 'ADP'), ('London', 'PROPN'), ('in', 'ADP'), ('2024', 'NUM'), ('.', 'PUNCT')]
Entities (NER): [('Apple', 'ORG'), ('London', 'GPE'), ('2024', 'DATE')]
```

üìå ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶¶‡ßá‡¶ñ‡ßÅ‡¶®:

* **Lemmatization** ‡¶ï‡¶∞‡ßá ‚Äúis ‚Üí be‚Äù (grammar-aware)
* **NER** ‡¶¶‡¶ø‡ßü‡ßá ORG/GPE/DATE ‡¶ß‡¶∞‡¶õ‡ßá (NLTK basic ‡¶∏‡ßá‡¶ü‡¶æ‡¶™‡ßá ‡¶è‡¶ü‡¶æ ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£‡¶§ ‡¶•‡¶æ‡¶ï‡ßá ‡¶®‡¶æ)

---

## 3) Quick Takeaway (‡¶è‡¶ï ‡¶≤‡¶æ‡¶á‡¶®‡ßá)

* **NLTK**: ‡¶∂‡ßá‡¶ñ‡¶æ + basic preprocessing (tokenize, stem, basic POS)
* **spaCy**: fast + accurate + **Lemmatization + NER + full NLP pipeline**

---


